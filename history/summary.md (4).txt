# [Configuring each kubelet in your cluster using kubeadm | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/)

## Yellow

- The lifecycle of the kubeadm CLI tool is decoupled from the kubelet, which is a daemon that runs on each node within the Kubernetes cluster.
- You can manage the configuration of your kubelets manually, but kubeadm now provides a KubeletConfiguration API

## Orange

- Propagating cluster-level configuration to each kubelet

## Yellow

- You can provide the kubelet with default values to be used by kubeadm init and kubeadm join commands.
- If you want your services to use the subnet 10.96.0.0/12 as the default for services, you can pass the --service-cidr parameter to kubeadm: kubeadm init --service-cidr 10.96.0.0/12
- You also need to set the DNS address used by the kubelet, using the --cluster-dns flag.
- This setting needs to be the same for every kubelet on every manager and Node in the cluster.
- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10
- When you call kubeadm init, the kubelet configuration is marshalled to disk
- After marshalling these two files to disk, kubeadm attempts to run the following two commands, if you are using systemd: systemctl daemon-reload && systemctl restart kubelet
- Kubernetes binaries and package contents The DEB and RPM packages shipped with the Kubernetes releases are: Package name Description kubeadm Installs the /usr/bin/kubeadm CLI tool and the kubelet drop-in file for the kubelet. kubelet Installs the /usr/bin/kubelet binary. kubectl Installs the /usr/bin/kubectl binary. cri-tools Installs the /usr/bin/crictl binary from the cri-tools git repository. kubernetes-cni Installs the /opt/cni/bin binaries from the plugins git repository.

# [Considerations for large clusters | Kubernetes](https://kubernetes.io/docs/setup/best-practices/cluster-large/)

## Yellow

- A cluster is a set of nodes (physical or virtual machines) running Kubernetes agents, managed by the control plane. Kubernetes v1.32 supports clusters with up to 5,000 nodes. More specifically, Kubernetes is designed to accommodate configurations that meet all of the following criteria: No more than 110 pods per node No more than 5,000 nodes No more than 150,000 total pods No more than 300,000 total containers
- For a large cluster, you need a control plane with sufficient compute and other resources. Typically you would run one or two control plane instances per failure zone, scaling those instances vertically first and then scaling horizontally after reaching the point of falling returns to (vertical) scale. You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes nodes do not automatically steer traffic towards control-plane endpoints that are in the same failure zone; however, your cloud provider might have its own mechanisms to do this. For example, using a managed load balancer, you configure the load balancer to send traffic that originates from the kubelet and Pods in failure zone A, and direct that traffic only to the control plane hosts that are also in zone A. If a single control-plane host or endpoint failure zone A goes offline, that means that all the control-plane traffic for nodes in zone A is now being sent between zones. Running multiple control plane hosts in each zone makes that outcome less likely.
- To improve performance of large clusters, you can store Event objects in a separate dedicated etcd instance. When creating a cluster, you can (using custom tooling): start and configure additional etcd instance configure the API server to use it for storing events
- To avoid running into cluster addon resource issues, when creating a cluster with many nodes, consider the following
- Some addons scale vertically
- Many addons scale horizontally
- Some addons run as one copy per node, controlled by a DaemonSet

# [Container Runtimes | Kubernetes](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

## Yellow

- You need to install a container runtime into each node in the cluster so that Pods can run there.

## Orange

- Network configuration

## Yellow

- By default, the Linux kernel does not allow IPv4 packets to be routed between interfaces. Most Kubernetes cluster networking implementations will change this setting

## Orange

- cgroup drivers

## Yellow

- Both the kubelet and the underlying container runtime need to interface with control groups to enforce resource management for pods and containers and set resources such as cpu/memory requests and limits. To interface with control groups, the kubelet and the container runtime need to use a cgroup driver. It's critical that the kubelet and the container runtime use the same cgroup driver and are configured the same. There are two cgroup drivers available: cgroupfs systemd
- Two cgroup managers result in two views of the available and in-use resources in the system.

# [Customizing components with the kubeadm API | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)

## Orange

- Customizing components with the kubeadm API

## Yellow

- Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually patch the kube-system/coredns ConfigMap and recreate the CoreDNS Pods after that.

# [Getting started | Kubernetes](https://kubernetes.io/docs/setup/)

## Yellow

- Several Kubernetes components such as kube-apiserver or kube-proxy can also be deployed as container images within the cluster.

# [Options for Highly Available Topology | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)

## Yellow

- You can set up an HA cluster: With stacked control plane nodes, where etcd nodes are colocated with control plane nodes With external etcd nodes, where etcd runs on separate nodes from the control plane

## Orange

- Stacked etcd topology

## Yellow

- Each control plane node creates a local etcd member and this etcd member communicates only with the kube-apiserver of this node.
- You should therefore run a minimum of three stacked control plane nodes for an HA cluster.

## Orange

- External etcd topology

## Yellow

- the kube-apiserver is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the kube-apiserver of each control plane node.
- However, this topology requires twice the number of hosts as the stacked HA topology. A minimum of three hosts for control plane nodes and three hosts for etcd nodes

# [Production environment | Kubernetes](https://kubernetes.io/docs/setup/production-environment/)

## Yellow

- Creating a highly available cluster means considering: Separating the control plane from the worker nodes. Replicating the control plane components on multiple nodes. Load balancing traffic to the cluster’s API server. Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.
- Before building a Kubernetes production environment on your own, consider handing off some or all of this job to Turnkey Cloud Solutions providers or other Kubernetes Partners. Options include: Serverless: Just run workloads on third-party equipment without managing a cluster at all. You will be charged for things like CPU usage, memory, and disk requests. Managed control plane: Let the provider manage the scale and availability of the cluster's control plane, as well as handle patches and upgrades. Managed worker nodes: Configure pools of nodes to meet your needs, then the provider makes sure those nodes are available and ready to implement upgrades when needed. Integration: There are providers that integrate Kubernetes with other services you may need, such as storage, container registries, authentication methods, and development tools.
- Production control plane
- Manage certificates
- Configure load balancer for apiserver
- Separate and backup etcd service
- Create multiple control plane systems
- running control plane services as pods in Kubernetes ensures that the replicated number of services that you request will always be available. The scheduler should be fault tolerant, but not highly available. Some deployment tools set up Raft consensus algorithm to do leader election of Kubernetes services. If the primary goes away, another service elects itself and take over.
- Span multiple zones: If keeping your cluster available at all times is critical, consider creating a cluster that runs across multiple data centers, referred to as zones in cloud environments. Groups of zones are referred to as regions. By spreading a cluster across multiple zones in the same region, it can improve the chances that your cluster will continue to function even if one zone becomes unavailable.
- Production worker nodes
- Configure nodes: Nodes can be physical or virtual machines. If you want to create and manage your own nodes, you can install a supported operating system, then add and run the appropriate Node services. Consider: The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available. Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation. Validate nodes: See Valid node setup for information on how to ensure that a node meets the requirements to join a Kubernetes cluster. Add nodes to the cluster: If you are managing your own cluster you can add nodes by setting up your own machines and either adding them manually or having them register themselves to the cluster’s apiserver. See the Nodes section for information on how to set up Kubernetes to add nodes in these ways. Scale nodes: Have a plan for expanding the capacity your cluster will eventually need. See Considerations for large clusters to help determine how many nodes you need, based on the number of pods and containers you need to run. If you are managing nodes yourself, this can mean purchasing and installing your own physical equipment. Autoscale nodes: Read Cluster Autoscaling to learn about the tools available to automatically manage your nodes and the capacity they provide. Set up node health checks: For important workloads, you want to make sure that the nodes and pods running on those nodes are healthy. Using the Node Problem Detector daemon, you can ensure your nodes are healthy.
- Taking on a production-quality cluster means deciding how you want to selectively allow access by other users. In particular, you need to select strategies for validating the identities of those who try to access your cluster (authentication) and deciding if they have permissions to do what they are asking (authorization):
- Authentication: The apiserver can authenticate users using client certificates, bearer tokens, an authenticating proxy, or HTTP basic auth.
- Using plugins, the apiserver can leverage your organization’s existing authentication methods, such as LDAP or Kerberos.
- Authorization: When you set out to authorize your regular users, you will probably choose between RBAC and ABAC authorization.
- Role-based access control (RBAC): Lets you assign access to your cluster by allowing specific sets of permissions to authenticated users. Permissions can be assigned for a specific namespace (Role) or across the entire cluster (ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached to particular users.
- Attribute-based access control (ABAC): Lets you create policies based on resource attributes in the cluster and will allow or deny access based on those attributes. Each line of a policy file identifies versioning properties (apiVersion and kind) and a map of spec properties to match the subject (user or group), resource property, non-resource property (/version or /apis), and readonly.
- Set the authorization mode: When the Kubernetes API server (kube-apiserver) starts, supported authorization modes must be set using an --authorization-config file or the --authorization-mode flag.
- For example, that flag in the kube-adminserver.yaml file (in /etc/kubernetes/manifests) could be set to Node,RBAC.
- Webhooks and other special authorization types need to be enabled by adding Admission Controllers to the API server.
- Set namespace limits: Set per-namespace quotas on things like memory and CPU.
- Prepare for DNS demand
- Create additional service accounts: User accounts determine what users can do on a cluster, while a service account defines pod access within a particular namespace. By default, a pod takes on the default service account from its namespace.

# [Running in multiple zones | Kubernetes](https://kubernetes.io/docs/setup/best-practices/multiple-zones/)

## Yellow

- Kubernetes is designed so that a single Kubernetes cluster can run across multiple failure zones, typically where these zones fit within a logical grouping called a region. Major cloud providers define a region as a set of failure zones (also called availability zones) that provide a consistent set of features: within a region, each zone offers the same APIs and services. Typical cloud architectures aim to minimize the chance that a failure in one zone also impairs services in another zone.
- All control plane components support running as a pool of interchangeable resources, replicated per component. When you deploy a cluster control plane, place replicas of control plane components across multiple failure zones. If availability is an important concern, select at least three failure zones and replicate each individual control plane component (API server, scheduler, etcd, cluster controller manager) across at least three failure zones. If you are running a cloud controller manager then you should also replicate this across all the failure zones you selected.
- Kubernetes automatically spreads the Pods for workload resources (such as Deployment or StatefulSet) across different nodes in a cluster. This spreading helps reduce the impact of failures. When nodes start up, the kubelet on each node automatically adds labels to the Node object that represents that specific kubelet in the Kubernetes API. These labels can include zone information.
- If your cluster spans multiple zones or regions, you can use node labels in conjunction with Pod topology spread constraints to control how Pods are spread across your cluster among fault domains
- Distributing nodes across zones Kubernetes' core does not create nodes for you; you need to do that yourself, or use a tool such as the Cluster API to manage nodes on your behalf. Using tools such as the Cluster API you can define sets of machines to run as worker nodes for your cluster across multiple failure domains, and rules to automatically heal the cluster in case of whole-zone service disruption.

---

Created with Super Simple Highlighter. ©2010-24 [Dexterous Logic software](https://www.dexterouslogic.com/)