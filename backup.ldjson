{"magic":"Super Simple Highlighter Exported Database","version":1,"subversion":"26"}
{"highlightDefinitions":[{"blockquote":false,"blur":0,"bold":false,"className":"default-red-aa94e3d5-ab2f-4205-b74e-18ce31c7c0ce","default":true,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#fecaca","color":"#7f1d1d"},"text_color_equals_background":false,"title":"Red","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#fed7aa","color":"#7c2d12"},"text_color_equals_background":false,"title":"Orange","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#fef08a","color":"#713f12"},"text_color_equals_background":false,"title":"Yellow","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-green-c4d41e0a-e40f-4c3f-91ad-2d66481614c2","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#bbf7d0","color":"#14532d"},"text_color_equals_background":false,"title":"Green","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-cyan-f88e8827-e652-4d79-a9d9-f6c8b8ec9e2b","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#a5f3fc","color":"#164e63"},"text_color_equals_background":false,"title":"Cyan","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-purple-c472dcdb-f2b8-41ab-bb1e-2fb293df172a","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#e9d5ff","color":"#581c87"},"text_color_equals_background":false,"title":"Purple","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"default-grey-da7cb902-89c6-46fe-b0e7-d3b35aaf237a","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#e5e7eb","color":"#111827"},"text_color_equals_background":false,"title":"Grey","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"c0df1986-e8fb-4981-8924-ba8c207c8041","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":true,"scale":1,"strikethrough":false,"style":{"background-color":"#404040","color":"#000000"},"text_color_equals_background":true,"title":"Redact","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"a60a98d6-946d-46a9-9044-9a97db57c7d1","default":false,"drop_shadow":false,"hidden":true,"inherit_style_background_color":true,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#dc2626"},"text_color_equals_background":false,"title":"Red Text","underline":false},{"blockquote":false,"blur":0,"bold":true,"className":"ad1b8cd8-ab73-47dd-97a7-8344bf6db723","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Bold","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"df4a0c7b-c9e2-4583-b09e-8eb4c71382a1","default":false,"drop_shadow":false,"hidden":true,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":true,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Italic","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"c5b8582a-b9d7-421f-b027-1587b1f5367e","default":false,"drop_shadow":false,"hidden":true,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Underline","underline":true},{"blockquote":false,"blur":0,"bold":false,"className":"def33222-aa74-4df1-894a-59a882dd26fb","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":true,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Strikethrough","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"c6bab7ff-c90f-4f0f-8a44-51c32015d7bc","default":false,"drop_shadow":false,"hidden":true,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":true,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Fixed-width","underline":false},{"blockquote":false,"blur":1,"bold":false,"className":"a1b2b84c-1c56-44d3-aed1-b2137617587f","default":false,"drop_shadow":false,"hidden":true,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Blur","underline":false},{"blockquote":false,"blur":0,"bold":false,"className":"a2fd0e9f-02a3-496e-9f87-cd8ec0bd1edb","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":true,"inherit_style_color":true,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":2,"strikethrough":false,"style":{"background-color":"#ffff00","color":"#000000"},"text_color_equals_background":false,"title":"Magnify","underline":false},{"blockquote":true,"blur":0,"bold":false,"className":"ab9cf480-775e-436a-ad0a-08f8ba0d2136","default":false,"drop_shadow":false,"hidden":false,"inherit_style_background_color":false,"inherit_style_color":false,"invert":false,"italic":false,"monospace":false,"no_user_select":false,"scale":1,"strikethrough":false,"style":{"background-color":"#f5f5f4","color":"#57534e"},"text_color_equals_background":false,"title":"Quotation","underline":false}],"sharedHighlightStyle":{}}
{"version":"1.2.9","db_type":"idb","start_time":"2025-02-04T09:06:26.819Z","db_info":{"doc_count":692,"update_seq":714,"idb_attachment_format":"binary","db_name":"sos","auto_compaction":true,"adapter":"idb"}}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166606811,"correspondingDocumentId":"a295556c9afec9459d6f826d8667016e","_id":"373426d3-d465-40ea-bbc7-db942d59063c","_rev":"1-3f521c1e6dd4e9b3cb64281a91e32699","_revisions":{"start":1,"ids":["3f521c1e6dd4e9b3cb64281a91e32699"]}},{"verb":"delete","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166617904,"correspondingDocumentId":"b805a20f88e950ce18d455d25cbb1198","_id":"480b2f3c-1046-4392-8da5-47c41c3d05a7","_rev":"1-9f74ef5aee0f2afcb04c286bdd3987a9","_revisions":{"start":1,"ids":["9f74ef5aee0f2afcb04c286bdd3987a9"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736153347695,"range":{"startContainerPath":"//h3[@id=\"production-control-plane\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"production-control-plane\"]/text()","endOffset":24},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Production control plane","v":"26","_id":"a222bdfaa4bcef802f23bdfe96d2bf33","_rev":"1-b6db4d98b8ab4c62af72a2387e003b06","_revisions":{"start":1,"ids":["b6db4d98b8ab4c62af72a2387e003b06"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166581316,"range":{"startContainerPath":"//h2[@id=\"production-considerations\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"production-considerations\"]/text()","endOffset":25},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Production considerations","v":"26","_id":"a295556c9afec9459d6f826d8667016e","_rev":"1-b3d7f491e13941008c752cf1062dd478","_revisions":{"start":1,"ids":["b3d7f491e13941008c752cf1062dd478"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/","date":1736172799208,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/h1/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/h1/text()","endOffset":43},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Customizing components with the kubeadm API","v":"26","first":true,"title":"Customizing components with the kubeadm API | Kubernetes","_id":"a2cfb86d0ff8df4673ddd8d7d51bbb43","_rev":"1-0fc2523412a09e57ae0664e8f552c2e1","_revisions":{"start":1,"ids":["0fc2523412a09e57ae0664e8f552c2e1"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166627568,"range":{"startContainerPath":"//h3[@id=\"production-worker-nodes\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"production-worker-nodes\"]/text()","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Production worker nodes","v":"26","_id":"a2eb6e49719071963399d1de30c2572a","_rev":"1-7149179e519e67d5d2da40dad9274f40","_revisions":{"start":1,"ids":["7149179e519e67d5d2da40dad9274f40"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172129171,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":165},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"By default, the Linux kernel does not allow IPv4 packets to be routed between interfaces. Most Kubernetes cluster networking implementations will change this setting","v":"26","_id":"a6b94bda900241524e56a43a4491e18e","_rev":"1-ad870e9fc6a1acb9d68bd942190e6089","_revisions":{"start":1,"ids":["ad870e9fc6a1acb9d68bd942190e6089"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172032019,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":58},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You need to install a container runtime into each node in the cluster so that Pods can run there.","v":"26","first":true,"title":"Container Runtimes | Kubernetes","_id":"a7320e3e0f37676e1cc093157c658b03","_rev":"1-9098958e964e65f5b0234e7c7f9fffa4","_revisions":{"start":1,"ids":["9098958e964e65f5b0234e7c7f9fffa4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169930201,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li/text()[4]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Set the authorization mode: When the Kubernetes API server (kube-apiserver) starts, supported authorization modes must be set using an --authorization-config file or the --authorization-mode flag.","v":"26","_id":"aa4dbd3ddac467f29e016dd97a826f12","_rev":"1-98a76c11009afbd2ecd6eca5b4cb13af","_revisions":{"start":1,"ids":["98a76c11009afbd2ecd6eca5b4cb13af"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736172990009,"range":{"startContainerPath":"//h2[@id=\"stacked-etcd-topology\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"stacked-etcd-topology\"]/text()","endOffset":21},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Stacked etcd topology","v":"26","_id":"af63062c9aa0420045da972e05df0e98","_rev":"1-2de19f7cfe4c8afdabb80bdfb9cf0cb9","_revisions":{"start":1,"ids":["2de19f7cfe4c8afdabb80bdfb9cf0cb9"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169958392,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li/text()","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li/text()[3]","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, that flag in the kube-adminserver.yaml file (in /etc/kubernetes/manifests) could be set to Node,RBAC.","v":"26","_id":"af9aff7f46c3a6453bad6d6e0e443e76","_rev":"1-e9e3c2c10af4b1faed780a64d1e8a18c","_revisions":{"start":1,"ids":["e9e3c2c10af4b1faed780a64d1e8a18c"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166702041,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[6]/text()[2]","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Configure nodes: Nodes can be physical or virtual machines. If you want to create and manage your own nodes, you can install a supported operating system, then add and run the appropriate Node services. Consider: The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available. Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation. Validate nodes: See Valid node setup for information on how to ensure that a node meets the requirements to join a Kubernetes cluster. Add nodes to the cluster: If you are managing your own cluster you can add nodes by setting up your own machines and either adding them manually or having them register themselves to the cluster’s apiserver. See the Nodes section for information on how to set up Kubernetes to add nodes in these ways. Scale nodes: Have a plan for expanding the capacity your cluster will eventually need. See Considerations for large clusters to help determine how many nodes you need, based on the number of pods and containers you need to run. If you are managing nodes yourself, this can mean purchasing and installing your own physical equipment. Autoscale nodes: Read Cluster Autoscaling to learn about the tools available to automatically manage your nodes and the capacity they provide. Set up node health checks: For important workloads, you want to make sure that the nodes and pods running on those nodes are healthy. Using the Node Problem Detector daemon, you can ensure your nodes are healthy.","v":"26","_id":"b77dc03289ae229656f76693048e2c08","_rev":"1-2a19d2fbc41257be80509e7c8bf5dd0d","_revisions":{"start":1,"ids":["2a19d2fbc41257be80509e7c8bf5dd0d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166484408,"range":{"startContainerPath":"//h3[@id=\"production-worker-nodes\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"production-worker-nodes\"]/text()","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Production worker nodes","v":"26","_id":"b805a20f88e950ce18d455d25cbb1198","_rev":"1-73cc0f25d27fdcb1d6ce34e59565247c","_revisions":{"start":1,"ids":["73cc0f25d27fdcb1d6ce34e59565247c"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169628965,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/ul/li[2]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/ul/li[2]/text()[2]","endOffset":353},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Attribute-based access control (ABAC): Lets you create policies based on resource attributes in the cluster and will allow or deny access based on those attributes. Each line of a policy file identifies versioning properties (apiVersion and kind) and a map of spec properties to match the subject (user or group), resource property, non-resource property (/version or /apis), and readonly.","v":"26","_id":"b8fbf50ab6b146cc49dcd462fb5db7bf","_rev":"1-424e69c875345990e002727845fdce21","_revisions":{"start":1,"ids":["424e69c875345990e002727845fdce21"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736153367656,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/em/text()","endOffset":37},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Configure load balancer for apiserver","v":"26","_id":"bae5be606fbc37a688da4e9e23c79458","_rev":"1-4ead51d54918563c0266185f31e7a825","_revisions":{"start":1,"ids":["4ead51d54918563c0266185f31e7a825"]}},{"verb":"delete","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166608818,"correspondingDocumentId":"e91728d0807ee8313b8b0cb4fa2c3384","_id":"bcbec0cf-ec5d-44f1-861f-b56cc685cc85","_rev":"1-e907c3eaf4d6e9fe0a602dfeab01d3a4","_revisions":{"start":1,"ids":["e907c3eaf4d6e9fe0a602dfeab01d3a4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736151755458,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/p/text()[2]","startOffset":32,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/ul/li[4]/text()","endOffset":108},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Creating a highly available cluster means considering:  Separating the control plane from the worker nodes. Replicating the control plane components on multiple nodes. Load balancing traffic to the cluster’s API server. Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.","v":"26","first":true,"title":"Production environment | Kubernetes","_id":"c178eef5d5f91cd91ec6136c118cfc32","_rev":"1-9f7b4cd8f9d242092430048ef7b6ae00","_revisions":{"start":1,"ids":["9f7b4cd8f9d242092430048ef7b6ae00"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736173105880,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","endOffset":90},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You should therefore run a minimum of three stacked control plane nodes for an HA cluster.","v":"26","_id":"c1a1da41a5f372f22ebd40ef2186ad94","_rev":"1-617e689d17f92652a77198bf3a83592d","_revisions":{"start":1,"ids":["617e689d17f92652a77198bf3a83592d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736153360049,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/em/text()","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Manage certificates","v":"26","_id":"c337eb5b5803b5d5175834446b085606","_rev":"1-03edaf259ca3bb3e7a6eeadd3ad7b53c","_revisions":{"start":1,"ids":["03edaf259ca3bb3e7a6eeadd3ad7b53c"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172188301,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[2]/a/code/text()","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Both the kubelet and the underlying container runtime need to interface with control groups to enforce resource management for pods and containers and set resources such as cpu/memory requests and limits. To interface with control groups, the kubelet and the container runtime need to use a cgroup driver. It's critical that the kubelet and the container runtime use the same cgroup driver and are configured the same.  There are two cgroup drivers available:  cgroupfs systemd","v":"26","_id":"c3404b9d943e540bf68dba673340f2f9","_rev":"1-3bd7ad96b90b06d169bd7af15e35c787","_revisions":{"start":1,"ids":["3bd7ad96b90b06d169bd7af15e35c787"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736153386414,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/em/text()","endOffset":37},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Create multiple control plane systems","v":"26","_id":"c421fb115b8b02d5dc95f4a1e7df220c","_rev":"1-d6166aa290eec2b8bbb1a7f844700da4","_revisions":{"start":1,"ids":["d6166aa290eec2b8bbb1a7f844700da4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169598765,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/ul/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/ul/li/text()[2]","endOffset":314},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Role-based access control (RBAC): Lets you assign access to your cluster by allowing specific sets of permissions to authenticated users. Permissions can be assigned for a specific namespace (Role) or across the entire cluster (ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached to particular users.","v":"26","_id":"c6d435fb18c1fdeabd351947ff0d7d41","_rev":"1-e5f3f562fe26756bee1e4ea11ed25c47","_revisions":{"start":1,"ids":["e5f3f562fe26756bee1e4ea11ed25c47"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169503946,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li/text()","startOffset":62,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li/text()","endOffset":182},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Using plugins, the apiserver can leverage your organization’s existing authentication methods, such as LDAP or Kerberos.","v":"26","_id":"c7736bcf54f1077cabcaf2648c718602","_rev":"1-d34672c480232416ee181bc649a628db","_revisions":{"start":1,"ids":["d34672c480232416ee181bc649a628db"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169493819,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li/text()","endOffset":125},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Authentication: The apiserver can authenticate users using client certificates, bearer tokens, an authenticating proxy, or HTTP basic auth.","v":"26","_id":"c91fe6f2a7e423ded65e8a72fa3a7fd0","_rev":"1-a8bd3be683282d48753e80913320bfd4","_revisions":{"start":1,"ids":["a8bd3be683282d48753e80913320bfd4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736170142402,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li/text()","endOffset":57},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Set namespace limits: Set per-namespace quotas on things like memory and CPU.","v":"26","_id":"ca84f7c7747fd0c99a74f8e74459dfb8","_rev":"1-ed73f69c452120fbbc8182b4347d521e","_revisions":{"start":1,"ids":["ed73f69c452120fbbc8182b4347d521e"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172397207,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":92},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Two cgroup managers result in two views of the available and in-use resources in the system.","v":"26","_id":"ce11f49a4c3a9f4713b76e75dcf51905","_rev":"1-9fbc0372b895d41194a31a4b1b50d9a6","_revisions":{"start":1,"ids":["9fbc0372b895d41194a31a4b1b50d9a6"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736173064508,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":14},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each control plane node creates a local etcd member and this etcd member communicates only with the kube-apiserver of this node.","v":"26","_id":"ce1bdc448c5ffcb22159ed7324122ecc","_rev":"1-0dbd5813d856b2f86f1e875118c7bb36","_revisions":{"start":1,"ids":["0dbd5813d856b2f86f1e875118c7bb36"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172150101,"range":{"startContainerPath":"//h2[@id=\"cgroup-drivers\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]","endOffset":0},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"cgroup drivers","v":"26","_id":"cebe13e8289423813b7e574fea23f4bb","_rev":"1-bb4a55fc92b17bed8e5fcaa004be774d","_revisions":{"start":1,"ids":["bb4a55fc92b17bed8e5fcaa004be774d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736152083335,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[4]/text()","endOffset":167},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Before building a Kubernetes production environment on your own, consider handing off some or all of this job to Turnkey Cloud Solutions providers or other Kubernetes Partners. Options include:  Serverless: Just run workloads on third-party equipment without managing a cluster at all. You will be charged for things like CPU usage, memory, and disk requests. Managed control plane: Let the provider manage the scale and availability of the cluster's control plane, as well as handle patches and upgrades. Managed worker nodes: Configure pools of nodes to meet your needs, then the provider makes sure those nodes are available and ready to implement upgrades when needed. Integration: There are providers that integrate Kubernetes with other services you may need, such as storage, container registries, authentication methods, and development tools.","v":"26","_id":"cf5349c1c4b954ba872e64c8d8b12d49","_rev":"1-a48f4ed45dbdbd2292bda4abf2baa2b4","_revisions":{"start":1,"ids":["a48f4ed45dbdbd2292bda4abf2baa2b4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239172938,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":73},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The lifecycle of the kubeadm CLI tool is decoupled from the kubelet, which is a daemon that runs on each node within the Kubernetes cluster.","v":"26","first":true,"title":"Configuring each kubelet in your cluster using kubeadm | Kubernetes","_id":"cfb63a7936100b08ca7f310d23044583","_rev":"1-4a8cce78aacaa6f83a47cb181df08e7f","_revisions":{"start":1,"ids":["4a8cce78aacaa6f83a47cb181df08e7f"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/","date":1736172806588,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()[4]","endOffset":12},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually patch the kube-system/coredns ConfigMap and recreate the CoreDNS Pods after that.","v":"26","_id":"cff121b113af862a2eccd5a7eceace2f","_rev":"1-cda43762d3eea5aea610ca94a78ff376","_revisions":{"start":1,"ids":["cda43762d3eea5aea610ca94a78ff376"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736154121288,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/text()","startOffset":223,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/text()[2]","endOffset":137},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"running control plane services as pods in Kubernetes ensures that the replicated number of services that you request will always be available. The scheduler should be fault tolerant, but not highly available. Some deployment tools set up Raft consensus algorithm to do leader election of Kubernetes services. If the primary goes away, another service elects itself and take over.","v":"26","_id":"d126b2570a6c739b271f206aa48e7ff7","_rev":"1-341a1843ba74c8f87d29be35e896c200","_revisions":{"start":1,"ids":["341a1843ba74c8f87d29be35e896c200"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736172944053,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","endOffset":82},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can set up an HA cluster:  With stacked control plane nodes, where etcd nodes are colocated with control plane nodes With external etcd nodes, where etcd runs on separate nodes from the control plane","v":"26","first":true,"title":"Options for Highly Available Topology | Kubernetes","_id":"d22c0d377a680f8b7f00f794db8261b7","_rev":"1-c41a5040a8d3c5b1d2bf88806913fb1d","_revisions":{"start":1,"ids":["c41a5040a8d3c5b1d2bf88806913fb1d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736153375639,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/em/text()","endOffset":32},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Separate and backup etcd service","v":"26","_id":"d2d074e7b08fe7e46b2e24a86358543c","_rev":"1-75f274938ed3c57f29d34d136b41778c","_revisions":{"start":1,"ids":["75f274938ed3c57f29d34d136b41778c"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736170026350,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[4]/text()[2]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[4]/text()[3]","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Webhooks and other special authorization types need to be enabled by adding Admission Controllers to the API server.","v":"26","_id":"d4016c51bac084e98c4bdcd49d491f1b","_rev":"1-e3b7ae8561140c791e982ec166d3cd4b","_revisions":{"start":1,"ids":["e3b7ae8561140c791e982ec166d3cd4b"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736170205104,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[3]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[3]/text()","endOffset":206},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Create additional service accounts: User accounts determine what users can do on a cluster, while a service account defines pod access within a particular namespace. By default, a pod takes on the default service account from its namespace.","v":"26","_id":"d5ac05b4031c670301a0275c2cd51be7","_rev":"1-751141e5360ceb3d42b4039f1f4d92d8","_revisions":{"start":1,"ids":["751141e5360ceb3d42b4039f1f4d92d8"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/","date":1736151374154,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[5]","endOffset":20},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Several Kubernetes components such as kube-apiserver or kube-proxy can also be deployed as container images within the cluster.","v":"26","first":true,"title":"Getting started | Kubernetes","_id":"d68265fe6cf8f0730d998b4cc03b71e8","_rev":"1-47c1de430c8d7f1addf3b792082d769e","_revisions":{"start":1,"ids":["47c1de430c8d7f1addf3b792082d769e"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736173113873,"range":{"startContainerPath":"//h2[@id=\"external-etcd-topology\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"external-etcd-topology\"]/text()","endOffset":22},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"External etcd topology","v":"26","_id":"d6bb40692e9328b1285683d0648cadc9","_rev":"1-be61a8087625bbafb38e0592f26e35ea","_revisions":{"start":1,"ids":["be61a8087625bbafb38e0592f26e35ea"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736173160794,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[4]","startOffset":6,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[6]","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"the kube-apiserver is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the kube-apiserver of each control plane node.","v":"26","_id":"d8b682b643faa95f54b744c203cf70ce","_rev":"1-ad164a0e32e5c9a9c43f819e81837627","_revisions":{"start":1,"ids":["ad164a0e32e5c9a9c43f819e81837627"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/","date":1736173228833,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":165},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"However, this topology requires twice the number of hosts as the stacked HA topology. A minimum of three hosts for control plane nodes and three hosts for etcd nodes","v":"26","_id":"d926cb12ab6e8449fbd9d1307a978e3f","_rev":"1-a61c20e1fbd50b65fefda1fbf9021fc4","_revisions":{"start":1,"ids":["a61c20e1fbd50b65fefda1fbf9021fc4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169483003,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":324},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Taking on a production-quality cluster means deciding how you want to selectively allow access by other users. In particular, you need to select strategies for validating the identities of those who try to access your cluster (authentication) and deciding if they have permissions to do what they are asking (authorization):","v":"26","_id":"da05ee2d0b338653b566718a83326f70","_rev":"1-be0cbd27e3b969a1826fc60c54dd0b24","_revisions":{"start":1,"ids":["be0cbd27e3b969a1826fc60c54dd0b24"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/","date":1736172106437,"range":{"startContainerPath":"//h3[@id=\"network-configuration\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"network-configuration\"]/text()","endOffset":21},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Network configuration","v":"26","_id":"de017c5f8f91828d41bc7d994c3c4efa","_rev":"1-53bfa92908f5564bbc274ecdeaa2fb2d","_revisions":{"start":1,"ids":["53bfa92908f5564bbc274ecdeaa2fb2d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736154416498,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[6]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[6]/text()","endOffset":391},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Span multiple zones: If keeping your cluster available at all times is critical, consider creating a cluster that runs across multiple data centers, referred to as zones in cloud environments. Groups of zones are referred to as regions. By spreading a cluster across multiple zones in the same region, it can improve the chances that your cluster will continue to function even if one zone becomes unavailable.","v":"26","_id":"e15461bc5a8a2a94dabe3919a2208e15","_rev":"1-9866fede169cab2fb0f8306fddc7df06","_revisions":{"start":1,"ids":["9866fede169cab2fb0f8306fddc7df06"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736166590764,"range":{"startContainerPath":"//h2[@id=\"production-cluster-setup\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"production-cluster-setup\"]/text()","endOffset":24},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Production cluster setup","v":"26","_id":"e91728d0807ee8313b8b0cb4fa2c3384","_rev":"1-9cc6e43757dce6abaaf12c13892f3af0","_revisions":{"start":1,"ids":["9cc6e43757dce6abaaf12c13892f3af0"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736169545391,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[2]/text()","endOffset":113},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Authorization: When you set out to authorize your regular users, you will probably choose between RBAC and ABAC authorization.","v":"26","_id":"eb2c6c0b46e71d13516a791ee6deca1f","_rev":"1-e0a7d983f6d8fa1a2578f109d5180b96","_revisions":{"start":1,"ids":["e0a7d983f6d8fa1a2578f109d5180b96"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/","date":1736170160516,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[2]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[2]/em/text()","endOffset":22},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Prepare for DNS demand","v":"26","_id":"ee281602fd28051a0c88053cdc7ba28a","_rev":"1-982ed0aaa0ed6df7e8c09888b6533c0f","_revisions":{"start":1,"ids":["982ed0aaa0ed6df7e8c09888b6533c0f"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239201277,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":275,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":4},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can manage the configuration of your kubelets manually, but kubeadm now provides a KubeletConfiguration API","v":"26","_id":"f7601aefe8f03167519e981ea6fd0157","_rev":"1-f2e7152c9219b4503b5fbbb93b1f1f70","_revisions":{"start":1,"ids":["f2e7152c9219b4503b5fbbb93b1f1f70"]}}]}

{"seq":51}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326989359,"correspondingDocumentId":"a97d6c3951c14a10d92adfd3f67f83ed","_id":"086d8480-856d-489b-873a-87d61bb21a9f","_rev":"1-e286d128bb4eb57d39eb4c2c2c3ac917","_revisions":{"start":1,"ids":["e286d128bb4eb57d39eb4c2c2c3ac917"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326886662,"correspondingDocumentId":"a87a5a40af5f49a0881db1ac1ab90957","_id":"963eab1c-d71b-472e-b1b5-b5819e678c27","_rev":"1-51734958ed5740d972d3eaaf09251a53","_revisions":{"start":1,"ids":["51734958ed5740d972d3eaaf09251a53"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247824304,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","startOffset":94,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[5]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If you expect workload authors to make changes to fit within the desired level, enable the warn mode. If you expect to use audit logs to monitor/drive changes to fit within the desired level, enable the audit mode.","v":"26","_id":"a37c00acfc05a6e894db15abdfebe147","_rev":"1-f26a6c94eea139f78f6d4a5c0ea0b7eb","_revisions":{"start":1,"ids":["f26a6c94eea139f78f6d4a5c0ea0b7eb"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244775467,"range":{"startContainerPath":"//h3[@id=\"distributing-nodes-across-zones\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","endOffset":226},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Distributing nodes across zones Kubernetes' core does not create nodes for you; you need to do that yourself, or use a tool such as the Cluster API to manage nodes on your behalf.  Using tools such as the Cluster API you can define sets of machines to run as worker nodes for your cluster across multiple failure domains, and rules to automatically heal the cluster in case of whole-zone service disruption.","v":"26","_id":"a68b19f3123d26d8148bd87dccf37d50","_rev":"1-25d6b9593edbeb8f8c2fe3f67bc07f67","_revisions":{"start":1,"ids":["25d6b9593edbeb8f8c2fe3f67bc07f67"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326878433,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl[2]/dd[3]/text()","endOffset":44},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Run on every node, maintaining running pods and providing the Kubernetes runtime environment:  kubelet Ensures that Pods are running, including their containers. kube-proxy (optional) Maintains network rules on nodes to implement Services. Container runtime Software responsible for running containers.","v":"26","_id":"a87a5a40af5f49a0881db1ac1ab90957","_rev":"1-061894cc46e7cc0aea5e0c67ec6e18bc","_revisions":{"start":1,"ids":["061894cc46e7cc0aea5e0c67ec6e18bc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326986992,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Your cluster may require additional software on each node; for example, you might also run systemd on a Linux node to supervise local components.","v":"26","_id":"a97d6c3951c14a10d92adfd3f67f83ed","_rev":"1-675087b3d39124165c5f9a328a4d68bd","_revisions":{"start":1,"ids":["675087b3d39124165c5f9a328a4d68bd"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736241990421,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[5]","endOffset":115},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For a large cluster, you need a control plane with sufficient compute and other resources.  Typically you would run one or two control plane instances per failure zone, scaling those instances vertically first and then scaling horizontally after reaching the point of falling returns to (vertical) scale.  You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes nodes do not automatically steer traffic towards control-plane endpoints that are in the same failure zone; however, your cloud provider might have its own mechanisms to do this.  For example, using a managed load balancer, you configure the load balancer to send traffic that originates from the kubelet and Pods in failure zone A, and direct that traffic only to the control plane hosts that are also in zone A. If a single control-plane host or endpoint failure zone A goes offline, that means that all the control-plane traffic for nodes in zone A is now being sent between zones. Running multiple control plane hosts in each zone makes that outcome less likely.","v":"26","_id":"a9d20cf55b5eb74cdb8c1ca8c8cd31f2","_rev":"1-89561457e5b790115fc3ca812593ee7f","_revisions":{"start":1,"ids":["89561457e5b790115fc3ca812593ee7f"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247816464,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[3]","endOffset":162},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The audit and warn modes of the Pod Security Standards admission controller make it easy to collect important security insights about your pods without breaking existing workloads.","v":"26","_id":"abf4cfcee2bbdd76e320fcadf4920233","_rev":"1-d215231864c84adc5fbd2c424369b656","_revisions":{"start":1,"ids":["d215231864c84adc5fbd2c424369b656"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736241477875,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/pre/code/span/span/text()[2]","endOffset":26},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"After marshalling these two files to disk, kubeadm attempts to run the following two commands, if you are using systemd:  systemctl daemon-reload && systemctl restart kubelet","v":"26","_id":"ad0318bd5f2d43055a77a592cd63eaa2","_rev":"1-4f7fef3ec5e5bc5ef98902fc363726ea","_revisions":{"start":1,"ids":["4f7fef3ec5e5bc5ef98902fc363726ea"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239374880,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/pre/code/span/span/text()","endOffset":40},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If you want your services to use the subnet 10.96.0.0/12 as the default for services, you can pass the --service-cidr parameter to kubeadm:  kubeadm init --service-cidr 10.96.0.0/12","v":"26","_id":"ad775f1f8c80caec7b438c2884ac451f","_rev":"1-f5368f32e2a6f6868599dd53e261b3e5","_revisions":{"start":1,"ids":["f5368f32e2a6f6868599dd53e261b3e5"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/certificates/","date":1736248014824,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()[3]","endOffset":13},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes requires PKI for the following operations:  Server certificates Server certificate for the API server endpoint Server certificate for the etcd server Server certificates for each kubelet (every node runs a kubelet) Optional server certificate for the front-proxy Client certificates Client certificates for each kubelet, used to authenticate to the API server as a client of the Kubernetes API Client certificate for each API server, used to authenticate to etcd Client certificate for the controller manager to securely communicate with the API server Client certificate for the scheduler to securely communicate with the API server Client certificates, one for each node, for kube-proxy to authenticate to the API server Optional client certificates for administrators of the cluster to authenticate to the API server Optional client certificate for the front-proxy Kubelet's server and client certificates To establish a secure connection and authenticate itself to the kubelet, the API Server requires a client certificate and key pair.  In this scenario, there are two approaches for certificate usage:  Shared Certificates: The kube-apiserver can utilize the same certificate and key pair it uses to authenticate its clients. This means that the existing certificates, such as apiserver.crt and apiserver.key, can be used for communicating with the kubelet servers.  Separate Certificates: Alternatively, the kube-apiserver can generate a new client certificate and key pair to authenticate its communication with the kubelet servers. In this case, a distinct certificate named kubelet-client.crt and its corresponding private key, kubelet-client.key are created.","v":"26","_id":"ae81355d2fc64763975ba126c70c9ca7","_rev":"1-7b4261de22553e2384cb43e504fbb1e7","_revisions":{"start":1,"ids":["7b4261de22553e2384cb43e504fbb1e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/certificates/","date":1736248120460,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":40,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/code/text()","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"most certificates are stored in /etc/kubernetes/pki","v":"26","_id":"b0f57800eb96b09d27e8f2eba0ab3a13","_rev":"1-8a9ce3248b80584fd37bab76b62a7355","_revisions":{"start":1,"ids":["8a9ce3248b80584fd37bab76b62a7355"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244251275,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":120},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes is designed so that a single Kubernetes cluster can run across multiple failure zones, typically where these zones fit within a logical grouping called a region. Major cloud providers define a region as a set of failure zones (also called availability zones) that provide a consistent set of features: within a region, each zone offers the same APIs and services.  Typical cloud architectures aim to minimize the chance that a failure in one zone also impairs services in another zone.","v":"26","first":true,"title":"Running in multiple zones | Kubernetes","_id":"b4563183fd456fbb2d8ba7f35c2f1dfe","_rev":"1-97ee6f6b3567b20e56b578f8b92eb27b","_revisions":{"start":1,"ids":["97ee6f6b3567b20e56b578f8b92eb27b"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239392020,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":61,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You also need to set the DNS address used by the kubelet, using the --cluster-dns flag.","v":"26","_id":"b49386de700c0a14f912330947d9f491","_rev":"1-46669bc329cb1e9f76e102535748c843","_revisions":{"start":1,"ids":["46669bc329cb1e9f76e102535748c843"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736242194103,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/text()[2]","endOffset":29},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To improve performance of large clusters, you can store Event objects in a separate dedicated etcd instance.  When creating a cluster, you can (using custom tooling):  start and configure additional etcd instance configure the API server to use it for storing events","v":"26","_id":"b4a9e2982c90e24100adab44baeec4bc","_rev":"1-01632054363e418c2671d7ed58a7ca60","_revisions":{"start":1,"ids":["01632054363e418c2671d7ed58a7ca60"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247901254,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Other alternatives for enforcing security profiles are being developed in the Kubernetes ecosystem:  Kubewarden. Kyverno. OPA Gatekeeper.","v":"26","_id":"b4fd17a14720911510a4e493cdab84f1","_rev":"1-f80af83b52dfd534e41549f142c27639","_revisions":{"start":1,"ids":["f80af83b52dfd534e41549f142c27639"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326894149,"range":{"startContainerPath":"//h3[@id=\"node-components\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl[2]/dd[3]/text()","endOffset":44},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node Components Run on every node, maintaining running pods and providing the Kubernetes runtime environment:  kubelet Ensures that Pods are running, including their containers. kube-proxy (optional) Maintains network rules on nodes to implement Services. Container runtime Software responsible for running containers.","v":"26","_id":"b592d2f4935cf1ced492187068b05a77","_rev":"1-2d5e6a313361b5f411f891c1c0d1fbf6","_revisions":{"start":1,"ids":["2d5e6a313361b5f411f891c1c0d1fbf6"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244646542,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()[2]","endOffset":71},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If your cluster spans multiple zones or regions, you can use node labels in conjunction with Pod topology spread constraints to control how Pods are spread across your cluster among fault domains","v":"26","_id":"b65535b49b0605936f05dbf5cfd02618","_rev":"1-c7dde7a1b5fdae480f40e3fe80f3430f","_revisions":{"start":1,"ids":["c7dde7a1b5fdae480f40e3fe80f3430f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/","date":1736252651871,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":97},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production workloads at scale with best-of-breed ideas and practices from the community.","v":"26","_id":"b6e723f4e2b2afe5ae91eef0ff0a25aa","_rev":"1-bc7e58ca4fcdcb61f74ffa5cd417c16a","_revisions":{"start":1,"ids":["bc7e58ca4fcdcb61f74ffa5cd417c16a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/","date":1736252582307,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","endOffset":173},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.","v":"26","first":true,"title":"Overview | Kubernetes","_id":"b6ed291a4d829bd21345e3cac1161561","_rev":"1-8d1d701bca9d0e7650597869d5bdb5b4","_revisions":{"start":1,"ids":["8d1d701bca9d0e7650597869d5bdb5b4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326842891,"range":{"startContainerPath":"//h3[@id=\"control-plane-components\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[5]/text()","endOffset":45},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Control Plane Components Manage the overall state of the cluster:  kube-apiserver The core component server that exposes the Kubernetes HTTP API etcd Consistent and highly-available key value store for all API server data kube-scheduler Looks for Pods not yet bound to a node, and assigns each Pod to a suitable node. kube-controller-manager Runs controllers to implement Kubernetes API behavior. cloud-controller-manager (optional) Integrates with underlying cloud provider(s).","v":"26","_id":"b87c07f8bc52977707261c93e6570f42","_rev":"1-863c2707b8e2f98fa5ed796cc22fb08e","_revisions":{"start":1,"ids":["863c2707b8e2f98fa5ed796cc22fb08e"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247698533,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","endOffset":200},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For workloads running in those permissive namespaces, maintain documentation about their unique security requirements. If at all possible, consider how those requirements could be further constrained.","v":"26","_id":"b8b794696e4656b5c5279f204e0ddf19","_rev":"1-192c22ecbfee021bbc798a3ab02a8ce3","_revisions":{"start":1,"ids":["192c22ecbfee021bbc798a3ab02a8ce3"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736242488049,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[3]/a/text()","endOffset":9},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some addons run as one copy per node, controlled by a DaemonSet","v":"26","_id":"b993db3c971dbce7b0a31d127884087d","_rev":"1-8349a79f13685a9e0601010e6a171546","_revisions":{"start":1,"ids":["8349a79f13685a9e0601010e6a171546"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736242471047,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li/text()","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some addons scale vertically","v":"26","_id":"b9cd48e4a619f2c2c4626f6dccbc3def","_rev":"1-025994f82d01ae49bbeb5e2ac6036ac8","_revisions":{"start":1,"ids":["025994f82d01ae49bbeb5e2ac6036ac8"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736242477507,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()","endOffset":30},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Many addons scale horizontally","v":"26","_id":"bbcacfdd7eeb793f4fdb29ecf8d2256f","_rev":"1-d20ab4065d8b25f542fa1a5af8f45c34","_revisions":{"start":1,"ids":["d20ab4065d8b25f542fa1a5af8f45c34"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239335236,"range":{"startContainerPath":"//h3[@id=\"propagating-cluster-level-configuration-to-each-kubelet\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]","endOffset":0},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"Propagating cluster-level configuration to each kubelet","v":"26","_id":"bd2764fa49e99e203f1ef4cf421ba436","_rev":"1-b96ff8091bbe5fcaea866bf40888eb73","_revisions":{"start":1,"ids":["b96ff8091bbe5fcaea866bf40888eb73"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244968387,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()[3]","endOffset":135},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"By itself, Kubernetes does not include zone-aware networking. You can use a network plugin to configure cluster networking, and that network solution might have zone-specific elements. For example, if your cloud provider supports Services with type=LoadBalancer, the load balancer might only send traffic to Pods running in the same zone as the load balancer element processing a given connection","v":"26","_id":"c2ecb3a537ed51d7543ef3eeeb757a99","_rev":"1-f8eaefbea62ba0801175ca76c272fcb6","_revisions":{"start":1,"ids":["f8eaefbea62ba0801175ca76c272fcb6"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/node-conformance/","date":1736246954716,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":104,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"At a minimum, the node should have the following daemons installed:  CRI-compatible container runtimes such as Docker, containerd and CRI-O kubelet","v":"26","_id":"c4dba52fb9ca300de0ad537f7f2dfce6","_rev":"1-23897d6e5232f4ebb5c4988d6f77e5da","_revisions":{"start":1,"ids":["23897d6e5232f4ebb5c4988d6f77e5da"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239447075,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span[4]/span/span[2]/text()","endOffset":10},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10","v":"26","_id":"c8a95cf83e89a8e14d7b3c0c70fcd9c6","_rev":"1-eca50cad0f0bad97aca21afac44a5dd4","_revisions":{"start":1,"ids":["eca50cad0f0bad97aca21afac44a5dd4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/node-conformance/","date":1736247491869,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[3]/td[2]/text()","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes also provides node conformance test docker images for other architectures:  Arch Image amd64 node-test-amd64 arm node-test-arm arm64 node-test-arm64","v":"26","_id":"c93fa0557da5128d34b5143e11c6f5ff","_rev":"1-95ff93fcc3e3c93808d00cb1f9b326fb","_revisions":{"start":1,"ids":["95ff93fcc3e3c93808d00cb1f9b326fb"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/node-conformance/","date":1736246593127,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":259},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node conformance test is a containerized test framework that provides a system verification and functionality test for a node. The test validates whether the node meets the minimum requirements for Kubernetes; a node that passes the test is qualified to join a Kubernetes cluster.","v":"26","first":true,"title":"Validate node setup | Kubernetes","_id":"c97e4aa5afded4d741567462cb2d6826","_rev":"1-32efd8dc4510f19e1fbb08136265004d","_revisions":{"start":1,"ids":["32efd8dc4510f19e1fbb08136265004d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244471937,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes automatically spreads the Pods for workload resources (such as Deployment or StatefulSet) across different nodes in a cluster. This spreading helps reduce the impact of failures.  When nodes start up, the kubelet on each node automatically adds labels to the Node object that represents that specific kubelet in the Kubernetes API. These labels can include zone information.","v":"26","_id":"c9fb962176690605cc4f5b515d0f1dad","_rev":"1-3dd27d857aa0e385cd5902064d2479d3","_revisions":{"start":1,"ids":["3dd27d857aa0e385cd5902064d2479d3"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244401748,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":465},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"All control plane components support running as a pool of interchangeable resources, replicated per component.  When you deploy a cluster control plane, place replicas of control plane components across multiple failure zones. If availability is an important concern, select at least three failure zones and replicate each individual control plane component (API server, scheduler, etcd, cluster controller manager) across at least three failure zones. If you are running a cloud controller manager then you should also replicate this across all the failure zones you selected.","v":"26","_id":"cb3ba6f1d76976ecee2b5dbf48dec5d5","_rev":"1-d1ac064b824fdb6a52aa31292595f5c6","_revisions":{"start":1,"ids":["d1ac064b824fdb6a52aa31292595f5c6"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/multiple-zones/","date":1736244898246,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[3]","endOffset":109},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When persistent volumes are created, Kubernetes automatically adds zone labels to any PersistentVolumes that are linked to a specific zone. The scheduler then ensures, through its NoVolumeZoneConflict predicate, that pods which claim a given PersistentVolume are only placed into the same zone as that volume.","v":"26","_id":"cc996edfbc09105713fb6ed4707edfd8","_rev":"1-fa06b54e29487c8071129c3915832690","_revisions":{"start":1,"ids":["fa06b54e29487c8071129c3915832690"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247866416,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":62,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"setting them to the desired level and version you would eventually like to enforce.","v":"26","_id":"cd1bc15df94b0d53c52f29598d675b84","_rev":"1-0cef73dc5f35ae1fee0145c0ff0b35a4","_revisions":{"start":1,"ids":["0cef73dc5f35ae1fee0145c0ff0b35a4"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/node-conformance/","date":1736247082898,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[2]/div/pre/code/span[5]/span/text()","endOffset":31},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To run the node conformance test, perform the following steps:  Work out the value of the --kubeconfig option for the kubelet; for example: --kubeconfig=/var/lib/kubelet/config.yaml. Because the test framework starts a local control plane to test the kubelet, use http://localhost:8080 as the URL of the API server. There are some other kubelet command line parameters you may want to use:  --cloud-provider: If you are using --cloud-provider=gce, you should remove the flag to run the test. Run the node conformance test with command:  # $CONFIG_DIR is the pod manifest path of your kubelet. # $LOG_DIR is the test output path. sudo docker run -it --rm --privileged --net=host \\   -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\   registry.k8s.io/node-test:0.2","v":"26","_id":"d1e4e28665385faa21716a3bd3594637","_rev":"1-90f146a5508471b38f00083143f8189b","_revisions":{"start":1,"ids":["90f146a5508471b38f00083143f8189b"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736242466172,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","endOffset":116},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To avoid running into cluster addon resource issues, when creating a cluster with many nodes, consider the following","v":"26","_id":"d37dce8a547aa5e94fed403e608cb76a","_rev":"1-e080ffc25f3ded58c21865298ebf3aec","_revisions":{"start":1,"ids":["e080ffc25f3ded58c21865298ebf3aec"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736241777057,"range":{"startContainerPath":"//h2[@id=\"kubernetes-binaries-and-package-contents\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[5]/td[2]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes binaries and package contents The DEB and RPM packages shipped with the Kubernetes releases are:  Package name Description kubeadm Installs the /usr/bin/kubeadm CLI tool and the kubelet drop-in file for the kubelet. kubelet Installs the /usr/bin/kubelet binary. kubectl Installs the /usr/bin/kubectl binary. cri-tools Installs the /usr/bin/crictl binary from the cri-tools git repository. kubernetes-cni Installs the /opt/cni/bin binaries from the plugins git repository.","v":"26","_id":"d68ac10b5fbf00a5103f5c803728a716","_rev":"1-fd95fa6cb3e2275b8e77095f0cab9538","_revisions":{"start":1,"ids":["fd95fa6cb3e2275b8e77095f0cab9538"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736241464726,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[2]","endOffset":49},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When you call kubeadm init, the kubelet configuration is marshalled to disk","v":"26","_id":"d9758a78c52dafb07a99a848c95038d7","_rev":"1-ab80a9ec08dce472f53a65ae0a04181b","_revisions":{"start":1,"ids":["ab80a9ec08dce472f53a65ae0a04181b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736326780229,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":126},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Kubernetes cluster consists of a control plane and one or more worker nodes. Here's a brief overview of the main components:","v":"26","first":true,"title":"Kubernetes Components | Kubernetes","_id":"df634fbc76cb0e16aa60a54ade550446","_rev":"1-d113d04ef6cbd48c3eb668201980f3d2","_revisions":{"start":1,"ids":["d113d04ef6cbd48c3eb668201980f3d2"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239360188,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[3]","endOffset":10},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can provide the kubelet with default values to be used by kubeadm init and kubeadm join commands.","v":"26","_id":"dff9b0e68bb5464b47b4dd1d276af91e","_rev":"1-c61eb175ea67b54ddefa9592773f57c9","_revisions":{"start":1,"ids":["c61eb175ea67b54ddefa9592773f57c9"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/cluster-large/","date":1736241874769,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":37},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A cluster is a set of nodes (physical or virtual machines) running Kubernetes agents, managed by the control plane. Kubernetes v1.32 supports clusters with up to 5,000 nodes. More specifically, Kubernetes is designed to accommodate configurations that meet all of the following criteria:  No more than 110 pods per node No more than 5,000 nodes No more than 150,000 total pods No more than 300,000 total containers","v":"26","first":true,"title":"Considerations for large clusters | Kubernetes","_id":"e0b28a6d845aebf430d28240b4ea6283","_rev":"1-4492f10899c15b1ce2af67b30949215f","_revisions":{"start":1,"ids":["4492f10899c15b1ce2af67b30949215f"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/certificates/","date":1736248152760,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","startOffset":75,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/code[2]/text()","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"exception of user account certificates which kubeadm places in /etc/kubernetes","v":"26","_id":"e4832a0a9aeab89184b86828d0104494","_rev":"1-cbd670f0007f6999dd43e4198069c66d","_revisions":{"start":1,"ids":["cbd670f0007f6999dd43e4198069c66d"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247642791,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":379},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Namespaces that lack any configuration at all should be considered significant gaps in your cluster security model. We recommend taking the time to analyze the types of workloads occurring in each namespace, and by referencing the Pod Security Standards, decide on an appropriate level for each of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.","v":"26","_id":"e8c20fbaa1259a97b3afe500eadec7ea","_rev":"1-e27f248a10d638d0c43b13836c4e0e6b","_revisions":{"start":1,"ids":["e27f248a10d638d0c43b13836c4e0e6b"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/certificates/","date":1736247969331,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":65},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes requires PKI certificates for authentication over TLS.","v":"26","first":true,"title":"PKI certificates and requirements | Kubernetes","_id":"f20fa7734434cd7d50d07818285ba0e2","_rev":"1-a1c8d88bae366c3be6a7013b55b5460e","_revisions":{"start":1,"ids":["a1c8d88bae366c3be6a7013b55b5460e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/","date":1736255332046,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":362,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","endOffset":456},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"However, Kubernetes is not monolithic, and these default solutions are optional and pluggable.","v":"26","_id":"f736f9bcb8b0a261c39ec6f7a2451012","_rev":"1-10d8e2ef0716237c33208b611b001b7a","_revisions":{"start":1,"ids":["10d8e2ef0716237c33208b611b001b7a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/","date":1736322658951,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[7]/text()","endOffset":568},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.","v":"26","_id":"f7e45a6b53fd68f419f954a3b8a1cd03","_rev":"1-c5b664133c8bb8d166f0028616cdb97c","_revisions":{"start":1,"ids":["c5b664133c8bb8d166f0028616cdb97c"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/","date":1736239405980,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":94},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This setting needs to be the same for every kubelet on every manager and Node in the cluster.","v":"26","_id":"f8c4d1255948405f5b235b08f3141fd9","_rev":"1-a8b7b2ca846ae2558c5336fc8170f8e3","_revisions":{"start":1,"ids":["a8b7b2ca846ae2558c5336fc8170f8e3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/components/","date":1736327011836,"range":{"startContainerPath":"//h2[@id=\"addons\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl[3]/dd[4]/text()","endOffset":48},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Addons Addons extend the functionality of Kubernetes. A few important examples include:  DNS For cluster-wide DNS resolution Web UI (Dashboard) For cluster management via a web interface Container Resource Monitoring For collecting and storing container metrics Cluster-level Logging For saving container logs to a central log store","v":"26","_id":"f8e9f55730c67b328f707a2695e4fc7e","_rev":"1-37a47346a234f7664946062bb1fd7c58","_revisions":{"start":1,"ids":["37a47346a234f7664946062bb1fd7c58"]}},{"verb":"create","match":"https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/","date":1736247529108,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":55},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Pod Security Admission Controller intends to replace the deprecated PodSecurityPolicies.","v":"26","first":true,"title":"Enforcing Pod Security Standards | Kubernetes","_id":"fdd1c2bc9f3a192a91f714804a0d48a6","_rev":"1-9cc08ecbcab8a1129f75d0e6e49ac793","_revisions":{"start":1,"ids":["9cc08ecbcab8a1129f75d0e6e49ac793"]}}]}

{"seq":101}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330435293,"correspondingDocumentId":"e3acb46be1a302dde394ffd7cdb5da29","_id":"3a15c70b-7e74-4be6-82f1-db8b6582d6a2","_rev":"1-6f92c9c5714add5ba80aa933febbb2e3","_revisions":{"start":1,"ids":["6f92c9c5714add5ba80aa933febbb2e3"]}},{"_deleted":true,"_id":"6c229a2f-82c0-44a4-a336-b4a06d267cdb","_rev":"2-c739427b4627ddb38411eb1ce22eda36","_revisions":{"start":2,"ids":["c739427b4627ddb38411eb1ce22eda36","487d64b769c16c8870c2251f4fb7066e"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330176341,"correspondingDocumentId":"aae2f3e6257a2fc6d00148a22d4a2427","_id":"83dca567-ed34-4004-9fd4-dcd09d0b75eb","_rev":"1-032aa9e40659252d132b8208b2b0a0e3","_revisions":{"start":1,"ids":["032aa9e40659252d132b8208b2b0a0e3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328621738,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/pre/code/span/span/text()","endOffset":45},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl create deployment nginx --image nginx","v":"26","_id":"a1ff8748c6d6ba28738259902b497196","_rev":"1-3d4381d591d71fcd34b5a92d53d0029c","_revisions":{"start":1,"ids":["3d4381d591d71fcd34b5a92d53d0029c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330653167,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":90},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some resource types require their names to be able to be safely encoded as a path segment.","v":"26","_id":"a4ec7d3e99bd6dbe1302aa9b7f320cd0","_rev":"1-fb43d8ad0ac74927389447e5fa5aeeb1","_revisions":{"start":1,"ids":["fb43d8ad0ac74927389447e5fa5aeeb1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328040227,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[2]","endOffset":78},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via kubectl), that API request must include that information as JSON in the request body.","v":"26","_id":"a50cd7785c46a12cd746f0f1f62e875f","_rev":"1-1dd3253205c23fb5877457f2a3cd835b","_revisions":{"start":1,"ids":["1dd3253205c23fb5877457f2a3cd835b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330163069,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","startOffset":133,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":203},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"it may conflict with existing names resulting in a HTTP 409 response.","v":"26","_id":"aae2f3e6257a2fc6d00148a22d4a2427","_rev":"1-c7651611bf1717358d4733691a750809","_revisions":{"start":1,"ids":["c7651611bf1717358d4733691a750809"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736330898358,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":411,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":454},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each Key must be unique for a given object.","v":"26","_id":"adb353fd69c70bdf7d067286fa7a6f3d","_rev":"1-26e3c678ba9a13496d4b81f86c9609dd","_revisions":{"start":1,"ids":["26e3c678ba9a13496d4b81f86c9609dd"]}},{"_deleted":true,"_id":"ae21346a216d3f7fb8082368b6d51d3d","_rev":"2-ea2c64e139c1a4fd6e57a92a02bd9783","_revisions":{"start":2,"ids":["ea2c64e139c1a4fd6e57a92a02bd9783","3df2c432cfe18a2d3cfa33a8a662c58b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328289509,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()[12]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubectl tool uses the --validate flag to set the level of field validation. It accepts the values ignore, warn, and strict while also accepting the values true (equivalent to strict) and false (equivalent to ignore). The default validation setting for kubectl is --validate=true.","v":"26","_id":"af08eff4bcb94ebc4667fb4de753af34","_rev":"1-6202d92164f70fc50047810524753ea1","_revisions":{"start":1,"ids":["6202d92164f70fc50047810524753ea1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331240908,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":22,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Valid label keys have two segments: an optional prefix and name, separated by a slash (/).","v":"26","_id":"af7202903e5ffe3d3bb65805336e10ee","_rev":"1-a69fa996f1e367e0d801d33051876c49","_revisions":{"start":1,"ids":["a69fa996f1e367e0d801d33051876c49"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331417428,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()[2]","startOffset":48,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[2]","endOffset":113},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"we expect many objects to carry the same label(s).  Via a label selector, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.","v":"26","_id":"b3baddaf1d5c08dbc791a9ac1a6dbfc8","_rev":"1-ce92b6611678550c9a284a0105892c02","_revisions":{"start":1,"ids":["ce92b6611678550c9a284a0105892c02"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331032439,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/code[2]/text()","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Example labels:  \"release\" : \"stable\", \"release\" : \"canary\" \"environment\" : \"dev\", \"environment\" : \"qa\", \"environment\" : \"production\" \"tier\" : \"frontend\", \"tier\" : \"backend\", \"tier\" : \"cache\" \"partition\" : \"customerA\", \"partition\" : \"customerB\" \"track\" : \"daily\", \"track\" : \"weekly\"","v":"26","_id":"b44f1fa98e5eb48e94e9b08debed76a3","_rev":"1-45bb1a889fe180203c9ce615b390f582","_revisions":{"start":1,"ids":["45bb1a889fe180203c9ce615b390f582"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328674187,"range":{"startContainerPath":"//h2[@id=\"imperative-object-configuration\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","endOffset":235},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Imperative object configuration In imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one file name. The file specified must contain a full definition of the object in YAML or JSON format.","v":"26","_id":"be9ba50f8eb1f31e27494a15966b23b6","_rev":"1-9e7fb8d8edb0e7d01cf6a2cc9892e130","_revisions":{"start":1,"ids":["9e7fb8d8edb0e7d01cf6a2cc9892e130"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331279135,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[7]","startOffset":30,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[7]","endOffset":102},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The prefix is optional. If specified, the prefix must be a DNS subdomain","v":"26","_id":"bed25946bf23fdf9de310284b3076f59","_rev":"1-f77cfb3d0df91e1a9187fb86429910d0","_revisions":{"start":1,"ids":["f77cfb3d0df91e1a9187fb86429910d0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330058541,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","endOffset":265},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.","v":"26","_id":"befb65ce31c0abf46570ab242c5984d5","_rev":"1-aa95bb33eee2d17c0639b9fab39cabc6","_revisions":{"start":1,"ids":["aa95bb33eee2d17c0639b9fab39cabc6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330141825,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":30},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The server may generate a name when generateName is provided instead of name in a resource create request.","v":"26","_id":"bfd2a62c385cc7e8f099b30b9d646914","_rev":"1-d8ca2f1c47d61d9fdb3eff7d12ef0907","_revisions":{"start":1,"ids":["d8ca2f1c47d61d9fdb3eff7d12ef0907"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736327143518,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","endOffset":106},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe:  What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance","v":"26","first":true,"title":"Objects In Kubernetes | Kubernetes","_id":"c0142f3ade658a8c2f7c5a0b5581008a","_rev":"1-fb7f6940f0c62e2765ad9ff76653043f","_revisions":{"start":1,"ids":["fb7f6940f0c62e2765ad9ff76653043f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331269667,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":63},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The name segment is required and must be 63 characters or less","v":"26","_id":"c1c5478eb1b1ab086dc00a1479706026","_rev":"1-9b9685ffbeea6ce9f485d91c25066de1","_revisions":{"start":1,"ids":["9b9685ffbeea6ce9f485d91c25066de1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328094258,"range":{"startContainerPath":"//div[@id=\"application-deployment-yaml\"]/div/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"application-deployment-yaml\"]/div/pre/code/span[19]/span/span[4]/text()","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment spec:   selector:     matchLabels:       app: nginx   replicas: 2 # tells deployment to run 2 pods matching the template   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2         ports:         - containerPort: 80","v":"26","_id":"c1f12b5d953ba23862cb7f296843dd14","_rev":"1-26b67fad22d01f5d01eff7607b85759b","_revisions":{"start":1,"ids":["26b67fad22d01f5d01eff7607b85759b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736327188807,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[3]","endOffset":95},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system will constantly work to ensure that the object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to look like; this is your cluster's desired state.  To work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you.","v":"26","_id":"c4c512a5ee1429125616a660e526c1d9","_rev":"1-bfba24c76950ed5cf80455383179880e","_revisions":{"start":1,"ids":["bfba24c76950ed5cf80455383179880e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736330881282,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/a/text()","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Labels are key/value pairs that are attached to objects","v":"26","first":true,"title":"Labels and Selectors | Kubernetes","_id":"c672e606c5f9539f35dc0cf3ca927c3e","_rev":"1-8316b9e9fc36fd7644fddbe8b7bf583a","_revisions":{"start":1,"ids":["8316b9e9fc36fd7644fddbe8b7bf583a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328115056,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/pre/code/text()","endOffset":40},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"One way to create a Deployment using a manifest file like the one above is to use the kubectl apply command in the kubectl command-line interface, passing the .yaml file as an argument. Here's an example:  kubectl apply -f https://k8s.io/examples/application/deployment.yaml The output is similar to this:  deployment.apps/nginx-deployment created","v":"26","_id":"ce7652591816769ec2ea50cab8e03ae7","_rev":"1-829ff0ec444703282a9323a3f3c35251","_revisions":{"start":1,"ids":["829ff0ec444703282a9323a3f3c35251"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328129901,"range":{"startContainerPath":"//h3[@id=\"required-fields\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"required-fields\"]/text()","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Required fields","v":"26","_id":"ce833f047cc52bbd063a0da54a240c86","_rev":"1-8954d835df285bee843b1f5e735f4c07","_revisions":{"start":1,"ids":["8954d835df285bee843b1f5e735f4c07"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736329968632,"range":{"startContainerPath":"//h2[@id=\"names\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Names A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.","v":"26","_id":"d09937e88a4aecf9610b6d021b6475c9","_rev":"1-d5aa11ed171b9d36237348f2821d7fc6","_revisions":{"start":1,"ids":["d5aa11ed171b9d36237348f2821d7fc6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328142089,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[4]/text()","endOffset":39},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion - Which version of the Kubernetes API you're using to create this object kind - What kind of object you want to create metadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespace spec - What state you desire for the object","v":"26","_id":"d1370b23ddfbe9c307a0e7e51247d44d","_rev":"1-9cd2e275aee02a61b8e00e5d1a078024","_revisions":{"start":1,"ids":["9cd2e275aee02a61b8e00e5d1a078024"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331524028,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()[2]","endOffset":55},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Equality- or inequality-based requirements allow filtering by label keys and values.","v":"26","_id":"d14cd7b263c5e8a5a3803f19dbf06c3b","_rev":"1-4d8393980a1d18205299c294d0ee7902","_revisions":{"start":1,"ids":["4d8393980a1d18205299c294d0ee7902"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330181936,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","startOffset":134,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":379},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"it may conflict with existing names resulting in a HTTP 409 response. This became far less likely to happen in Kubernetes v1.31 and later, since the server will make up to 8 attempt to generate a unique name before returning a HTTP 409 response.","v":"26","_id":"d1b56fb1a49fba08fcc4c0424314658a","_rev":"1-720f735bcdcf9c4e0ee30c162cb5e4c2","_revisions":{"start":1,"ids":["720f735bcdcf9c4e0ee30c162cb5e4c2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736327252623,"range":{"startContainerPath":"//h3[@id=\"object-spec-and-status\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[4]","endOffset":102},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Object spec and status Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the resource to have: its desired state.  The status describes the current state of the object, supplied and updated by the Kubernetes system and its components. The Kubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied.","v":"26","_id":"d1d42f7474c14fea00cffa84e61223ed","_rev":"1-44e17d741cba291baf42edd39aae2af2","_revisions":{"start":1,"ids":["44e17d741cba291baf42edd39aae2af2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328613937,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":140,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":190},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"it provides no history of previous configurations.","v":"26","_id":"d231a2cabae44589db6c45cf48bef580","_rev":"1-96ed7b04e68daaf8fd9697425261bd5a","_revisions":{"start":1,"ids":["96ed7b04e68daaf8fd9697425261bd5a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330391438,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/text()","startOffset":78,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/text()","endOffset":206},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"RFC 1123 labels are allowed to start with a digit, whereas RFC 1035 labels can start with a lowercase alphabetic character only.","v":"26","_id":"d2772dd32e6045b1e0ada338554c351d","_rev":"1-312144fa20437e597cc27049c7877ff3","_revisions":{"start":1,"ids":["312144fa20437e597cc27049c7877ff3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330689480,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","startOffset":84,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","endOffset":134},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"standardized as ISO/IEC 9834-8 and as ITU-T X.667.","v":"26","_id":"d304dc510df531b421ae5800e7c9503c","_rev":"1-ea4e6e5fc3fd94372a5ae9e6e6aef043","_revisions":{"start":1,"ids":["ea4e6e5fc3fd94372a5ae9e6e6aef043"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736330923709,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Labels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-identifying information should be recorded using annotations.","v":"26","_id":"d370ded6e64c8df064e2dab48022d773","_rev":"1-2b2002e4596958fdb0cc1322cf107e5c","_revisions":{"start":1,"ids":["2b2002e4596958fdb0cc1322cf107e5c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736329051826,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[8]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/pre/code/span[2]/span/text()","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl diff -f configs/ kubectl apply -f configs/ Recursively process directories:  kubectl diff -R -f configs/ kubectl apply -R -f configs/","v":"26","_id":"d3c87822207d72a1082b2de715fafa10","_rev":"1-993b55ecbe2f6edbb9389a6a5ecf6c49","_revisions":{"start":1,"ids":["993b55ecbe2f6edbb9389a6a5ecf6c49"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328592118,"range":{"startContainerPath":"//h2[@id=\"imperative-commands\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":86},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Imperative commands When using imperative commands, a user operates directly on live objects in a cluster.","v":"26","_id":"d3fb1fde45bae452629b8c06d093da7c","_rev":"1-33ca768a3069f09e4d52502c63573a11","_revisions":{"start":1,"ids":["33ca768a3069f09e4d52502c63573a11"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331499945,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The API currently supports two types of selectors: equality-based and set-based.","v":"26","_id":"d6bedd68bbcf010e4a4b61b2d3dc8229","_rev":"1-54e01bf69070ee5d1e24a822eb8d35b2","_revisions":{"start":1,"ids":["54e01bf69070ee5d1e24a822eb8d35b2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330682300,"range":{"startContainerPath":"//h2[@id=\"uids\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"UIDs A Kubernetes systems-generated string to uniquely identify objects.","v":"26","_id":"d77d15b104147c2cb202717772214892","_rev":"1-dec7679b7ce831779873db0cc06a5967","_revisions":{"start":1,"ids":["dec7679b7ce831779873db0cc06a5967"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331298667,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[4]","endOffset":32},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add labels to end-user objects must specify a prefix.  The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.","v":"26","_id":"dde5e27faaa7b5ac74a44c9c07b3f9d5","_rev":"1-652cb6038b216bf17e3af8c38e323d34","_revisions":{"start":1,"ids":["652cb6038b216bf17e3af8c38e323d34"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736329880499,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, you can only have one Pod named myapp-1234 within the same namespace, but you can have one Pod and one Deployment that are each named myapp-1234.  For non-unique user-provided attributes, Kubernetes provides labels and annotations.","v":"26","_id":"e0af308cb1168092880990f986c22910","_rev":"1-cdb72529b01b249ad5a6dc958d3569fd","_revisions":{"start":1,"ids":["cdb72529b01b249ad5a6dc958d3569fd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328718790,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span/span/text()","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl create -f nginx.yaml","v":"26","_id":"e0b4298a6d7c73f54095ee927f3a2c16","_rev":"1-3e53e0866c02a3d7877959203e4eb2a9","_revisions":{"start":1,"ids":["3e53e0866c02a3d7877959203e4eb2a9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328525214,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/h4/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","endOffset":145},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Warning: A Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object results in undefined behavior.","v":"26","first":true,"title":"Kubernetes Object Management | Kubernetes","_id":"e1dc582a3e842e4809c9573ac842cce2","_rev":"1-59f0e1773b2c3fd3c8ea36fd7b0d9391","_revisions":{"start":1,"ids":["59f0e1773b2c3fd3c8ea36fd7b0d9391"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330370166,"range":{"startContainerPath":"//h3[@id=\"dns-subdomain-names\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[4]/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DNS Subdomain Names Most resource types require a name that can be used as a DNS subdomain name as defined in RFC 1123. This means the name must:  contain no more than 253 characters contain only lowercase alphanumeric characters, '-' or '.' start with an alphanumeric character end with an alphanumeric character RFC 1123 Label Names Some resource types require their names to follow the DNS label standard as defined in RFC 1123. This means the name must:  contain at most 63 characters contain only lowercase alphanumeric characters or '-' start with an alphanumeric character end with an alphanumeric character","v":"26","_id":"e3acb46be1a302dde394ffd7cdb5da29","_rev":"1-ca132cead9245ce09eb5b57d423ecc7f","_revisions":{"start":1,"ids":["ca132cead9245ce09eb5b57d423ecc7f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736329986500,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/strong/text()[2]","endOffset":196},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Names must be unique across all API versions of the same resource. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.","v":"26","_id":"e5af32226734c1fe7686358040cff853","_rev":"1-9b35a44c1f4c4ed43a27d9e0850ec707","_revisions":{"start":1,"ids":["9b35a44c1f4c4ed43a27d9e0850ec707"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736328915117,"range":{"startContainerPath":"//h2[@id=\"declarative-object-configuration\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()[2]","endOffset":104},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Declarative object configuration When using declarative object configuration, a user operates on object configuration files stored locally, however the user does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by kubectl. This enables working on directories, where different operations might be needed for different objects.","v":"26","_id":"ea97d75358a34661bc92265dd09e9895","_rev":"1-e63556058d7f3528217374a083a0d0a1","_revisions":{"start":1,"ids":["e63556058d7f3528217374a083a0d0a1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/","date":1736328375991,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()[2]","startOffset":115,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()[2]","endOffset":217},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes 1.27 and later versions always offer field validation; older Kubernetes releases might not.","v":"26","_id":"ec709d1e385198e12a333777282af48a","_rev":"1-7a10221860fe39e9c5805f2ade8616b6","_revisions":{"start":1,"ids":["7a10221860fe39e9c5805f2ade8616b6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736329786577,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[4]","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique across your whole cluster.","v":"26","first":true,"title":"Object Names and IDs | Kubernetes","_id":"ec75bca738e5efc38387acfaa35096e3","_rev":"1-f0d75f81b67bd977281c468b773f508d","_revisions":{"start":1,"ids":["f0d75f81b67bd977281c468b773f508d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331564236,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/pre/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/pre/code/text()","endOffset":41},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"environment = production tier != frontend","v":"26","_id":"f167dbd5fac8336247d2268510e577f5","_rev":"1-c923103fff1b9134b0c4f9674f07e0fd","_revisions":{"start":1,"ids":["c923103fff1b9134b0c4f9674f07e0fd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330444586,"range":{"startContainerPath":"//h3[@id=\"dns-subdomain-names\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DNS Subdomain Names Most resource types require a name that can be used as a DNS subdomain name as defined in RFC 1123. This means the name must:  contain no more than 253 characters contain only lowercase alphanumeric characters, '-' or '.' start with an alphanumeric character end with an alphanumeric character","v":"26","_id":"f2162ad58cb1944898b93bd1c1cbba2e","_rev":"1-e51ec58cce3c5872b51323606e0788fe","_revisions":{"start":1,"ids":["e51ec58cce3c5872b51323606e0788fe"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/","date":1736329034341,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[7]/text()","startOffset":146,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[7]/text()[3]","endOffset":58},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This is possible by using the patch API operation to write only observed differences, instead of using the replace API operation to replace the entire object configuration.","v":"26","_id":"f7a7fbf9661923fadd5e300943dd35d6","_rev":"1-4c300831607fb27a240ee59719094212","_revisions":{"start":1,"ids":["4c300831607fb27a240ee59719094212"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/names/","date":1736330431420,"correspondingDocumentId":"d2772dd32e6045b1e0ada338554c351d","_id":"f9090c3e-823b-4f7c-be9d-ae6a3455cbd9","_rev":"1-c8ef028953f07ffa66374d5687bc14bd","_revisions":{"start":1,"ids":["c8ef028953f07ffa66374d5687bc14bd"]}}]}

{"seq":153}

{"docs":[{"_deleted":true,"_id":"2d1295da-5c49-4b1d-9037-6c505d5eb72e","_rev":"2-6366786c9531beece48f4874f5a4b6c4","_revisions":{"start":2,"ids":["6366786c9531beece48f4874f5a4b6c4","4a77b5e27e09e2a0e644251c3ec2aa2a"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338923364,"correspondingDocumentId":"e3a0c581fc3fcb3639b138026f172b27","_id":"53cce4dc-5eda-4fe8-b249-2559b8d8869a","_rev":"1-3e7c05ae379d9507724d4ab709f34040","_revisions":{"start":1,"ids":["3e7c05ae379d9507724d4ab709f34040"]}},{"_deleted":true,"_id":"97493322-0df4-44b2-b7f7-09aa71292a3c","_rev":"2-6a1a4f8011d8d5882ce14d14037653bd","_revisions":{"start":2,"ids":["6a1a4f8011d8d5882ce14d14037653bd","35716d90419697fbb78fbcdd83b31def"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333166953,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[12]/pre/code/span[6]/span/text()[3]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Newer resources, such as Job, Deployment, ReplicaSet, and DaemonSet, support set-based requirements as well.  selector:   matchLabels:     component: redis   matchExpressions:     - { key: tier, operator: In, values: [cache] }     - { key: environment, operator: NotIn, values: [dev] }","v":"26","_id":"a60fcd995d5436f144cfec73f449e5f9","_rev":"1-12ff8962ed51a49641f0c0261315294d","_revisions":{"start":1,"ids":["12ff8962ed51a49641f0c0261315294d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736337754188,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.).","v":"26","first":true,"title":"Namespaces | Kubernetes","_id":"a8a0cfc4197dd2c72a7f30d036c052cd","_rev":"1-30749c4cb956b4e0c3baa14fce455fc4","_revisions":{"start":1,"ids":["30749c4cb956b4e0c3baa14fce455fc4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333158359,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[11]/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[11]/pre/code/span[2]/span/text()[2]","endOffset":5},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"selector:   component: redis","v":"26","_id":"a8ed6416fc10206fb32bfab3253cf4f1","_rev":"1-f818f4503a18401affdff70e1667c064","_revisions":{"start":1,"ids":["f818f4503a18401affdff70e1667c064"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/","date":1736343558200,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/pre/code/text()","endOffset":202},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Supported field selectors vary by Kubernetes resource type. All resource types support the metadata.name and metadata.namespace fields. Using unsupported field selectors produces an error. For example:  kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field selector \"foo.bar=baz\": \"foo.bar\" is not a known field selector: only \"metadata.name\", \"metadata.namespace\"","v":"26","_id":"acf03995c9876831e15bf3be88c87a8b","_rev":"1-048b4ec79212581ff9c27da233ac2de5","_revisions":{"start":1,"ids":["048b4ec79212581ff9c27da233ac2de5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736332766553,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some Kubernetes objects, such as services and replicationcontrollers, also use label selectors to specify sets of other resources, such as pods.","v":"26","_id":"af0f125b1568276fa504bad7661e4b09","_rev":"1-e3692e142dcd3ebb906be30eb0adf7a3","_revisions":{"start":1,"ids":["e3692e142dcd3ebb906be30eb0adf7a3"]}},{"_deleted":true,"_id":"b074fb57230448f794a405175304576e","_rev":"2-093ac8e7324bc1447748f5274faad496","_revisions":{"start":2,"ids":["093ac8e7324bc1447748f5274faad496","5da8992dfab46697dcbdf7c50fa15178"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339120410,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/pre/code/span[2]/span/text()[2]","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl run nginx --image=nginx --namespace=<insert-namespace-name-here> kubectl get pods --namespace=<insert-namespace-name-here>","v":"26","_id":"b370930573df24c330e0736026a0ed1b","_rev":"1-1e53299e0d6221beba6c8d51bf12d8b4","_revisions":{"start":1,"ids":["1e53299e0d6221beba6c8d51bf12d8b4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339141230,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[5]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[5]/pre/code/span[3]/span/text()","endOffset":46},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl config set-context --current --namespace=<insert-namespace-name-here> # Validate it kubectl config view --minify | grep namespace:","v":"26","_id":"b5c7866956474d5613822dc692f39a73","_rev":"1-c9fad2569f480fbc757498904aeb2ea1","_revisions":{"start":1,"ids":["c9fad2569f480fbc757498904aeb2ea1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/","date":1736342970459,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":51},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Field selectors let you select Kubernetes objects based on the value of one or more resource fields.","v":"26","first":true,"title":"Field Selectors | Kubernetes","_id":"b6a3c8455abb44628d119e2865ee9e10","_rev":"1-2b332c464975dbcb3f1356bebc4e3192","_revisions":{"start":1,"ids":["2b332c464975dbcb3f1356bebc4e3192"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736337413524,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","endOffset":95},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The labels allow for slicing and dicing the resources along any dimension specified by a label:","v":"26","_id":"b8604ce0d3818daa69e7e62b6530c645","_rev":"1-5dfc97f16d83eb379ff0cf8aa803289b","_revisions":{"start":1,"ids":["5dfc97f16d83eb379ff0cf8aa803289b"]}},{"_deleted":true,"_id":"b9e988d28bcedbb401b05dc5eff550f4","_rev":"2-14407f1e1e54f36ac65351b393790df1","_revisions":{"start":2,"ids":["14407f1e1e54f36ac65351b393790df1","5081ebfc7ab8795235db0ea436cb7943"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339432739,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/p[2]/text()","startOffset":113,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/p[2]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"configure third-party security controls, such as admission webhooks, to block creating any namespace with the name of public TLDs.","v":"26","_id":"bfa840309bfeb702f81de21eecaa69b5","_rev":"1-6309c6daedc77cba636a82b094817d64","_revisions":{"start":1,"ids":["6309c6daedc77cba636a82b094817d64"]}},{"_deleted":true,"_id":"c0cba850-acf9-4a59-9e79-b0641f9fb06e","_rev":"2-aaea4e7bcf583175043652f40e9194f0","_revisions":{"start":2,"ids":["aaea4e7bcf583175043652f40e9194f0","56e1804fb311760a6cd76e2be44faef9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736332383101,"range":{"startContainerPath":"//h3[@id=\"list-and-watch-filtering\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/text()[3]","endOffset":132},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"LIST and WATCH filtering For list and watch operations, you can specify label selectors to filter the sets of objects returned; you specify the filter using a query parameter.","v":"26","_id":"c3629b01e4a6ce97de35bfe4a67b6372","_rev":"1-11c18dc36ea2237eb19fbd9f18d27dd7","_revisions":{"start":1,"ids":["11c18dc36ea2237eb19fbd9f18d27dd7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/","date":1736348705787,"range":{"startContainerPath":"//h2[@id=\"how-finalizers-work\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()[2]","endOffset":30},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"How finalizers work When you create a resource using a manifest file, you can specify finalizers in the metadata.finalizers field. When you attempt to delete the resource, the API server handling the delete request notices the values in the finalizers field and does the following:  Modifies the object to add a metadata.deletionTimestamp field with the time you started the deletion. Prevents the object from being removed until all items are removed from its metadata.finalizers field Returns a 202 status code (HTTP \"Accepted\")","v":"26","_id":"c5abd8255486ef31cb24c99e9375354e","_rev":"1-18c9ec26c6205899f49f6baa22d6f366","_revisions":{"start":1,"ids":["18c9ec26c6205899f49f6baa22d6f366"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333282237,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[9]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[9]","endOffset":123},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn.","v":"26","_id":"c721d96a32719d2c64ca84a04e098ae4","_rev":"1-fcaa9088671c70098eb6584a137ed57c","_revisions":{"start":1,"ids":["fcaa9088671c70098eb6584a137ed57c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736332478205,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()[3]","startOffset":10,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[7]/pre/code/span/span/span/text()","endOffset":48},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"equality-based one may write:  kubectl get pods -l environment=production,tier=frontend or using set-based requirements:  kubectl get pods -l 'environment in (production),tier in (frontend)'","v":"26","_id":"c760dd7343a4b2cb73a3786b5837fd20","_rev":"1-3e126c050679587769fdd4c6d43a9a66","_revisions":{"start":1,"ids":["3e126c050679587769fdd4c6d43a9a66"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331636381,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","endOffset":70},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Set-based label requirements allow filtering keys according to a set of values.","v":"26","_id":"c8010f253914eb7483d8d8f2d8db74ed","_rev":"1-2f0ce74eed021002487f1acdac76f60c","_revisions":{"start":1,"ids":["2f0ce74eed021002487f1acdac76f60c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339309784,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()[5]","endOffset":63},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespace-name>.svc.cluster.local, which means that if a container only uses <service-name>, it will resolve to the service which is local to a namespace.","v":"26","_id":"c80ffa4e2008a5dd52258cf688433cfe","_rev":"1-e241524bb646009cb2527b9f6c9ca5e4","_revisions":{"start":1,"ids":["e241524bb646009cb2527b9f6c9ca5e4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736332451381,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/code/text()","endOffset":76},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"equality-based requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend set-based requirements: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29","v":"26","_id":"c8247b1278447993cdd0763aaa02d313","_rev":"1-7e689647974acf75b8afc2f9572bf3df","_revisions":{"start":1,"ids":["7e689647974acf75b8afc2f9572bf3df"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338873644,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","startOffset":85,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":190},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.","v":"26","_id":"ca3264809e1d9dd14e94817f2ec54bfd","_rev":"1-a44ba3ced3f687278fd1ffd2cc363c13","_revisions":{"start":1,"ids":["a44ba3ced3f687278fd1ffd2cc363c13"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/","date":1736343591107,"range":{"startContainerPath":"//h3[@id=\"list-of-supported-fields\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[9]/td[2]/code/text()","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"List of supported fields Kind Fields Pod spec.nodeName spec.restartPolicy spec.schedulerName spec.serviceAccountName spec.hostNetwork status.phase status.podIP status.nominatedNodeName Event involvedObject.kind involvedObject.namespace involvedObject.name involvedObject.uid involvedObject.apiVersion involvedObject.resourceVersion involvedObject.fieldPath reason reportingComponent source type Secret type Namespace status.phase ReplicaSet status.replicas ReplicationController status.replicas Job status.successful Node spec.unschedulable CertificateSigningRequest spec.signerName","v":"26","_id":"cc494196035df0575fa065c2b2214449","_rev":"1-a464d21d00a013712992090ca30746de","_revisions":{"start":1,"ids":["a464d21d00a013712992090ca30746de"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/","date":1736342978714,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/pre/code/span/span/text()[2]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl get pods --field-selector status.phase=Running","v":"26","_id":"ccc6cd9df68621e30b82c4e9defe8f1b","_rev":"1-a2e21187631a98ea2e2bd20b6bda3288","_revisions":{"start":1,"ids":["a2e21187631a98ea2e2bd20b6bda3288"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338868145,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":37},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Namespaces provide a scope for names.","v":"26","_id":"cd2e417e160b173f27e3a7e7c356d246","_rev":"1-3a9b1b63a530985bb2c781cc618321a2","_revisions":{"start":1,"ids":["3a9b1b63a530985bb2c781cc618321a2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736337498269,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[40]/text()[2]","startOffset":15,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[18]/pre/code/span/span/text()[3]","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"if you want to label all your NGINX Pods as frontend tier, run:  kubectl label pods -l app=nginx tier=fe","v":"26","_id":"cd6edb330853de0989eecc1c47b40367","_rev":"1-8cb5445e3b3a7ca5dea800e55f5d1c47","_revisions":{"start":1,"ids":["8cb5445e3b3a7ca5dea800e55f5d1c47"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339319671,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()[2]","startOffset":121,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()[2]","endOffset":216},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).","v":"26","_id":"cf73fcf9b68c5041a712cce99ef18bfc","_rev":"1-48ac9319326c9b553ab6ddfe2dea3a96","_revisions":{"start":1,"ids":["48ac9319326c9b553ab6ddfe2dea3a96"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333131963,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()[3]","startOffset":23,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()[4]","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"only equality-based requirement selectors are supported","v":"26","_id":"d44cc04cd288c94cdefc918c6ddc6caa","_rev":"1-d0513d3ad475fedd958ec33ccfc5fb73","_revisions":{"start":1,"ids":["d0513d3ad475fedd958ec33ccfc5fb73"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736337492827,"range":{"startContainerPath":"//h2[@id=\"updating-labels\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"updating-labels\"]/text()","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Updating labels","v":"26","_id":"d5ff98da95502295971cf10f2e949a42","_rev":"1-29d922ced890ea878b8b8e79016d6ae4","_revisions":{"start":1,"ids":["29d922ced890ea878b8b8e79016d6ae4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736340393905,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[7]/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[7]/pre/code/span[5]/span/span[2]/text()","endOffset":5},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"# In a namespace kubectl api-resources --namespaced=true  # Not in a namespace kubectl api-resources --namespaced=false","v":"26","_id":"d6497692f1cc0397bb80645b54f9acc6","_rev":"1-a98cafc0122233198a0191a57f828058","_revisions":{"start":1,"ids":["a98cafc0122233198a0191a57f828058"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/","date":1736340577275,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":257},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects.","v":"26","_id":"d64bef70ec999a3221192a1b76eab579","_rev":"1-749a7e4db1b08970971faa5e6ab7a3e5","_revisions":{"start":1,"ids":["749a7e4db1b08970971faa5e6ab7a3e5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736337417850,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[16]/pre/code/span[2]/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[16]/pre/code/span[2]/span/text()","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl get pods -Lapp -Ltier -Lrole","v":"26","_id":"d674bfaa49ccf91d1c5d2ccd18aee57b","_rev":"1-5330b4bb9bde36af9dc3041bbaefccfc","_revisions":{"start":1,"ids":["5330b4bb9bde36af9dc3041bbaefccfc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/","date":1736348808596,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","startOffset":8,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[5]","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When the finalizers field is emptied, an object with a deletionTimestamp field set is automatically deleted.","v":"26","_id":"d6a10aec63a5a3ce89d2d976649f537f","_rev":"1-115da9f036dc484a844ef3f13e170ac8","_revisions":{"start":1,"ids":["115da9f036dc484a844ef3f13e170ac8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/","date":1736341353550,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/text()","endOffset":51},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The keys and the values in the map must be strings.","v":"26","_id":"dc0a79d90cbab83326e3f9c05a1eff19","_rev":"1-085a0ea404edeea4c8c9e7d20403da71","_revisions":{"start":1,"ids":["085a0ea404edeea4c8c9e7d20403da71"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736339386816,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/p/text()[2]","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"By creating namespaces with the same name as public top-level domains, Services in these namespaces can have short DNS names that overlap with public DNS records.","v":"26","_id":"dc400b9eb1006d047ac72f3781e78559","_rev":"1-36b5e2af7c4a8ea9cf47b340b2043e8e","_revisions":{"start":1,"ids":["36b5e2af7c4a8ea9cf47b340b2043e8e"]}},{"_deleted":true,"_id":"e041af2bb624838758e8f72e01533475","_rev":"2-d645fe8a0c7ba3bd49bf564b270030bb","_revisions":{"start":2,"ids":["d645fe8a0c7ba3bd49bf564b270030bb","e8831c14cd0857ac9c335e8b1b1bb74e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338829924,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":107,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":276},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.","v":"26","_id":"e2b2fa955e5ffef0aa5bd8297c7bd6d7","_rev":"1-80efc9cd9535e79e128f16f989582dee","_revisions":{"start":1,"ids":["80efc9cd9535e79e128f16f989582dee"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338917871,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":138,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":52},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"use labels to distinguish resources within the same namespace.","v":"26","_id":"e3a0c581fc3fcb3639b138026f172b27","_rev":"1-f6d9b9d18aa3b6ae15709f31cb8d3e16","_revisions":{"start":1,"ids":["f6d9b9d18aa3b6ae15709f31cb8d3e16"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333371025,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()","endOffset":101},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"One use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule","v":"26","_id":"e416b7032c5b073e4b1932002ccfae24","_rev":"1-a6560941add354798e198ec7cb911519","_revisions":{"start":1,"ids":["a6560941add354798e198ec7cb911519"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/","date":1736348024101,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":229},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.  When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating .metadata.deletionTimestamp, and returns a 202 status code (HTTP \"Accepted\"). The target object remains in a terminating state while the control plane, or other components, take the actions defined by the finalizers. After these actions are complete, the controller removes the relevant finalizers from the target object. When the metadata.finalizers field is empty, Kubernetes considers the deletion complete and deletes the object.  You can use finalizers to control garbage collection of resources. For example, you can define a finalizer to clean up related resources or infrastructure before the controller deletes the target resource.  You can use finalizers to control garbage collection of objects by alerting controllers to perform specific cleanup tasks before deleting the target resource.  Finalizers don't usually specify the code to execute. Instead, they are typically lists of keys on a specific resource similar to annotations. Kubernetes specifies some finalizers automatically, but you can also specify your own.","v":"26","first":true,"title":"Finalizers | Kubernetes","_id":"e9de50710b8ce257b289876c74abae91","_rev":"1-86d0ae7ef1993b98567ca7360d685be3","_revisions":{"start":1,"ids":["86d0ae7ef1993b98567ca7360d685be3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/","date":1736342432462,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":22,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","endOffset":90},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Valid annotation keys have two segments: an optional prefix and name","v":"26","_id":"eb6bd2cc759955e0b03bbde5d5ded018","_rev":"1-744e11962d974f43c172d0753c1b0a56","_revisions":{"start":1,"ids":["744e11962d974f43c172d0753c1b0a56"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736332529371,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/pre/code/span/span/span/text()","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"set-based requirements are more expressive. For instance, they can implement the OR operator on values:  kubectl get pods -l 'environment in (production, qa)' or restricting negative matching via notin operator:  kubectl get pods -l 'environment,environment notin (frontend)'","v":"26","_id":"eea939cdb2d762975fd5276c14208578","_rev":"1-de7f256f88b95a221736706198dc5127","_revisions":{"start":1,"ids":["de7f256f88b95a221736706198dc5127"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338900482,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Namespaces are a way to divide cluster resources between multiple users (via resource quota).","v":"26","_id":"f0c7c7bd137316656c69492439dce353","_rev":"1-c208913b912b269b1dda5ea79acce8d0","_revisions":{"start":1,"ids":["c208913b912b269b1dda5ea79acce8d0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338969940,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[4]/text()","endOffset":59},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes starts with four initial namespaces:  default Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace. kube-node-lease This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. kube-public This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-system The namespace for objects created by the Kubernetes system.","v":"26","_id":"f3f133949f98f4398f9edd1c4796b141","_rev":"1-3bb64aa6e518742af87022383603765f","_revisions":{"start":1,"ids":["3bb64aa6e518742af87022383603765f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/","date":1736340457718,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects.","v":"26","first":true,"title":"Annotations | Kubernetes","_id":"f68022489b471ead18711628b108fb88","_rev":"1-b12502da603602e3007ad65fc48dbaed","_revisions":{"start":1,"ids":["b12502da603602e3007ad65fc48dbaed"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736333238415,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/code[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[9]","endOffset":40},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"matchExpressions is a list of pod selector requirements.","v":"26","_id":"f72379801df55116ba714f412286311c","_rev":"1-9e1ab73b638dd0a5046ae4b65afc52dc","_revisions":{"start":1,"ids":["9e1ab73b638dd0a5046ae4b65afc52dc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/","date":1736331640004,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/pre[2]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/pre[2]/code/text()","endOffset":83},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"environment in (production, qa) tier notin (frontend, backend) partition !partition","v":"26","_id":"f77a2d3316fac54c298ba8c2ca580840","_rev":"1-8603b8aa0a510763e107af80bd983e7e","_revisions":{"start":1,"ids":["8603b8aa0a510763e107af80bd983e7e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","date":1736338928470,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":52},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","v":"26","_id":"fe7ffcd91994c8ee55f52eba0536f41d","_rev":"1-69628f490fc11a5b2b8ca4368820c04b","_revisions":{"start":1,"ids":["69628f490fc11a5b2b8ca4368820c04b"]}}]}

{"seq":209}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736512877540,"correspondingDocumentId":"daf62b9e7736e7a39dac15b88909b89d","_id":"2509fdb3-78d3-4b45-aaee-eadf358c2a26","_rev":"1-1ba3c1e0a9c4f884afa14b229d5542c6","_revisions":{"start":1,"ids":["1ba3c1e0a9c4f884afa14b229d5542c6"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430587150,"correspondingDocumentId":"b2d2efc224fa7aaa31f5f86ac3cb9015","_id":"3d5ac527-1389-466a-84bd-393b7eff764d","_rev":"1-5b871cb2c2441599ae81d3a7c291760a","_revisions":{"start":1,"ids":["5b871cb2c2441599ae81d3a7c291760a"]}},{"_deleted":true,"_id":"41c738c6-7583-4d8e-90ae-f353962f6ef1","_rev":"2-7838e4f0bc3d62cb370b2c3e81a438fe","_revisions":{"start":2,"ids":["7838e4f0bc3d62cb370b2c3e81a438fe","8c4eea8764729583b05cfa9e5940d8ea"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435847463,"correspondingDocumentId":"a4e9d020856f578c2f441eb59ce7a7a9","_id":"554f680b-8f20-447a-8fd8-b557887bf0e8","_rev":"1-753481cebe7735f33de609bfebeb5ac9","_revisions":{"start":1,"ids":["753481cebe7735f33de609bfebeb5ac9"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435823840,"correspondingDocumentId":"c5439bcf503aedd15033fb00494182bf","_id":"66952398-d63e-40a5-8f3f-e3a7d4ffd70c","_rev":"1-47f28db1f98e4ebcd6724591e6c840a8","_revisions":{"start":1,"ids":["47f28db1f98e4ebcd6724591e6c840a8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435837514,"range":{"startContainerPath":"//h3[@id=\"etcd\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/mark/text()","endOffset":21},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"etcd Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.  If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.  You can find in-depth information about etcd in the official https://etcd.io/docs/","v":"26","_id":"a4e9d020856f578c2f441eb59ce7a7a9","_rev":"1-8e037f41a3cadbbbb9ed394082ca8747","_revisions":{"start":1,"ids":["8e037f41a3cadbbbb9ed394082ca8747"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736432264114,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":334},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.","v":"26","_id":"a61d4c5020247bfff2e73adfdaed561e","_rev":"1-0e4d1b09fc34d645cc2a116de4abf8e8","_revisions":{"start":1,"ids":["0e4d1b09fc34d645cc2a116de4abf8e8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/","date":1736428246113,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/pre/code/span[11]/span/text()[2]","endOffset":4},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In order to take full advantage of using these labels, they should be applied on every resource object.  Key Description Example Type app.kubernetes.io/name The name of the application mysql string app.kubernetes.io/instance A unique name identifying the instance of an application mysql-abcxyz string app.kubernetes.io/version The current version of the application (e.g., a SemVer 1.0, revision hash, etc.) 5.7.21 string app.kubernetes.io/component The component within the architecture database string app.kubernetes.io/part-of The name of a higher level application this one is part of wordpress string app.kubernetes.io/managed-by The tool being used to manage the operation of an application Helm string To illustrate these labels in action, consider the following StatefulSet object:  # This is an excerpt apiVersion: apps/v1 kind: StatefulSet metadata:   labels:     app.kubernetes.io/name: mysql     app.kubernetes.io/instance: mysql-abcxyz     app.kubernetes.io/version: \"5.7.21\"     app.kubernetes.io/component: database     app.kubernetes.io/part-of: wordpress     app.kubernetes.io/managed-by: Helm","v":"26","_id":"aac1cf0764b5abf563f048604c6c8856","_rev":"1-41c15a33703642c236dca7bcefaf7bc7","_revisions":{"start":1,"ids":["41c15a33703642c236dca7bcefaf7bc7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736512060880,"range":{"startContainerPath":"//h3[@id=\"kubelet\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubelet An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.","v":"26","_id":"af1606cc36202736c7fc1a0a0e80a956","_rev":"1-c0478798262b2ebe4a97da7e6097313c","_revisions":{"start":1,"ids":["c0478798262b2ebe4a97da7e6097313c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/","date":1736348839404,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()[9]","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A common example of a finalizer is kubernetes.io/pv-protection, which prevents accidental deletion of PersistentVolume objects. When a PersistentVolume object is in use by a Pod, Kubernetes adds the pv-protection finalizer. If you try to delete the PersistentVolume, it enters a Terminating status, but the controller can't delete it because the finalizer exists. When the Pod stops using the PersistentVolume, Kubernetes clears the pv-protection finalizer, and the controller deletes the volume.","v":"26","_id":"afc25b00c2beeeff979d43b85a0e82a6","_rev":"1-db053a528326e326d41b2e79f184ac59","_revisions":{"start":1,"ids":["db053a528326e326d41b2e79f184ac59"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736513974758,"range":{"startContainerPath":"//h3[@id=\"kube-proxy\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()[3]","endOffset":9},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-proxy (optional) kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.","v":"26","_id":"b1a2c5558615e3564a963933300bd136","_rev":"1-d533e0ca3dab71328fc1ae5f5737a205","_revisions":{"start":1,"ids":["d533e0ca3dab71328fc1ae5f5737a205"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430837942,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[2]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Kubernetes API can be extended in one of two ways:  Custom resources let you declaratively define how the API server should provide your chosen resource API. You can also extend the Kubernetes API by implementing an aggregation layer.","v":"26","_id":"b1f7334464a782ac45340278b07bd655","_rev":"1-10b7df57693e9cb50ee7e55786fe89b4","_revisions":{"start":1,"ids":["10b7df57693e9cb50ee7e55786fe89b4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430547356,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","endOffset":114},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.","v":"26","_id":"b2d2efc224fa7aaa31f5f86ac3cb9015","_rev":"1-fb34a1538a3bf69f4deda3bf33d4774f","_revisions":{"start":1,"ids":["fb34a1538a3bf69f4deda3bf33d4774f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/","date":1736428144233,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","startOffset":177,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":233},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The definition of what an application contains is loose.","v":"26","_id":"b3760255b313362739ea1ca9c4da1ba4","_rev":"1-513b34ae77343c8f2a0a698800e6e1ae","_revisions":{"start":1,"ids":["513b34ae77343c8f2a0a698800e6e1ae"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/","date":1736419176534,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[4]","startOffset":103,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[2]","endOffset":213},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"when a Job creates one or more Pods, the Job controller applies labels to those pods and tracks changes to any Pods in the cluster with the same label.  The Job controller also adds owner references to those Pods, pointing at the Job that created the Pods. If you delete the Job while these Pods are running, Kubernetes uses the owner references (not labels) to determine which Pods in the cluster need cleanup.","v":"26","_id":"b4fb0f46f604aa161dcc31d8e8ca090a","_rev":"1-c76a951efa33b7fd2fe1fd8ed37c60be","_revisions":{"start":1,"ids":["c76a951efa33b7fd2fe1fd8ed37c60be"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/","date":1736428120227,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":109},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In addition to supporting tooling, the recommended labels describe applications in a way that can be queried.","v":"26","first":true,"title":"Recommended Labels | Kubernetes","_id":"b54fa73d63c8b78bbe9f99d6829cc10d","_rev":"1-852d2d952fc134e67efd36ef4d7666fe","_revisions":{"start":1,"ids":["852d2d952fc134e67efd36ef4d7666fe"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430446709,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()","endOffset":132},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes implements an alternative Protobuf based serialization format that is primarily intended for intra-cluster communication.","v":"26","_id":"b674003e79719c450c8b7f92933e9131","_rev":"1-5fbc61034e20c5d96b95fef6f1885472","_revisions":{"start":1,"ids":["5fbc61034e20c5d96b95fef6f1885472"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736513980436,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-proxy maintains network rules on nodes.","v":"26","_id":"b7b01f54020ce1152867a6604b868b08","_rev":"1-fde17ce13bf3232b1df1e5f21285b417","_revisions":{"start":1,"ids":["fde17ce13bf3232b1df1e5f21285b417"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/","date":1736428187329,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Shared labels and annotations share a common prefix: app.kubernetes.io. Labels without a prefix are private to users.","v":"26","_id":"b7ecd447071d91e568bc8de981abfe93","_rev":"1-36752d5dd083bef78c882048c91b7112","_revisions":{"start":1,"ids":["36752d5dd083bef78c882048c91b7112"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736504540582,"range":{"startContainerPath":"//h3[@id=\"cloud-controller-manager\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/text()[2]","endOffset":266},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"cloud-controller-manager A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.","v":"26","_id":"bc5b3617e219ef06bbea0fba5eea86cb","_rev":"1-22f9176f1b380b75e2457472f99dbe3f","_revisions":{"start":1,"ids":["22f9176f1b380b75e2457472f99dbe3f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514116908,"range":{"startContainerPath":"//h3[@id=\"container-runtime\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()","endOffset":79},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container runtime A fundamental component that empowers Kubernetes to run containers effectively.","v":"26","_id":"bc745f06edd2dde5df86d9915f785f7e","_rev":"1-1e84e85decec5659031d3e6243f36b0d","_revisions":{"start":1,"ids":["1e84e85decec5659031d3e6243f36b0d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435855194,"range":{"startContainerPath":"//h3[@id=\"etcd\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"etcd Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.  If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.  You can find in-depth information about etcd in the official https://etcd.io/docs/documentation.","v":"26","_id":"bcf8a7ddd6339328c21a17e7dc792368","_rev":"1-ac2cad2c0bf0a4b65f10308dec60d8a5","_revisions":{"start":1,"ids":["ac2cad2c0bf0a4b65f10308dec60d8a5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435921021,"range":{"startContainerPath":"//h3[@id=\"kube-scheduler\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":249},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-scheduler Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.  Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.","v":"26","_id":"be45e3c0626a1f948359b1f5f8a35af3","_rev":"1-340d254e19d360156e7b1645ae175dea","_revisions":{"start":1,"ids":["340d254e19d360156e7b1645ae175dea"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435604862,"range":{"startContainerPath":"//h3[@id=\"kube-apiserver\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":191},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-apiserver The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.  The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.","v":"26","_id":"bf30d9b6596ce3260a15c095ea413af2","_rev":"1-adac22e1c56843c194738742e2697b38","_revisions":{"start":1,"ids":["adac22e1c56843c194738742e2697b38"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/","date":1736427526163,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[5]","endOffset":96},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes also adds finalizers to an owner resource when you use either foreground or orphan cascading deletion. In foreground deletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true before it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds the orphan finalizer so that the controller ignores dependent resources after it deletes the owner object.","v":"26","_id":"c17b6163618672642da8457c57a8abb4","_rev":"1-e48ae53d4a564155efdfab518484a1d4","_revisions":{"start":1,"ids":["e48ae53d4a564155efdfab518484a1d4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736435632107,"range":{"startContainerPath":"//h3[@id=\"etcd\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"etcd Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.  If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.  You can find in-depth information about etcd in the official documentation.","v":"26","_id":"c5439bcf503aedd15033fb00494182bf","_rev":"1-27961c41a8f1b1ba76c47c0c1774e69e","_revisions":{"start":1,"ids":["27961c41a8f1b1ba76c47c0c1774e69e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736434599219,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","startOffset":28,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[4]","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"as well as detecting and responding to cluster events (for example, starting up a new pod when a Deployment's replicas field is unsatisfied).","v":"26","_id":"c7c53c74d0cc5960435e7f573b2d8322","_rev":"1-ba87cb3ed657e827445a0e0f73f2db3d","_revisions":{"start":1,"ids":["ba87cb3ed657e827445a0e0f73f2db3d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736434585342,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","startOffset":28,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":81},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"as well as detecting and responding to cluster events","v":"26","_id":"c8ba87c07898d88e5fd5bc3a9a23b70a","_rev":"1-e6b263f70a7ce243ec4a963d90839f58","_revisions":{"start":1,"ids":["e6b263f70a7ce243ec4a963d90839f58"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736434576945,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","endOffset":70},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The control plane's components make global decisions about the cluster","v":"26","_id":"c93da4ee6b4a92f68e1e7b405e319552","_rev":"1-67b37f0cc4e5cdd863c50ada9fba36bf","_revisions":{"start":1,"ids":["67b37f0cc4e5cdd863c50ada9fba36bf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736429570657,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","startOffset":104,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/p/text()","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The two supported mechanisms are as follows:  The Discovery API provides information about the Kubernetes APIs","v":"26","_id":"c952554c0b57c13a3e4f457846a56331","_rev":"1-be55a3435bcdbeb38a7fba3979c3c20c","_revisions":{"start":1,"ids":["be55a3435bcdbeb38a7fba3979c3c20c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/","date":1736426956121,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":137},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Dependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object.","v":"26","_id":"cb3c223d34c0edbfc900045828927184","_rev":"1-61de24ca8a58d7a7f39d897e0246d716","_revisions":{"start":1,"ids":["61de24ca8a58d7a7f39d897e0246d716"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514128145,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).","v":"26","_id":"ceda49887d6de8f4bdc2fb46a37a1c0e","_rev":"1-2ec65c23f575f12b16ae063136c3592f","_revisions":{"start":1,"ids":["2ec65c23f575f12b16ae063136c3592f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736428897338,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[3]","endOffset":144},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The core of Kubernetes' control plane is the API server. The API server exposes an HTTP API that lets end users, different parts of your cluster, and external components communicate with one another.","v":"26","first":true,"title":"The Kubernetes API | Kubernetes","_id":"d00ff82e11f2bbb2276e56b6ddfda524","_rev":"1-2e1ae55e33664f1cc80c313040ee3b3b","_revisions":{"start":1,"ids":["2e1ae55e33664f1cc80c313040ee3b3b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736431424082,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":130},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes, that run containerized applications.","v":"26","first":true,"title":"Cluster Architecture | Kubernetes","_id":"d112dce957134561ccba332a2f8e1a93","_rev":"1-7059cd612a59b8d16bb5c4b76040cfb4","_revisions":{"start":1,"ids":["7059cd612a59b8d16bb5c4b76040cfb4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736504579670,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","endOffset":247},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.","v":"26","_id":"d22ec261cb58c54ec6b85c31b62ebb1e","_rev":"1-510006693aab9896007938311eb4ccf0","_revisions":{"start":1,"ids":["510006693aab9896007938311eb4ccf0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736429471556,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":87},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes","v":"26","_id":"d342ce29f25066a91a2485c63bd3f15e","_rev":"1-b8041ee5bf36a0774bd6ebd97fffce38","_revisions":{"start":1,"ids":["b8041ee5bf36a0774bd6ebd97fffce38"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736512051383,"range":{"startContainerPath":"//h2[@id=\"node-components\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()","endOffset":109},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node components Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.","v":"26","_id":"d551e7b97d8be0247c0c2f76abb23120","_rev":"1-af92250bfdfe909548447177a5ede958","_revisions":{"start":1,"ids":["af92250bfdfe909548447177a5ede958"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736512869625,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/text()[2]","startOffset":155,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/text()[2]","endOffset":230},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubelet doesn't manage containers which were not created by Kubernetes.","v":"26","_id":"daf62b9e7736e7a39dac15b88909b89d","_rev":"1-54fdcb24f32b8b0f2133491fd101cc26","_revisions":{"start":1,"ids":["54fdcb24f32b8b0f2133491fd101cc26"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736513984608,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()","endOffset":146},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.","v":"26","_id":"dd1a9d2d6774193ff7ebb1b56855ecf0","_rev":"1-94aa399fdba45809e4570aff2f1e7ac2","_revisions":{"start":1,"ids":["94aa399fdba45809e4570aff2f1e7ac2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736429498260,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[3]","endOffset":92},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Most operations can be performed through the kubectl command-line interface or other command-line tools, such as kubeadm, which in turn use the API. However, you can also access the API directly using REST calls.","v":"26","_id":"dd86d4c6d8b0d784153e0833a69e74cf","_rev":"1-725f354fbc3b56082697ffa5dfe098d7","_revisions":{"start":1,"ids":["725f354fbc3b56082697ffa5dfe098d7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/","date":1736424508650,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[6]","startOffset":64,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[6]","endOffset":166},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Owner references help different parts of Kubernetes avoid interfering with objects they don’t control.","v":"26","first":true,"title":"Owners and Dependents | Kubernetes","_id":"ddaa37a9a2403f8a8c5ca25007c9726f","_rev":"1-1762937bf5a671c3e73c44fd368fe20f","_revisions":{"start":1,"ids":["1762937bf5a671c3e73c44fd368fe20f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430598894,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","endOffset":114},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.","v":"26","_id":"de4486b0fef66fa0089f5116b9442ad8","_rev":"1-b437c15ebfda63bc2dee65e0c038f60e","_revisions":{"start":1,"ids":["b437c15ebfda63bc2dee65e0c038f60e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736430455998,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes stores the serialized state of objects by writing them into etcd.","v":"26","_id":"de7899a8d64c72632ac526d4705e5df2","_rev":"1-6c7daa3397d8c4952d532376d28de35c","_revisions":{"start":1,"ids":["6c7daa3397d8c4952d532376d28de35c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/","date":1736424522856,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[4]","endOffset":25},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Dependent objects have a metadata.ownerReferences field that references their owner object. A valid owner reference consists of the object name and a UID within the same namespace as the dependent object.","v":"26","_id":"df7534d6521f88d5a31071d850c41248","_rev":"1-9086b86e5ac9ea39d2614906b4f0a412","_revisions":{"start":1,"ids":["9086b86e5ac9ea39d2614906b4f0a412"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736434595028,"correspondingDocumentId":"c8ba87c07898d88e5fd5bc3a9a23b70a","_id":"e17f6791-ee8c-4926-95e9-cd0f7994dc62","_rev":"1-de0a98645d350411d34a7bc9d773f8ae","_revisions":{"start":1,"ids":["de0a98645d350411d34a7bc9d773f8ae"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736434635181,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":219},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control plane components on the same machine, and do not run user containers on this machine.","v":"26","_id":"e6aaa97e4d54c19781ed4d0ab0e67fc8","_rev":"1-7182351b2f486c1fa41de71416002993","_revisions":{"start":1,"ids":["7182351b2f486c1fa41de71416002993"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/","date":1736428137738,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The metadata is organized around the concept of an application.","v":"26","_id":"f0ad3d6c285fb851ab4b2de68560b44f","_rev":"1-dae2c2fd077293fde92bb45bce9c46bb","_revisions":{"start":1,"ids":["dae2c2fd077293fde92bb45bce9c46bb"]}},{"_deleted":true,"_id":"f635a3ab1afb4f0916a8ecc87c0ef6f5","_rev":"2-6e6fcfd095335e9c4a6dc913a1b481d9","_revisions":{"start":2,"ids":["6e6fcfd095335e9c4a6dc913a1b481d9","aee7515f40a094e238cad462f136fe31"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/overview/kubernetes-api/","date":1736429586411,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/p/text()[3]","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Kubernetes OpenAPI Document provides (full) OpenAPI v2.0 and 3.0 schemas for all Kubernetes API endpoints.","v":"26","_id":"f9cfc650784f3c815ccc4ee0306eff15","_rev":"1-b58774d00f0fb8594328b3873dfacc6b","_revisions":{"start":1,"ids":["b58774d00f0fb8594328b3873dfacc6b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736503401914,"range":{"startContainerPath":"//h3[@id=\"kube-controller-manager\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kube-controller-manager Control plane component that runs controller processes.  Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.  There are many different types of controllers. Some examples of them are:  Node controller: Responsible for noticing and responding when nodes go down. Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion. EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods). ServiceAccount controller: Create default ServiceAccounts for new namespaces. The above is not an exhaustive list.","v":"26","_id":"fcecd69aabc3ece968e50590befa9693","_rev":"1-60f36c3f714745da2fcf6e9dcffa728c","_revisions":{"start":1,"ids":["60f36c3f714745da2fcf6e9dcffa728c"]}}]}

{"seq":261}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771049156,"correspondingDocumentId":"ffd6d777547a9057c936d44173643e4b","_id":"27eb2612-ece8-47e5-8400-eda916336955","_rev":"1-b75ef892a3d754e356515cc409bce823","_revisions":{"start":1,"ids":["b75ef892a3d754e356515cc409bce823"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514233914,"correspondingDocumentId":"d1252f9ad0eaf3aaf98d6772c7a76791","_id":"2d89972f-789c-4b3a-b986-bc6fdc0f1352","_rev":"1-0355994c4f8371c5f71d93a599c873f8","_revisions":{"start":1,"ids":["0355994c4f8371c5f71d93a599c873f8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759467020,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The third is monitoring the nodes' health.","v":"26","_id":"a059875e2fc014825a807d6c0afb1aed","_rev":"1-dae64b82bf4c52004d92ed0797346ca7","_revisions":{"start":1,"ids":["dae64b82bf4c52004d92ed0797346ca7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757642019,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/p/text()","startOffset":109,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/p/text()","endOffset":192},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"you set this value to be the IPv4 address that the kubelet should use for the node.","v":"26","_id":"a375c63886d94d419b922fea33710c62","_rev":"1-13a84b002803effcac9ffa938cbe4e1d","_revisions":{"start":1,"ids":["13a84b002803effcac9ffa938cbe4e1d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736770861735,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]/text()","endOffset":153},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When the memory swap feature is turned on, Kubernetes data such as the content of Secret objects that were written to tmpfs now could be swapped to disk.","v":"26","_id":"a625c69e93a41b3fa605fbc6a3fc0a1a","_rev":"1-a34fe4d29ba7f16f062f0bd816127b96","_revisions":{"start":1,"ids":["a34fe4d29ba7f16f062f0bd816127b96"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757477105,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()[2]","startOffset":20,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()[2]","endOffset":152},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Two Nodes cannot have the same name at the same time. Kubernetes also assumes that a resource with the same name is the same object.","v":"26","_id":"aa8cc0cb6bbea1be9af24fffc86bf5b1","_rev":"1-23474e24951766ddf940e466ed10ae3a","_revisions":{"start":1,"ids":["23474e24951766ddf940e466ed10ae3a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514201125,"range":{"startContainerPath":"//h2[@id=\"addons\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[31]/text()","endOffset":35},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Addons Addons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace.  Selected addons are described below","v":"26","_id":"aca4d3b993e00654e9d4af2c499ff30f","_rev":"1-c1df7efc2353618b18038df7c4073294","_revisions":{"start":1,"ids":["c1df7efc2353618b18038df7c4073294"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757857138,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/p[2]/text()","startOffset":317,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/p[2]/text()","endOffset":397},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node re-registration ensures all Pods will be drained and properly re-scheduled.","v":"26","_id":"b0dc9300922b2179a3fe8df44a670424","_rev":"1-ca3fcda8a97224195cbb30908a6aabac","_revisions":{"start":1,"ids":["ca3fcda8a97224195cbb30908a6aabac"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759524977,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()[3]","endOffset":11},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"By default, the node controller checks the state of each node every 5 seconds. This period can be configured using the --node-monitor-period flag on the kube-controller-manager component.","v":"26","_id":"b3181ba6b717ed0db7171be9c073976e","_rev":"1-78c344a99f6304f5129a1dfe25ccae23","_revisions":{"start":1,"ids":["78c344a99f6304f5129a1dfe25ccae23"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760869052,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[3]","startOffset":65,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[3]","endOffset":213},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"the node controller assumes that there is some problem with connectivity between the control plane and the nodes, and doesn't perform any evictions.","v":"26","_id":"b5d1ffd58cdf640b8c3cffc8cadd15fe","_rev":"1-e9924759f411e03484e216522bffea12","_revisions":{"start":1,"ids":["e9924759f411e03484e216522bffea12"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760905538,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[2]","startOffset":48,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[3]","endOffset":136},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The node controller also adds taints corresponding to node problems like node unreachable or not ready. This means that the scheduler won't place Pods onto unhealthy nodes.","v":"26","_id":"b876a1008828f5db22d6b072554cf5d0","_rev":"1-99bd7dea9059e6e8416e324d3617ddab","_revisions":{"start":1,"ids":["99bd7dea9059e6e8416e324d3617ddab"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736758506559,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[5]/h4/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[5]/text()[2]","endOffset":45},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Note: Pods that are part of a DaemonSet tolerate being run on an unschedulable Node.","v":"26","_id":"b9b7a3e91bdb7c46db5520cb416f0253","_rev":"1-d22ed232bfdbfd8ce6c81b2c61547cd0","_revisions":{"start":1,"ids":["d22ed232bfdbfd8ce6c81b2c61547cd0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736516346919,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[2]/text()","endOffset":54},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"There are two main ways to have Nodes added to the API server:  The kubelet on a node self-registers to the control plane You (or another human user) manually add a Node object","v":"26","_id":"bbed0fa93a5e5e71eaaa423b68ae6113","_rev":"1-ef71b5a8f5c4ed06a64ba4ecd039415a","_revisions":{"start":1,"ids":["ef71b5a8f5c4ed06a64ba4ecd039415a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736761092022,"range":{"startContainerPath":"//h2[@id=\"node-capacity\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[3]","endOffset":82},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Resource capacity tracking Node objects track information about the Node's resource capacity: for example, the amount of memory available and the number of CPUs. Nodes that self register report their capacity during registration. If you manually add a Node, then you need to set the node's capacity information when you add it.","v":"26","_id":"be54343e2673dbdb10271bc91e0c3670","_rev":"1-4ded1ee037a676c3c314475a02610125","_revisions":{"start":1,"ids":["4ded1ee037a676c3c314475a02610125"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759351051,"range":{"startContainerPath":"//h2[@id=\"node-controller\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node controller","v":"26","_id":"bfede322acff82534324242037d93be4","_rev":"1-9af1532ed74227313ce1a710b0d1a82c","_revisions":{"start":1,"ids":["9af1532ed74227313ce1a710b0d1a82c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514260699,"range":{"startContainerPath":"//h3[@id=\"network-plugins\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Network plugins","v":"26","_id":"c1e9b7c79991141ec1d95d402bd8584c","_rev":"1-7a0d83a6ad6d08b974d50fe6881e305e","_revisions":{"start":1,"ids":["7a0d83a6ad6d08b974d50fe6881e305e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760772220,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[3]/text()[2]","endOffset":27},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If the fraction of unhealthy nodes is at least --unhealthy-zone-threshold (default 0.55), then the eviction rate is reduced. If the cluster is small (i.e. has less than or equal to --large-cluster-size-threshold nodes - default 50), then evictions are stopped. Otherwise, the eviction rate is reduced to --secondary-node-eviction-rate (default 0.01) per second.","v":"26","_id":"c8e6f32c0f14e965749aa924f14f093d","_rev":"1-b9538b5ed0a22beace3a25e30564b9b5","_revisions":{"start":1,"ids":["b9538b5ed0a22beace3a25e30564b9b5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736515470757,"range":{"startContainerPath":"//h3[@id=\"customization-and-extensibility\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[45]/text()","endOffset":195},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Customization and extensibility Kubernetes architecture allows for significant customization:  Custom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely. API servers can be extended with CustomResourceDefinitions and API Aggregation. Cloud providers can integrate deeply with Kubernetes using the cloud-controller-manager. The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as operational complexity, performance, and management overhead.","v":"26","_id":"ca5a62b9690d46a18e1dbab2debe075b","_rev":"1-1f37309a5517492701ea4e11d4441db0","_revisions":{"start":1,"ids":["1f37309a5517492701ea4e11d4441db0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759274428,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/text()[3]","endOffset":43},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For nodes there are two forms of heartbeats:  Updates to the .status of a Node. Lease objects within the kube-node-lease namespace. Each Node has an associated Lease object.","v":"26","_id":"cb0420c3e5446ea3deac0860a9f07198","_rev":"1-1d7a99caedd15da00244c55ec3c36ccb","_revisions":{"start":1,"ids":["1d7a99caedd15da00244c55ec3c36ccb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736756459299,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[3]","startOffset":43,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[3]","endOffset":77},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"then it is eligible to run a Pod.","v":"26","_id":"cdb9260294bfff7f91135f96e4955b38","_rev":"1-ba2a784fbc8b2a9f0f730e6829ebf4e5","_revisions":{"start":1,"ids":["ba2a784fbc8b2a9f0f730e6829ebf4e5"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736522653554,"correspondingDocumentId":"f3b6dfd1b0bb99eab317e98e61a184bf","_id":"cde8955f-7354-4947-9092-ddc0de9bd430","_rev":"1-5032cdbda34f35baa88466cf6f1324ba","_revisions":{"start":1,"ids":["5032cdbda34f35baa88466cf6f1324ba"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757611026,"correspondingDocumentId":"fe65778baafd1e66ed64b23ef317e4c5","_id":"d0016ece-e425-4e80-a1e3-b98e8b083933","_rev":"1-dc158f7efbb9824c669cc41ff6ca1598","_revisions":{"start":1,"ids":["dc158f7efbb9824c669cc41ff6ca1598"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760059711,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()[2]","endOffset":92},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In most cases, the node controller limits the eviction rate to --node-eviction-rate (default 0.1) per second, meaning it won't evict pods from more than 1 node per 10 seconds.","v":"26","_id":"d07cf63dd7621598c3caec36d4e4da4b","_rev":"1-8ddb2773058560c98618f29b6a5b9d58","_revisions":{"start":1,"ids":["8ddb2773058560c98618f29b6a5b9d58"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514204265,"range":{"startContainerPath":"//h3[@id=\"dns\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"dns\"]/text()","endOffset":3},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DNS","v":"26","_id":"d08ac33795feff86f1666bba5b6ce093","_rev":"1-0d0a022777f1eb1ab4bda491c67e68dc","_revisions":{"start":1,"ids":["0d0a022777f1eb1ab4bda491c67e68dc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757776281,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/p/text()[2]","startOffset":10,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/p/text()[3]","endOffset":147},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"when Node configuration needs to be updated, it is a good practice to re-register the node with the API server. For example, if the kubelet is being restarted with a new set of --node-labels, but the same Node name is used, the change will not take effect, as labels are only set (or modified) upon Node registration with the API server.","v":"26","_id":"d08d03876bb0a323f7c96d8131403182","_rev":"1-be52ef697580022084e1f22bcd10be5e","_revisions":{"start":1,"ids":["be52ef697580022084e1f22bcd10be5e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514207904,"range":{"startContainerPath":"//h3[@id=\"web-ui-dashboard\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Web UI (Dashboard)","v":"26","_id":"d1252f9ad0eaf3aaf98d6772c7a76791","_rev":"1-da5141ff64989b94fc9b89af2c3c858a","_revisions":{"start":1,"ids":["da5141ff64989b94fc9b89af2c3c858a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760859817,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[2]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[2]","endOffset":60},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The corner case is when all zones are completely unhealthy","v":"26","_id":"d2050faec6b0ec0b2c2779d5082c35af","_rev":"1-93c95fe0d7d28b4e541d81bd56f6db76","_revisions":{"start":1,"ids":["93c95fe0d7d28b4e541d81bd56f6db76"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759369134,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","endOffset":161},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The node controller has multiple roles in a node's life. The first is assigning a CIDR block to the node when it is registered (if CIDR assignment is turned on).","v":"26","_id":"d206bb129a0e4a9d52ea4129f44a7b64","_rev":"1-fb6e8da416e335de0b7069eba0fc3637","_revisions":{"start":1,"ids":["fb6e8da416e335de0b7069eba0fc3637"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736758476581,"range":{"startContainerPath":"//h3[@id=\"manual-node-administration\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()[3]","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Manual Node administration You can create and modify Node objects using kubectl.  When you want to create Node objects manually, set the kubelet flag --register-node=false.  You can modify Node objects regardless of the setting of --register-node. For example, you can set labels on an existing Node or mark it unschedulable.  You can use labels on Nodes in conjunction with node selectors on Pods to control scheduling. For example, you can constrain a Pod to only be eligible to run on a subset of the available nodes.  Marking a node as unschedulable prevents the scheduler from placing new pods onto that Node but does not affect existing Pods on the Node. This is useful as a preparatory step before a node reboot or other maintenance.  To mark a Node unschedulable, run:  kubectl cordon $NODENAME See (https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)Safely Drain a Node for more details.","v":"26","_id":"d292c4b772a44e479979c5a24eee7119","_rev":"1-6b52ca0a6f5cf7356afbeabe4d88e31f","_revisions":{"start":1,"ids":["6b52ca0a6f5cf7356afbeabe4d88e31f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736522682693,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes creates a Node object internally (the representation). Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node.","v":"26","_id":"d6b1532c41ea519013e29b6e77ddfcaf","_rev":"1-07fd4079e6e57ba008d34acdae4d28f5","_revisions":{"start":1,"ids":["07fd4079e6e57ba008d34acdae4d28f5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514251486,"range":{"startContainerPath":"//h3[@id=\"container-resource-monitoring\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container resource monitoring","v":"26","_id":"d771390c76c6868a82ac21280ef9e09b","_rev":"1-0b83207b5e32add618aed20b04796ae7","_revisions":{"start":1,"ids":["0b83207b5e32add618aed20b04796ae7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514709331,"range":{"startContainerPath":"//h3[@id=\"control-plane-deployment-options\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[4]/text()","endOffset":113},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Control plane deployment options The control plane components can be deployed in several ways:  Traditional deployment Control plane components run directly on dedicated machines or VMs, often managed as systemd services. Static Pods Control plane components are deployed as static Pods, managed by the kubelet on specific nodes. This is a common approach used by tools like kubeadm. Self-hosted The control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments and StatefulSets or other Kubernetes primitives. Managed Kubernetes services Cloud providers often abstract away the control plane, managing its components as part of their service offering.","v":"26","_id":"d7e654ab46ff918cc353fc7693ced86b","_rev":"1-8f1873056d1095ec75dde6d4531ee542","_revisions":{"start":1,"ids":["8f1873056d1095ec75dde6d4531ee542"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514242115,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"While the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it.  Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.  Containers started by Kubernetes automatically include this DNS server in their DNS searches.","v":"26","_id":"d9a99bf53d5ba197b4e9dac8f986aa67","_rev":"1-6224d23a16e6d63ec4b8eeb262f522c5","_revisions":{"start":1,"ids":["6224d23a16e6d63ec4b8eeb262f522c5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736516109518,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.","v":"26","first":true,"title":"Nodes | Kubernetes","_id":"dbd5cbf25b5e6a19a649e0c26fe0ad6f","_rev":"1-99b005e087eb993e2ca14d3ea41275fa","_revisions":{"start":1,"ids":["99b005e087eb993e2ca14d3ea41275fa"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759380317,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()","endOffset":131},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The second is keeping the node controller's internal list of nodes up to date with the cloud provider's list of available machines.","v":"26","_id":"dd176e219eeda377b8fa3572e38edbce","_rev":"1-be589cb805024f36d5ff69f4ed4301ac","_revisions":{"start":1,"ids":["be589cb805024f36d5ff69f4ed4301ac"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514256270,"range":{"startContainerPath":"//h3[@id=\"cluster-level-logging\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Cluster-level Logging","v":"26","_id":"dec617e2609ad0fe8ebcef728bab95d3","_rev":"1-00b0aa7bd7493a019c513004deab3443","_revisions":{"start":1,"ids":["00b0aa7bd7493a019c513004deab3443"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757622621,"range":{"startContainerPath":"//h3[@id=\"self-registration-of-nodes\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/p/text()","endOffset":66},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Self-registration of Nodes When the kubelet flag --register-node is true (the default), the kubelet will attempt to register itself with the API server. This is the preferred pattern, used by most distros.  For self-registration, the kubelet is started with the following options:  --kubeconfig - Path to credentials to authenticate itself to the API server.  --cloud-provider - How to talk to a cloud provider to read metadata about itself.  --register-node - Automatically register with the API server.  --register-with-taints - Register the node with the given list of taints (comma separated <key>=<value>:<effect>).  No-op if register-node is false.  --node-ip - Optional comma-separated list of the IP addresses for the node.","v":"26","_id":"e073c19661046b1b9476ab9b8798dce1","_rev":"1-cdc786684f3d1b06d036714cea07071d","_revisions":{"start":1,"ids":["cdc786684f3d1b06d036714cea07071d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759400543,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()[2]","startOffset":71,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()[2]","endOffset":227},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"the node controller asks the cloud provider if the VM for that node is still available. If not, the node controller deletes the node from its list of nodes.","v":"26","_id":"e67915e0dd6ca8b19965924e8a462bb8","_rev":"1-0edb0d9aa2a848053814e64694113339","_revisions":{"start":1,"ids":["0edb0d9aa2a848053814e64694113339"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757675894,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[7]/p/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[7]/p/text()","endOffset":71},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"--node-status-update-frequency - Specifies how often kubelet posts its node status to the API server.","v":"26","_id":"e90a9bbbbd44fbcaeb4763f375db59ba","_rev":"1-e2f01248e47b5c1e81e0c70c4716cc05","_revisions":{"start":1,"ids":["e2f01248e47b5c1e81e0c70c4716cc05"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736516112461,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"management\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The components on a node include the kubelet, a container runtime, and the kube-proxy.","v":"26","_id":"eb3b5b692937c45a4dde8b70454324e3","_rev":"1-7230e12f00d5c78d2eb2ae10459712e8","_revisions":{"start":1,"ids":["7230e12f00d5c78d2eb2ae10459712e8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757445407,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"node-name-uniqueness\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The name of a Node object must be a valid DNS subdomain name.","v":"26","_id":"eb41856d7517981177bf86da111a9e94","_rev":"1-bcf915b3037cb58452ba54ce6bd61c6e","_revisions":{"start":1,"ids":["bcf915b3037cb58452ba54ce6bd61c6e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514246971,"range":{"startContainerPath":"//h3[@id=\"web-ui-dashboard\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"web-ui-dashboard\"]/text()","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Web UI (Dashboard)","v":"26","_id":"edef897cf05e1685e8a2c004dbd218e6","_rev":"1-1cde946067d2bea9307ffac488b3d88d","_revisions":{"start":1,"ids":["1cde946067d2bea9307ffac488b3d88d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736759473765,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()[2]","startOffset":46,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()[3]","endOffset":43},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"By default, the node controller waits 5 minutes between marking the node as Unknown and submitting the first eviction request.","v":"26","_id":"eee2cb47782d4bf6fadecdf6c0d9547c","_rev":"1-77dad395a7ee6bcfec4d4926ccfa245c","_revisions":{"start":1,"ids":["77dad395a7ee6bcfec4d4926ccfa245c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736516613694,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":66,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node.","v":"26","_id":"f3b6dfd1b0bb99eab317e98e61a184bf","_rev":"1-9227cc2efa2a49215dabcfbedadff8fb","_revisions":{"start":1,"ids":["9227cc2efa2a49215dabcfbedadff8fb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757667608,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[6]/p/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[6]/p/text()[2]","endOffset":48},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"--node-labels - Labels to add when registering the node in the cluster","v":"26","_id":"f56abc90ef7ef837609157a42681182e","_rev":"1-153c0e1c26aa0f2b81b4ed827b745543","_revisions":{"start":1,"ids":["153c0e1c26aa0f2b81b4ed827b745543"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736760755055,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","endOffset":94},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The node eviction behavior changes when a node in a given availability zone becomes unhealthy.","v":"26","_id":"f98a9dea3e4272898a5dcba29f026b97","_rev":"1-427db2fc168cf0e9ca04fd66525a6284","_revisions":{"start":1,"ids":["427db2fc168cf0e9ca04fd66525a6284"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736756454086,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If the node is healthy","v":"26","_id":"fb8949deb8aadb4896efd3e365ceda91","_rev":"1-e79ad4d9d5e679ebdc7e1cc37be0cf1e","_revisions":{"start":1,"ids":["e79ad4d9d5e679ebdc7e1cc37be0cf1e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736757602026,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/p/text()","endOffset":66},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When the kubelet flag --register-node is true (the default), the kubelet will attempt to register itself with the API server. This is the preferred pattern, used by most distros.  For self-registration, the kubelet is started with the following options:  --kubeconfig - Path to credentials to authenticate itself to the API server.  --cloud-provider - How to talk to a cloud provider to read metadata about itself.  --register-node - Automatically register with the API server.  --register-with-taints - Register the node with the given list of taints (comma separated <key>=<value>:<effect>).  No-op if register-node is false.  --node-ip - Optional comma-separated list of the IP addresses for the node.","v":"26","_id":"fe65778baafd1e66ed64b23ef317e4c5","_rev":"1-33e9aa6200853d4a4d265db42925e386","_revisions":{"start":1,"ids":["33e9aa6200853d4a4d265db42925e386"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/","date":1736514624442,"range":{"startContainerPath":"//h2[@id=\"architecture-variations\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"architecture-variations\"]/text()","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Architecture variations","v":"26","_id":"ff36dd516e4048073e68f5cebc6e919b","_rev":"1-ee54c1a9395cc1bd0b10b9f51bd5320d","_revisions":{"start":1,"ids":["ee54c1a9395cc1bd0b10b9f51bd5320d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771042037,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]/text()[7]","endOffset":55},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To enable swap on a node, the NodeSwap feature gate must be enabled on the kubelet (default is true), and the --fail-swap-on command line flag or failSwapOn configuration setting must be set to false. To allow Pods to utilize swap, swapBehavior should not be set to NoSwap (which is the default behavior) in the kubelet config.","v":"26","_id":"ffd6d777547a9057c936d44173643e4b","_rev":"1-3195dacb85ce097046291a10e7102abc","_revisions":{"start":1,"ids":["3195dacb85ce097046291a10e7102abc"]}}]}

{"seq":311}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782154087,"correspondingDocumentId":"ddfb2a7834c674cdec60526ae0db8588","_id":"1b0e6a22-cde2-420b-8eaf-397df0a3b101","_rev":"1-81c5473ddbcc38e4b0cfa21f9d691ca5","_revisions":{"start":1,"ids":["81c5473ddbcc38e4b0cfa21f9d691ca5"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771051657,"correspondingDocumentId":"a625c69e93a41b3fa605fbc6a3fc0a1a","_id":"2a0bbdb3-8c6a-4efc-9d79-326f77a09754","_rev":"1-5d6d1e401092e7bc4d580d4fcf7fddbc","_revisions":{"start":1,"ids":["5d6d1e401092e7bc4d580d4fcf7fddbc"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781962559,"correspondingDocumentId":"f709569f0dad8843f86f4ad4e10fbe4b","_id":"61c8fc60-2523-459c-9583-d792d2715ca2","_rev":"1-36e6eeb31f1e34ca9b0b8fb4b607cc1c","_revisions":{"start":1,"ids":["36e6eeb31f1e34ca9b0b8fb4b607cc1c"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773450694,"correspondingDocumentId":"f51859324930f2e2e4bb1c2cbcf4e0f0","_id":"85c4a11a-bc0a-421e-bae2-ba61f2c2e79b","_rev":"1-a1fd77cc9b09d2ee585fe4ad7acd6863","_revisions":{"start":1,"ids":["a1fd77cc9b09d2ee585fe4ad7acd6863"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736775152621,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","endOffset":176},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"It's useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so Kubernetes is designed to allow for that.","v":"26","_id":"a00bb324e27f90727cb4884578a64e38","_rev":"1-bfcb8908c83143b95eb86192e60680a7","_revisions":{"start":1,"ids":["bfcb8908c83143b95eb86192e60680a7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/leases/","date":1736777896047,"range":{"startContainerPath":"//h2[@id=\"leader-election\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[3]","endOffset":133},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Leader election Kubernetes also uses Leases to ensure only one instance of a component is running at any given time. This is used by control plane components like kube-controller-manager and kube-scheduler in HA configurations, where only one instance of the component should be actively running while the other instances are on stand-by.","v":"26","_id":"a040eaf35718091f3e91586289840edf","_rev":"1-96e0f4a51aa8d875739774da26f95f0c","_revisions":{"start":1,"ids":["96e0f4a51aa8d875739774da26f95f0c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736774687572,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":177,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","endOffset":314},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Job controller does not run any Pods or containers itself. Instead, the Job controller tells the API server to create or remove Pods.","v":"26","_id":"a047f54cb125df5cc11736f1af5c2cd7","_rev":"1-0c3178fa88da09fe159bf56f9ac868b6","_revisions":{"start":1,"ids":["0c3178fa88da09fe159bf56f9ac868b6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/leases/","date":1736775967639,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[5]","endOffset":110},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Distributed systems often have a need for leases, which provide a mechanism to lock shared resources and coordinate activity between members of a set. In Kubernetes, the lease concept is represented by Lease objects in the coordination.k8s.io API Group, which are used for system-critical capabilities such as node heartbeats and component-level leader election.","v":"26","first":true,"title":"Leases | Kubernetes","_id":"a2b83dbddb8c385d5de24ae07934e237","_rev":"1-33d7100675d750c15577d3a046e7e08b","_revisions":{"start":1,"ids":["33d7100675d750c15577d3a046e7e08b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736775073850,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","endOffset":145},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"As long as the controllers for your cluster are running and able to make useful changes, it doesn't matter if the overall state is stable or not.","v":"26","_id":"a38e681a1494046020e4485bf2722e17","_rev":"1-0711fcc14d4eb01a7a589876ab2c9874","_revisions":{"start":1,"ids":["0711fcc14d4eb01a7a589876ab2c9874"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771058212,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]/h4","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To enable swap on a node, the NodeSwap feature gate must be enabled on the kubelet (default is true), and the --fail-swap-on command line flag or failSwapOn configuration setting must be set to false. To allow Pods to utilize swap, swapBehavior should not be set to NoSwap (which is the default behavior) in the kubelet config.","v":"26","_id":"a44032b34ad5006e55e407a15826c80d","_rev":"1-96e74f97dcea00d880316f0f43979989","_revisions":{"start":1,"ids":["96e74f97dcea00d880316f0f43979989"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cloud-controller/","date":1736779200375,"range":{"startContainerPath":"//h3[@id=\"route-controller\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":183},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Route controller The route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in your Kubernetes cluster can communicate with each other.","v":"26","_id":"a73bf3d39e48be1ebe5a44b682664bdf","_rev":"1-d0730c14335c2e2b92194f9752791eba","_revisions":{"start":1,"ids":["d0730c14335c2e2b92194f9752791eba"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773837414,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":61},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Job is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and then stop.","v":"26","_id":"a7be46d89de825a41c4743b20dcf16be","_rev":"1-f7ca4fba74142a082853af6864526246","_revisions":{"start":1,"ids":["f7ca4fba74142a082853af6864526246"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782192665,"correspondingDocumentId":"acbdc14786b18fded54a8b769fdf69fd","_id":"a9527225-13b3-45b2-b738-3ab3d520c613","_rev":"1-e33e387ae41077e89e321aeb18ff867d","_revisions":{"start":1,"ids":["e33e387ae41077e89e321aeb18ff867d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/leases/","date":1736778466324,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]/pre/code/span/span/text()[2]","endOffset":14},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Starting in Kubernetes v1.26, each kube-apiserver uses the Lease API to publish its identity to the rest of the system. While not particularly useful on its own, this provides a mechanism for clients to discover how many instances of kube-apiserver are operating the Kubernetes control plane. Existence of kube-apiserver leases enables future capabilities that may require coordination between each kube-apiserver.  You can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-system namespace with the name kube-apiserver-<sha256-hash>. Alternatively you can use the label selector apiserver.kubernetes.io/identity=kube-apiserver:  kubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver","v":"26","_id":"aab249e33e344e0a14f115548767e820","_rev":"1-f858ca4015d9534a9bea4b49f50e7aa2","_revisions":{"start":1,"ids":["f858ca4015d9534a9bea4b49f50e7aa2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782100102,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()[3]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Disk usage above the configured HighThresholdPercent value triggers garbage collection, which deletes images in order based on the last time they were used, starting with the oldest first. The kubelet deletes images until disk usage reaches the LowThresholdPercent value.","v":"26","_id":"acbdc14786b18fded54a8b769fdf69fd","_rev":"1-90978fd9306eb86d5c69aa0ac4c1fd5c","_revisions":{"start":1,"ids":["90978fd9306eb86d5c69aa0ac4c1fd5c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781981860,"range":{"startContainerPath":"//h3[@id=\"background-deletion\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","endOffset":462},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Background cascading deletion In background cascading deletion, the Kubernetes API server deletes the owner object immediately and the garbage collector controller (custom or default) cleans up the dependent objects in the background. If a finalizer exists, it ensures that objects are not deleted until all necessary clean-up tasks are completed. By default, Kubernetes uses background cascading deletion unless you manually use foreground deletion or choose to orphan the dependent objects.","v":"26","_id":"aef102451153bc6d2bfd50b2dbe439e8","_rev":"1-a2bf019fc4a96249f6a97a8ad5477a55","_revisions":{"start":1,"ids":["a2bf019fc4a96249f6a97a8ad5477a55"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cloud-controller/","date":1736779065687,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","endOffset":151},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds. Kubernetes believes in automated, API-driven infrastructure without tight coupling between components.  The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.  By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.  The cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their platforms with Kubernetes.","v":"26","first":true,"title":"Cloud Controller Manager | Kubernetes","_id":"af4bdfa1b0ac8690599474b2eccd2fe1","_rev":"1-7b0f4eb3b7eb4788a2690e8e68062cc9","_revisions":{"start":1,"ids":["7b0f4eb3b7eb4788a2690e8e68062cc9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781127358,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[8]/a/text()","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources. This allows the clean up of resources like the following:  Terminated pods Completed Jobs Objects without owner references Unused containers and container images Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete Stale or expired CertificateSigningRequests (CSRs) Nodes deleted in the following scenarios: On a cloud when the cluster uses a cloud controller manager On-premises when the cluster uses an addon similar to a cloud controller manager Node Lease objects","v":"26","first":true,"title":"Garbage Collection | Kubernetes","_id":"af65f5641832cb1bc1458de0a1c55f30","_rev":"1-9ed71bb683709b53f260d84adcb2c04b","_revisions":{"start":1,"ids":["9ed71bb683709b53f260d84adcb2c04b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736772596964,"range":{"startContainerPath":"//h3[@id=\"api-server-to-kubelet\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","endOffset":54},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"API server to kubelet The connections from the API server to the kubelet are used for:  Fetching logs for pods. Attaching (usually through kubectl) to running pods. Providing the kubelet's port-forwarding functionality.","v":"26","_id":"b527fc4731444b42e9512b513c76e9a6","_rev":"1-76e120504c9571eb91f5e2cc739714dc","_revisions":{"start":1,"ids":["76e120504c9571eb91f5e2cc739714dc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773483965,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":53},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state.","v":"26","_id":"bd1b3b877173a65976902fd1aa37e3f6","_rev":"1-6e167b99099e17b066104507a55da012","_revisions":{"start":1,"ids":["6e167b99099e17b066104507a55da012"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736772648228,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()[2]","endOffset":45},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks.  To verify this connection, use the --kubelet-certificate-authority flag to provide the API server with a root certificate bundle to use to verify the kubelet's serving certificate.  If that is not possible, use SSH tunneling between the API server and kubelet if required to avoid connecting over an untrusted or public network.  Finally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.","v":"26","_id":"bef726f1029572d53d2d8d18148b1e8c","_rev":"1-30ad56deb5cd1720180117764ef81b62","_revisions":{"start":1,"ids":["30ad56deb5cd1720180117764ef81b62"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/leases/","date":1736778443871,"range":{"startContainerPath":"//h2[@id=\"api-server-identity\"]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"api-server-identity\"]/text()","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"API server identity","v":"26","_id":"c0efb11716b4a8d13260bb09a8e484de","_rev":"1-7a925a5c4e932707b0a7596546bc79f3","_revisions":{"start":1,"ids":["7a925a5c4e932707b0a7596546bc79f3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cloud-controller/","date":1736779214417,"range":{"startContainerPath":"//h3[@id=\"service-controller\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","endOffset":147},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Service controller Services integrate with cloud infrastructure components such as managed load balancers, IP addresses, network packet filtering, and target health checking.","v":"26","_id":"c24dd3ca24d5738db68b490cb6aee077","_rev":"1-06fff921d6cabecdb5fd893ef8f56632","_revisions":{"start":1,"ids":["06fff921d6cabecdb5fd893ef8f56632"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773424147,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/text()[2]","endOffset":129},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In robotics and automation, a control loop is a non-terminating loop that regulates the state of a system.  Here is one example of a control loop: a thermostat in a room.  When you set the temperature, that's telling the thermostat about your desired state. The actual room temperature is the current state. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off.  In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.","v":"26","first":true,"title":"Controllers | Kubernetes","_id":"c435e33a2ff8e73d06fe67e300e63166","_rev":"1-db89d979f9bbe95279f11b68e20b8daf","_revisions":{"start":1,"ids":["db89d979f9bbe95279f11b68e20b8daf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771054272,"range":{"startContainerPath":"//h2[@id=\"swap-memory\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Swap memory management","v":"26","_id":"c8c632fec08f968790445da7ed06991b","_rev":"1-5331b53bfe8557c115ab1a91b1a030ce","_revisions":{"start":1,"ids":["5331b53bfe8557c115ab1a91b1a030ce"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782196154,"range":{"startContainerPath":"//h3[@id=\"container-image-lifecycle\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container image lifecycle","v":"26","_id":"ca4482aefd46136d68b9340b23e6373f","_rev":"1-f24e3ca48d9340cf6a90c557bf5e3c3f","_revisions":{"start":1,"ids":["f24e3ca48d9340cf6a90c557bf5e3c3f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781813456,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":136,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When you delete an object, you can control whether Kubernetes deletes the object's dependents automatically, in a process called cascading deletion. There are two types of cascading deletion, as follows:  Foreground cascading deletion Background cascading deletion You can also control how and when garbage collection deletes resources that have owner references using Kubernetes finalizers.","v":"26","_id":"ce11c3df31cdc78499b2750a55c095f2","_rev":"1-d1a93e9c73792c778b34cad919baec1c","_revisions":{"start":1,"ids":["d1a93e9c73792c778b34cad919baec1c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cgroups/","date":1736779701212,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/h1/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":5},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"About cgroup v2 On Linux, control groups constrain resources that are allocated to processes.  The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads.  There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new generation of the cgroup API.","v":"26","first":true,"title":"About cgroup v2 | Kubernetes","_id":"d1e67c3b48956f8ce25d0b924f8bbdab","_rev":"1-17996fee6bf32d6b6591b3552adb3bcf","_revisions":{"start":1,"ids":["17996fee6bf32d6b6591b3552adb3bcf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781914886,"range":{"startContainerPath":"//h3[@id=\"foreground-deletion\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/text()","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Foreground cascading deletion In foreground cascading deletion, the owner object you're deleting first enters a deletion in progress state. In this state, the following happens to the owner object:  The Kubernetes API server sets the object's metadata.deletionTimestamp field to the time the object was marked for deletion. The Kubernetes API server also sets the metadata.finalizers field to foregroundDeletion. The object remains visible through the Kubernetes API until the deletion process is complete.","v":"26","_id":"d1f7256bfed7240e6c2b52d5950c9229","_rev":"1-cad9a2c5181a0195b8728cc34713ecae","_revisions":{"start":1,"ids":["cad9a2c5181a0195b8728cc34713ecae"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781942953,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()[2]","endOffset":58},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"During foreground cascading deletion, the only dependents that block owner deletion are those that have the ownerReference.blockOwnerDeletion=true field and are in the garbage collection controller cache.","v":"26","_id":"d28d9996cf2dd0a0f586fdf8e90007da","_rev":"1-d91bb4a6b8e6b7c8aa1703a6c5e8117c","_revisions":{"start":1,"ids":["d91bb4a6b8e6b7c8aa1703a6c5e8117c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cloud-controller/","date":1736779177139,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[2]","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The controllers inside the cloud controller manager include:  Node controller The node controller is responsible for updating Node objects when new servers are created in your cloud infrastructure.","v":"26","_id":"d4b89ccd5a3de74e548d3cb7f8b2f005","_rev":"1-8edd9817cce0c4f7ec8d04f88e987c4c","_revisions":{"start":1,"ids":["8edd9817cce0c4f7ec8d04f88e987c4c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736771891683,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":45},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes has a \"hub-and-spoke\" API pattern.","v":"26","first":true,"title":"Communication between Nodes and the Control Plane | Kubernetes","_id":"d99eac0530bc74557f3f52c0798d229a","_rev":"1-f004a68921510ea755e055455513e0dd","_revisions":{"start":1,"ids":["f004a68921510ea755e055455513e0dd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736772714923,"range":{"startContainerPath":"//h3[@id=\"api-server-to-nodes-pods-and-services\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[3]","endOffset":41},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"API server to nodes, pods, and services The connections from the API server to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So while the connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run over untrusted or public networks","v":"26","_id":"dae9209288eefbeb56dcb673bce0931b","_rev":"1-bd235f44e8e055f21e0d5d9f90d640f0","_revisions":{"start":1,"ids":["bd235f44e8e055f21e0d5d9f90d640f0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736771897794,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","startOffset":79,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":161},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"None of the other control plane components are designed to expose remote services.","v":"26","_id":"dc9a49d319101d49b239c757fe33ca80","_rev":"1-6e08346f32ec15d62f3ce949b8fdcd97","_revisions":{"start":1,"ids":["6e08346f32ec15d62f3ce949b8fdcd97"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782131116,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()[2]","endOffset":41},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"As a beta feature, you can specify the maximum time a local image can be unused for, regardless of disk usage. This is a kubelet setting that you configure for each node.  To configure the setting, you need to set a value for the imageMaximumGCAge field in the kubelet configuration file.","v":"26","_id":"ddfb2a7834c674cdec60526ae0db8588","_rev":"1-9d5823cac02b8cf28331c7a227e08518","_revisions":{"start":1,"ids":["9d5823cac02b8cf28331c7a227e08518"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736774839770,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Controllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates that Job object to mark it Finished.","v":"26","_id":"df311bab2b34bbb722a7ab86f3954be2","_rev":"1-be16eb1a9e538900dd24f8ddbbdb2665","_revisions":{"start":1,"ids":["be16eb1a9e538900dd24f8ddbbdb2665"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781907437,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/text()","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In foreground cascading deletion, the owner object you're deleting first enters a deletion in progress state. In this state, the following happens to the owner object:  The Kubernetes API server sets the object's metadata.deletionTimestamp field to the time the object was marked for deletion. The Kubernetes API server also sets the metadata.finalizers field to foregroundDeletion. The object remains visible through the Kubernetes API until the deletion process is complete.","v":"26","_id":"e0a491b5b40f376ed4822d8b93cc4ca1","_rev":"1-78fb62520b5eee4c1de66a721a6a5c28","_revisions":{"start":1,"ids":["78fb62520b5eee4c1de66a721a6a5c28"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cgroups/","date":1736779748698,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":53},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"cgroup v2 offers several improvements over cgroup v1, such as the following:  Single unified hierarchy design in API Safer sub-tree delegation to containers Newer features like Pressure Stall Information Enhanced resource allocation management and isolation","v":"26","_id":"e55c16d12701b462745901be1f813d07","_rev":"1-c566da6b50368979efac147b06337555","_revisions":{"start":1,"ids":["c566da6b50368979efac147b06337555"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736772546782,"range":{"startContainerPath":"//h2[@id=\"control-plane-to-node\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":15},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Control plane to node There are two primary communication paths from the control plane (the API server) to the nodes. The first is from the API server to the kubelet process which runs on each node in the cluster. The second is from the API server to any node, pod, or service through the API server's proxy functionality.","v":"26","_id":"e8a0ffc5fb9a984243583b9ff8179782","_rev":"1-3646d84979ec6b4304687f281eb39059","_revisions":{"start":1,"ids":["3646d84979ec6b4304687f281eb39059"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/leases/","date":1736776050865,"range":{"startContainerPath":"//h2[@id=\"node-heart-beats\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[8]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Node heartbeats Kubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server. For every Node , there is a Lease object with a matching name in the kube-node-lease namespace. Under the hood, every kubelet heartbeat is an update request to this Lease object, updating the spec.renewTime field for the Lease. The Kubernetes control plane uses the time stamp of this field to determine the availability of this Node.","v":"26","_id":"e9492aca30b95e3f13aa7aae1dacf7cc","_rev":"1-ea38efc6fd65105503ca0506cf43277f","_revisions":{"start":1,"ids":["ea38efc6fd65105503ca0506cf43277f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736771927508,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":107},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the API server along with valid client credentials.","v":"26","_id":"eb60c08faaf2fdb79d1240f251f333fe","_rev":"1-01f0733cf09b8ed8ac0b2c7e3432aff5","_revisions":{"start":1,"ids":["01f0733cf09b8ed8ac0b2c7e3432aff5"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781911600,"correspondingDocumentId":"e0a491b5b40f376ed4822d8b93cc4ca1","_id":"f0a151cf-3220-4fea-a804-2c7f2d3f1f6d","_rev":"1-ac61785ee263bedcf4f4f3c5454b0498","_revisions":{"start":1,"ids":["ac61785ee263bedcf4f4f3c5454b0498"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/nodes/","date":1736771061598,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When the memory swap feature is turned on, Kubernetes data such as the content of Secret objects that were written to tmpfs now could be swapped to disk.","v":"26","_id":"f1784d24f85af6ce30a2d03d6fbcec9b","_rev":"1-3d3eebb5de2d64bf2e4ded43f57c401c","_revisions":{"start":1,"ids":["3d3eebb5de2d64bf2e4ded43f57c401c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/","date":1736772009802,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[4]","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods that wish to connect to the API server can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in default namespace) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the API server.","v":"26","_id":"f1bc5e16747394f4232973559629c61d","_rev":"1-9e1d28a0f35cf558bece75b6041df766","_revisions":{"start":1,"ids":["9e1d28a0f35cf558bece75b6041df766"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736775201725,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager.","v":"26","_id":"f31e1486ec08a01c699dade2d7c05f85","_rev":"1-2dc4b310abd27fa71dcee33c3bf82c4d","_revisions":{"start":1,"ids":["2dc4b310abd27fa71dcee33c3bf82c4d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/cri/","date":1736779852227,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The CRI is a plugin interface which enables the kubelet to use a wide variety of container runtimes, without having a need to recompile the cluster components.  You need a working container runtime on each Node in your cluster, so that the kubelet can launch Pods and their containers.  The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime.  The Kubernetes Container Runtime Interface (CRI) defines the main gRPC protocol for the communication between the node components kubelet and container runtime.","v":"26","first":true,"title":"Container Runtime Interface (CRI) | Kubernetes","_id":"f4cd03b28ab264ae202bcdb2050a8b06","_rev":"1-3a8bf5fef92b08d509b5ba2174e887e7","_revisions":{"start":1,"ids":["3a8bf5fef92b08d509b5ba2174e887e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773444342,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","endOffset":58},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A controller tracks at least one Kubernetes resource type.","v":"26","_id":"f51859324930f2e2e4bb1c2cbcf4e0f0","_rev":"1-40639cdac09f32e326c77acb2d251bf2","_revisions":{"start":1,"ids":["40639cdac09f32e326c77acb2d251bf2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736773508057,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":62},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The Job controller is an example of a Kubernetes built-in controller.","v":"26","_id":"f64a92d4954d933d781d76b802957da0","_rev":"1-f578558b207ff626bc65d3fa0c05fa4e","_revisions":{"start":1,"ids":["f578558b207ff626bc65d3fa0c05fa4e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736781952116,"range":{"startContainerPath":"//h3[@id=\"background-deletion\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","endOffset":205},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Background cascading deletion In background cascading deletion, the Kubernetes API server deletes the owner object immediately and the garbage collector controller (custom or default) cleans up the dependent objects in the background.","v":"26","_id":"f709569f0dad8843f86f4ad4e10fbe4b","_rev":"1-78265ac9d423bd1b253489e42ff3d6cc","_revisions":{"start":1,"ids":["78265ac9d423bd1b253489e42ff3d6cc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/controller/","date":1736774980665,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()[3]","endOffset":53},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"(There actually is a controller (https://github.com/kubernetes/autoscaler/) that horizontally scales the nodes in your cluster.)","v":"26","_id":"fad34414cee0d469438a5c5a956890ae","_rev":"1-02a999419a81915f66484d5b11a08a34","_revisions":{"start":1,"ids":["02a999419a81915f66484d5b11a08a34"]}}]}

{"seq":361}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862352657,"correspondingDocumentId":"c8e3486149bf38c1fae65a4986d56f0b","_id":"9a0421ee-9de1-4877-acd8-672f1ba02bf7","_rev":"1-3ee729400e6e97e487ffc8e7f87db55e","_revisions":{"start":1,"ids":["3ee729400e6e97e487ffc8e7f87db55e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/containers/","date":1736782833399,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[5]","endOffset":73},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"https://kubernetes.io/docs/concepts/containers/https://kubernetes.io/docs/concepts/containers/images/https://kubernetes.io/docs/concepts/containers/container-environment/https://kubernetes.io/docs/concepts/containers/runtime-class/https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/","v":"26","first":true,"title":"Containers | Kubernetes","_id":"a025b228598352b29059ceb17789ddae","_rev":"1-bd7ad1fbff85fca845ccf1ac8b7559e7","_revisions":{"start":1,"ids":["bd7ad1fbff85fca845ccf1ac8b7559e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851056008,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[2]","startOffset":64,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.","v":"26","_id":"a082d6f57bbf07de31f40753faba2b3b","_rev":"1-3d10409c4381f49865977e88fc50c505","_revisions":{"start":1,"ids":["3d10409c4381f49865977e88fc50c505"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850578627,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[3]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[3]","endOffset":138},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod models an application-specific \"logical host\": it contains one or more application containers which are relatively tightly coupled.","v":"26","_id":"a16fff49eba1e322d2987c35d3294bfb","_rev":"1-5795af59995438ae8a9ba91d586fe862","_revisions":{"start":1,"ids":["5795af59995438ae8a9ba91d586fe862"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782198978,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//h4[@id=\"image-maximum-age-gc\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Disk usage above the configured HighThresholdPercent value triggers garbage collection, which deletes images in order based on the last time they were used, starting with the oldest first. The kubelet deletes images until disk usage reaches the LowThresholdPercent value.","v":"26","_id":"a2e8f1323842a06061be08fc77215f0e","_rev":"1-fd16cad81a3ce8f4b192281ff31a6bea","_revisions":{"start":1,"ids":["fd16cad81a3ce8f4b192281ff31a6bea"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850735738,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container.","v":"26","_id":"a3bd1ea8e08f734ece2cfbe1d91cb355","_rev":"1-b68b0625a01fffe18b7738525bc3c8a4","_revisions":{"start":1,"ids":["b68b0625a01fffe18b7738525bc3c8a4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850893300,"range":{"startContainerPath":"//h2[@id=\"using-pods\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Using Pods The following is an example of a Pod which consists of a container running the image nginx:1.14.2.","v":"26","_id":"a3c633903a53c38afe9cc1bf25f8a642","_rev":"1-0d92046c3af3431859d246b16a8af08a","_revisions":{"start":1,"ids":["0d92046c3af3431859d246b16a8af08a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849857481,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/a[2]/text()","endOffset":10},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment and ReplicaSet","v":"26","_id":"a4656bb709c30e27627a36f1766459ce","_rev":"1-f8796e16458d1de3ffa562176a3fbc39","_revisions":{"start":1,"ids":["f8796e16458d1de3ffa562176a3fbc39"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850703728,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You need to install a container runtime into each node in the cluster so that Pods can run there.","v":"26","_id":"a470123953a1a641744d06896cfc6e49","_rev":"1-5ae6ee15b0d229e497dc991a7455adb7","_revisions":{"start":1,"ids":["5ae6ee15b0d229e497dc991a7455adb7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851255151,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods natively provide two kinds of shared resources for their constituent containers: networking and storage.","v":"26","_id":"a5255c745f6bd71dd0c9f83b909c762f","_rev":"1-218efbc45a670baf1b2837c0578b86eb","_revisions":{"start":1,"ids":["218efbc45a670baf1b2837c0578b86eb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/garbage-collection/","date":1736782206033,"range":{"startContainerPath":"//h3[@id=\"container-image-garbage-collection\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[5]/li[3]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container garbage collection The kubelet garbage collects unused containers based on the following variables, which you can define:  MinAge: the minimum age at which the kubelet can garbage collect a container. Disable by setting to 0. MaxPerPodContainer: the maximum number of dead containers each Pod can have. Disable by setting to less than 0. MaxContainers: the maximum number of dead containers the cluster can have. Disable by setting to less than 0.","v":"26","_id":"a7c17d394705a8e28ab596bacd6a922b","_rev":"1-cca16fb6d6bfe2a19c202b3945bbb3e3","_revisions":{"start":1,"ids":["cca16fb6d6bfe2a19c202b3945bbb3e3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736842519369,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[3]","endOffset":17},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of pods. In Kubernetes, a Pod represents a set of running containers on your cluster.","v":"26","first":true,"title":"Workloads | Kubernetes","_id":"aa72007eefb8c468a25d8dc1ca7820f5","_rev":"1-1a9f1d34caddefcce68f33d3283bcf0b","_revisions":{"start":1,"ids":["1a9f1d34caddefcce68f33d3283bcf0b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849885271,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"StatefulSet lets you run one or more related Pods that do track state somehow.","v":"26","_id":"aae637765f3cada95a6b3d3f4120c691","_rev":"1-181dc70023d6e32cfd9e67d476f5af83","_revisions":{"start":1,"ids":["181dc70023d6e32cfd9e67d476f5af83"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862947760,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p/text()[6]","endOffset":31},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pod updates may not change fields other than spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.tolerations. For spec.tolerations, you can only add new entries.","v":"26","_id":"ad2266cc068cd63643191c4cf904940f","_rev":"1-532831a664b9fe178149fbb37a8be2a0","_revisions":{"start":1,"ids":["532831a664b9fe178149fbb37a8be2a0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850781634,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/p/text()[2]","endOffset":111},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods in a Kubernetes cluster are used in two main ways:  Pods that run a single container. The \"one-container-per-Pod\" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.  Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit.","v":"26","_id":"ad5628057ddec5e9da9d12fbc6ecc314","_rev":"1-bcab601a304d5a6a5794ee4d2091af43","_revisions":{"start":1,"ids":["bcab601a304d5a6a5794ee4d2091af43"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849869265,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/text()[2]","startOffset":3,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/text()[2]","endOffset":89},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment is a good fit for managing a stateless application workload on your cluster","v":"26","_id":"adb65e31dc7c03aac752136afff666dd","_rev":"1-1e8ea25369a9b120d7bf4a5535ec3814","_revisions":{"start":1,"ids":["1e8ea25369a9b120d7bf4a5535ec3814"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/","date":1736782866453,"range":{"startContainerPath":"//h2[@id=\"container-hooks\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":63},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container hooks There are two hooks that are exposed to Containers:  PostStart  This hook is executed immediately after a container is created.","v":"26","first":true,"title":"Container Lifecycle Hooks | Kubernetes","_id":"ae7173c03164a1aba051c57116dc3ad8","_rev":"1-0ab9b789dd09cca72b1fea6907d3fe04","_revisions":{"start":1,"ids":["0ab9b789dd09cca72b1fea6907d3fe04"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862479025,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()[3]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If the metadata.deletionTimestamp is set, no new entry can be added to the metadata.finalizers list.","v":"26","_id":"b34cdacd1a8aa846ceaa28060319e7d2","_rev":"1-b4f60111b141393af2395f80649f64be","_revisions":{"start":1,"ids":["b4f60111b141393af2395f80649f64be"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862360796,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":38,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","endOffset":208},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"when the Pod template for a workload resource is changed, the controller creates new Pods based on the updated template instead of updating or patching the existing Pods.","v":"26","_id":"b365160cfe493f9561c49b69239ddb87","_rev":"1-c79ece180652fdc6dba6b7ab072a4d20","_revisions":{"start":1,"ids":["c79ece180652fdc6dba6b7ab072a4d20"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736861813953,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()[2]","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each workload resource implements its own rules for handling changes to the Pod template. If you want to read more about StatefulSet specifically, read Update strategy in the StatefulSet Basics tutorial.","v":"26","_id":"b6a6f9ab6c0e77fc776da5f5579f2fc1","_rev":"1-9f8d391a00e5de6697289ab435805330","_revisions":{"start":1,"ids":["9f8d391a00e5de6697289ab435805330"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736868033601,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[2]","endOffset":13},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Containers within the Pod see the system hostname as being the same as the configured name for the Pod.","v":"26","_id":"b748ea242d0690bf52dbda672e61c87e","_rev":"1-66bfc43e335571e095f44cf56ccb506b","_revisions":{"start":1,"ids":["66bfc43e335571e095f44cf56ccb506b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736860191920,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/text()[2]","endOffset":48},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"PodTemplates are specifications for creating Pods, and are included in workload resources such as Deployments, Jobs, and DaemonSets.  Each controller for a workload resource uses the PodTemplate inside the workload object to make actual Pods.","v":"26","_id":"bca8ec2e36a7b083eb3a9a9f34b145f9","_rev":"1-d4ba366f578d37d8c91ae80df1395f67","_revisions":{"start":1,"ids":["d4ba366f578d37d8c91ae80df1395f67"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850742058,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.","v":"26","_id":"bd0fbf37a5793117eb61d2e82121c063","_rev":"1-d4a8d788704dcc0568d6e9a7222bd7b7","_revisions":{"start":1,"ids":["d4a8d788704dcc0568d6e9a7222bd7b7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850905972,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/pre/code/span/span/text()","endOffset":61},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To create the Pod shown above, run the following command:  kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml","v":"26","_id":"be75730d9fb0bcdc4c0664c7a6c393ae","_rev":"1-47fe69bac2cb1984128813372248327f","_revisions":{"start":1,"ids":["47fe69bac2cb1984128813372248327f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850393843,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.","v":"26","first":true,"title":"Pods | Kubernetes","_id":"c02700d4abc95a7470e035fbb3344d57","_rev":"1-6e9fb044efb10167be383a40f73d5ddd","_revisions":{"start":1,"ids":["6e9fb044efb10167be383a40f73d5ddd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849904517,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","endOffset":62},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DaemonSet defines Pods that provide facilities that are local to nodes.","v":"26","_id":"c1291da4419076da189254f5a9a2b01a","_rev":"1-dde5de4eff0e7d049d2db328203f9c46","_revisions":{"start":1,"ids":["dde5de4eff0e7d049d2db328203f9c46"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736868012554,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[6]","startOffset":155,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[6]","endOffset":396},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Containers in different Pods have distinct IP addresses and can not communicate by OS-level IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.","v":"26","_id":"c4007386189779e50b61999b97b34354","_rev":"1-524d6b0ef05657fb5803be79eb4f173d","_revisions":{"start":1,"ids":["524d6b0ef05657fb5803be79eb4f173d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850898951,"range":{"startContainerPath":"//div[@id=\"pods-simple-pod-yaml\"]/div/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"pods-simple-pod-yaml\"]/div/pre/code/span[10]/span/span[4]/text()","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion: v1 kind: Pod metadata:   name: nginx spec:   containers:   - name: nginx     image: nginx:1.14.2     ports:     - containerPort: 80","v":"26","_id":"c6306f33ff8035ebed97323687d68bed","_rev":"1-aa0dcd01a66b7c85d74ec0a34df89e28","_revisions":{"start":1,"ids":["aa0dcd01a66b7c85d74ec0a34df89e28"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736861761160,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","endOffset":248},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Modifying the pod template or switching to a new pod template has no direct effect on the Pods that already exist. If you change the pod template for a workload resource, that resource needs to create replacement Pods that use the updated template.","v":"26","_id":"c8e3486149bf38c1fae65a4986d56f0b","_rev":"1-19bcbb5787515e4f7a832faa1c663070","_revisions":{"start":1,"ids":["19bcbb5787515e4f7a832faa1c663070"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736843038231,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":96,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","endOffset":56},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"you can use workload resources that manage a set of pods on your behalf. These resources configure controllers that make sure the right number of the right kind of pod are running, to match the state you specified.  Kubernetes provides several built-in workload resources:","v":"26","_id":"c8f0c9dec95e455f306a3e0a64c5efba","_rev":"1-eff243341d07b12813df3e046a2681a0","_revisions":{"start":1,"ids":["eff243341d07b12813df3e046a2681a0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851034311,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","endOffset":117},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally","v":"26","_id":"ca7f842962fd1a8a62c5e5dc8a3f60d6","_rev":"1-6468fff4ba342d2d5780c19eaf5f3294","_revisions":{"start":1,"ids":["6468fff4ba342d2d5780c19eaf5f3294"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850555586,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[3]","endOffset":95},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.","v":"26","_id":"cb25265cb9fee55bab0c553e943a239e","_rev":"1-430e2bdde44faec516493f7b23cc37e4","_revisions":{"start":1,"ids":["430e2bdde44faec516493f7b23cc37e4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849998920,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":114,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":117},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Using a custom resource definition, you can add in a third-party workload resource if you want a specific behavior that's not part of Kubernetes' core.","v":"26","_id":"cd2187d065973db3aca4c745118e6dd4","_rev":"1-ee144685f920233b196cf0d2443fb18d","_revisions":{"start":1,"ids":["ee144685f920233b196cf0d2443fb18d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851298374,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","endOffset":160},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because Pods are designed as relatively ephemeral, disposable entities.","v":"26","_id":"d02bd3a9cd5bba01ace076c104533353","_rev":"1-3ffe9c35998aafc0707fa2c82e3f85b1","_revisions":{"start":1,"ids":["3ffe9c35998aafc0707fa2c82e3f85b1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862407352,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()[3]","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes doesn't prevent you from managing Pods directly. It is possible to update some fields of a running Pod, in place. However, Pod update operations like patch, and replace have some limitations:","v":"26","_id":"d325ea899e8367311aca3c3b38de49a1","_rev":"1-f74a125907715547ceaa08528bdc16f8","_revisions":{"start":1,"ids":["f74a125907715547ceaa08528bdc16f8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851408474,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()[2]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The name of a Pod must be a valid DNS subdomain value","v":"26","_id":"d56e7426254695bfee3f487aa6546ea1","_rev":"1-00e90fee9e0d255d60bff4d0c5da2fc1","_revisions":{"start":1,"ids":["00e90fee9e0d255d60bff4d0c5da2fc1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851383884,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[2]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[5]","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When a Pod gets created (directly by you, or indirectly by a controller), the new Pod is scheduled to run on a Node in your cluster. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is evicted for lack of resources, or the node fails.","v":"26","_id":"d71160342396f98cfb5473a6c1236e4f","_rev":"1-376f788b955b9f3a0d0bfcc91510fce9","_revisions":{"start":1,"ids":["376f788b955b9f3a0d0bfcc91510fce9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736851399063,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.","v":"26","_id":"d916a1a42f5dcc6b277c30ef1a15c953","_rev":"1-f1d19f18cdd5ede0de0e19c918afb673","_revisions":{"start":1,"ids":["f1d19f18cdd5ede0de0e19c918afb673"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736867883267,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[2]","endOffset":99},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data.","v":"26","_id":"daf9dd9d25518b422420ddc055ed7a40","_rev":"1-883cc88574e34234ee3290b59880dac3","_revisions":{"start":1,"ids":["883cc88574e34234ee3290b59880dac3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/","date":1736782882990,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","endOffset":64},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"PreStop  This hook is called immediately before a container is terminated","v":"26","_id":"dc12ceedf61b0c27a6acd615ac4ce199","_rev":"1-8bef79ea059871c531ec8cd9a9b302cf","_revisions":{"start":1,"ids":["8bef79ea059871c531ec8cd9a9b302cf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736867856863,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[31]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"pod-storage\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods enable data sharing and communication among their constituent containers.","v":"26","_id":"dd560ce49b212c30d33ece433ff92618","_rev":"1-8b077ec978f69aee7a1cbcf6702958dc","_revisions":{"start":1,"ids":["8b077ec978f69aee7a1cbcf6702958dc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736867927470,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()","endOffset":164},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports.","v":"26","_id":"de6f770fb18277dce8696c9fd2b7831c","_rev":"1-c03a558cb5ed71cc45eb977bc59acd50","_revisions":{"start":1,"ids":["c03a558cb5ed71cc45eb977bc59acd50"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736862468698,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li/p/text()[6]","endOffset":83},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Most of the metadata about a Pod is immutable. For example, you cannot change the namespace, name, uid, or creationTimestamp fields; the generation field is unique. It only accepts updates that increment the field's current value.","v":"26","_id":"e4bdaf8c88bdbf055c4dc2ef6f2a5d61","_rev":"1-6908a6153cfa44956d11fea734502d6d","_revisions":{"start":1,"ids":["6908a6153cfa44956d11fea734502d6d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849960767,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","startOffset":162,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","endOffset":261},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each pod in a DaemonSet performs a job similar to a system daemon on a classic Unix / POSIX server.","v":"26","_id":"e542ea66591a00cd3c15ea04ac742f51","_rev":"1-e58d77fb7c202b98cb596ca692f5deb4","_revisions":{"start":1,"ids":["e58d77fb7c202b98cb596ca692f5deb4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/architecture/mixed-version-proxy/","date":1736782463297,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[3]","endOffset":195},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes 1.32 includes an alpha feature that lets an API Server proxy a resource requests to other peer API servers. This is useful when there are multiple API servers running different versions of Kubernetes in one cluster (for example, during a long-lived rollout to a new release of Kubernetes).","v":"26","first":true,"title":"Mixed Version Proxy | Kubernetes","_id":"e8da001b8abda14f0a57b3c1ca98c7e6","_rev":"1-47ace5e8b9da4c1112afa69ed85694fc","_revisions":{"start":1,"ids":["47ace5e8b9da4c1112afa69ed85694fc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850867594,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/p[3]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see Workload management (https://kubernetes.io/docs/concepts/workloads/controllers/) .","v":"26","_id":"ef36c2b5c557e852ea8d8570d05e3167","_rev":"1-efcafbf55c2377e6ba7b879b4d9c8999","_revisions":{"start":1,"ids":["efcafbf55c2377e6ba7b879b4d9c8999"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736860218263,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/pre/code/span[14]/span/span[2]/text()","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion: batch/v1 kind: Job metadata:   name: hello spec:   template:     # This is the pod template     spec:       containers:       - name: hello         image: busybox:1.28         command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']       restartPolicy: OnFailure     # The pod template ends here","v":"26","_id":"f01fb89c539ef97e5cfc56c01dcb7526","_rev":"1-f6990af6b8eaa5b7e735a22ce3fc77ac","_revisions":{"start":1,"ids":["f6990af6b8eaa5b7e735a22ce3fc77ac"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850564440,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":86},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod's contents are always co-located and co-scheduled, and run in a shared context.","v":"26","_id":"f50a8e4c8d65066a3ab7656f38503e8e","_rev":"1-b785d1bc26f13d3080b7545f596c8f31","_revisions":{"start":1,"ids":["b785d1bc26f13d3080b7545f596c8f31"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736850918324,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","endOffset":81},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods are generally not created directly and are created using workload resources.","v":"26","_id":"f5e04b15b37e42b176371c79605d0b82","_rev":"1-6c503da21850be3770a92c1a3d73e32e","_revisions":{"start":1,"ids":["6c503da21850be3770a92c1a3d73e32e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/","date":1736849970363,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()[2]","endOffset":77},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Job and CronJob provide different ways to define tasks that run to completion and then stop.","v":"26","_id":"ff877b4f912921c932d14451ef7abd8a","_rev":"1-32aed48cf024e98986c27f356f027b82","_revisions":{"start":1,"ids":["32aed48cf024e98986c27f356f027b82"]}}]}

{"seq":411}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934251107,"correspondingDocumentId":"e5ee4e1af00ae13fe938b70979190a44","_id":"0406f55d-06dc-4496-8e5e-647b350b7621","_rev":"1-15f8ee0c0f928050b85b8a1a34d06994","_revisions":{"start":1,"ids":["15f8ee0c0f928050b85b8a1a34d06994"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736932329167,"correspondingDocumentId":"c68c63cf87800c8bc37aad642bf7a97c","_id":"26df4910-a9ef-4398-9bac-638d03f4d908","_rev":"1-d63195e5a5d23f967867297d9ccade99","_revisions":{"start":1,"ids":["d63195e5a5d23f967867297d9ccade99"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936965072,"correspondingDocumentId":"e4cf44c695199461f70d46eb497c59ea","_id":"5377ff9d-d569-4e1c-91ab-1aef16fe64fc","_rev":"1-d4bb5c834defa79055fdfce96d3c6193","_revisions":{"start":1,"ids":["d4bb5c834defa79055fdfce96d3c6193"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736935024581,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/p[2]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.  Make sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.","v":"26","_id":"a0f0886ddeae0c0f6aac2ae43aa6593e","_rev":"1-d7afb665811ca5447f77294dc795f27d","_revisions":{"start":1,"ids":["d7afb665811ca5447f77294dc795f27d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937449177,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[2]/li[5]/text()","endOffset":186},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To investigate the root cause of a CrashLoopBackOff issue, a user can:  Check logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes. Inspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues. Review configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available. Check resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue. Debug application: There might exist bugs or misconfigurations in the application code. Running this container image locally or in a development environment can help diagnose application specific issues.","v":"26","_id":"a103e97b31dccba7fa4f08c30389d68e","_rev":"1-5e75b27c680dd141115f860376eae4b2","_revisions":{"start":1,"ids":["5e75b27c680dd141115f860376eae4b2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936715408,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/p[3]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force.","v":"26","_id":"a4a44a5ea6200488375a4f612909b89f","_rev":"1-94dfc2c285e612828c5fe9e31b95ce71","_revisions":{"start":1,"ids":["94dfc2c285e612828c5fe9e31b95ce71"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937396368,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","endOffset":104},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay","v":"26","_id":"a650ab6ac7b3282a87a34a64eb3290da","_rev":"1-f227733ece6e8a9c1d59a1eadc078728","_revisions":{"start":1,"ids":["f227733ece6e8a9c1d59a1eadc078728"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736932132692,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[40]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods in a Kubernetes cluster are used in two main ways:  Pods that run a single container.","v":"26","_id":"a6c1cbcd7169174ec258420b96e0a1b0","_rev":"1-cf87cc50c072d16b98e884e28344e9ee","_revisions":{"start":1,"ids":["cf87cc50c072d16b98e884e28344e9ee"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736933295640,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","endOffset":102},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If one of the containers in the Pod fails, then Kubernetes may try to restart that specific container.","v":"26","_id":"a8aa7a3cf163924b732821e486bc0c27","_rev":"1-488846160d080f49649e36356f5b2d4b","_revisions":{"start":1,"ids":["488846160d080f49649e36356f5b2d4b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937666219,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/a[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[6]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Sidecar containers ignore the Pod-level restartPolicy field","v":"26","_id":"a8db327a86ff1d02ad0655705bf76175","_rev":"1-1ffe251eb67de4f124d3527663b214bf","_revisions":{"start":1,"ids":["1ffe251eb67de4f124d3527663b214bf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934413200,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()[2]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()[2]","endOffset":154},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The phase is not intended to be a comprehensive rollup of observations of container or Pod state, nor is it intended to be a comprehensive state machine.","v":"26","_id":"abfa6ca5d4b4dc989c09161e24b71adb","_rev":"1-1d788d2d0f868fef78c7a08e63c7dcb2","_revisions":{"start":1,"ids":["1d788d2d0f868fef78c7a08e63c7dcb2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936862880,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()[2]","endOffset":40},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for setting the phase of all Pods on the lost node to Failed.","v":"26","_id":"b269735da41a65a9728742d7378a9d15","_rev":"1-372cd246b12c0b44b4aaa27535512d32","_revisions":{"start":1,"ids":["372cd246b12c0b44b4aaa27535512d32"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736869367670,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]/text()[2]","endOffset":161},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.","v":"26","_id":"b879c29711fd28adb81b68c6a661eca3","_rev":"1-cdaefc51355502b5370ab428884208ad","_revisions":{"start":1,"ids":["cdaefc51355502b5370ab428884208ad"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934469981,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[5]/td[2]/text()","endOffset":167},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Here are the possible values for phase:  Value Description Pending The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network. Running The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting. Succeeded All containers in the Pod have terminated in success, and will not be restarted. Failed All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting. Unknown For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.","v":"26","_id":"bbf12aaec755c13c226eaf7576af8323","_rev":"1-7f2fb69afd553b2c3147d4b4a7872805","_revisions":{"start":1,"ids":["7f2fb69afd553b2c3147d4b4a7872805"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934395431,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[4]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod's status field is a PodStatus object, which has a phase field.","v":"26","_id":"bef90c9cd5c7b1b6a6017096ce49b0d2","_rev":"1-c891632f5630e5ca5ce87ce2b8059f49","_revisions":{"start":1,"ids":["c891632f5630e5ca5ce87ce2b8059f49"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736869358514,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/em/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()[3]","endOffset":98},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Whereas most Pods are managed by the control plane (for example, a Deployment), for static Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).","v":"26","_id":"bf92a7ea8941c6209e2f252ee7b99c9b","_rev":"1-ff0b3cbc5badb19d7a5c475ef07fadfe","_revisions":{"start":1,"ids":["ff0b3cbc5badb19d7a5c475ef07fadfe"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937692790,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[3]/text()","endOffset":58},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Always: Automatically restarts the container after any termination. OnFailure: Only restarts the container if it exits with an error (non-zero exit status). Never: Does not automatically restart the terminated container.","v":"26","_id":"c35dd97388a9c8256f5440ee78d02508","_rev":"1-5070924a1591d0b8a8c2bc41c1b27caf","_revisions":{"start":1,"ids":["5070924a1591d0b8a8c2bc41c1b27caf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937511334,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[3]","endOffset":86},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The spec of a Pod has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always.","v":"26","_id":"c3a625e73b7ab07fe1456a20f79816d1","_rev":"1-d9fd057e0bb2d25bf9758cdb8e6da600","_revisions":{"start":1,"ids":["d9fd057e0bb2d25bf9758cdb8e6da600"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736932311841,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[45]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[3]/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:  ExecAction (performed with the help of the container runtime) TCPSocketAction (checked directly by the kubelet) HTTPGetAction (checked directly by the kubelet)","v":"26","_id":"c68c63cf87800c8bc37aad642bf7a97c","_rev":"1-2868c5bc92230b4426254bf5d8aa7a80","_revisions":{"start":1,"ids":["2868c5bc92230b4426254bf5d8aa7a80"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937265494,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li/strong/text()","endOffset":13},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Initial crash","v":"26","_id":"c74ea1b85197fa84a5fb1dca73f129e6","_rev":"1-f8cd7c18af6350d0daf7ca28d15562d4","_revisions":{"start":1,"ids":["f8cd7c18af6350d0daf7ca28d15562d4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736932137645,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[2]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[2]/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods that run multiple containers that need to work together.","v":"26","_id":"c8ad3503731ddf7638819c0ea88dc4b3","_rev":"1-d285ce53eb59fa376548128a7d9b45c0","_revisions":{"start":1,"ids":["d285ce53eb59fa376548128a7d9b45c0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736868139549,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[4]/text()","endOffset":76},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To set security constraints on Pods and containers, you use the securityContext field in the Pod specification. This field gives you granular control over what a Pod or individual containers can do. For example:  Drop specific Linux capabilities to avoid the impact of a CVE. Force all processes in the Pod to run as a non-root user or as a specific user or group ID. Set a specific seccomp profile. Set Windows security options, such as whether containers run as HostProcess.","v":"26","_id":"c97ed3c7c382b37cbfb82d799dad10aa","_rev":"1-0347adc43ea6225884e68b1619ca69be","_revisions":{"start":1,"ids":["0347adc43ea6225884e68b1619ca69be"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934255678,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[2]","endOffset":93},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When something is said to have the same lifetime as a Pod, such as a volume, that means that the thing exists as long as that specific Pod (with that exact UID) exists.","v":"26","_id":"ca5cd8605d612b0933bdf64759296a7a","_rev":"1-011935105ae8198b70a02d9e1a3a0f6d","_revisions":{"start":1,"ids":["011935105ae8198b70a02d9e1a3a0f6d"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936906666,"correspondingDocumentId":"b269735da41a65a9728742d7378a9d15","_id":"cf5b7b7b-66e4-417f-bfc4-6f3f99b1220a","_rev":"1-344f72f229f398d893539c43b83d2421","_revisions":{"start":1,"ids":["344f72f229f398d893539c43b83d2421"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936982976,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Once the scheduler assigns a Pod to a Node, the kubelet starts creating containers for that Pod using a container runtime. There are three possible container states: Waiting, Running, and Terminated.","v":"26","_id":"d007e9bfc6703b68905440a4c23d5774","_rev":"1-de6f4f05e8e62426b82156553ac7180f","_revisions":{"start":1,"ids":["de6f4f05e8e62426b82156553ac7180f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936802607,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()[2]","endOffset":40},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for setting the phase of all Pods on the lost node to Failed.","v":"26","_id":"d0229b81e973782f99928dce78d8934b","_rev":"1-74c3266b8dc07688fdf10bc6a457cb20","_revisions":{"start":1,"ids":["74c3266b8dc07688fdf10bc6a457cb20"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937406648,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()[3]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()[3]","endOffset":112},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.","v":"26","_id":"d26178fe91033c7e89f965af10f26622","_rev":"1-3b4ea5bdcfee7fb0b4779cbd7995183b","_revisions":{"start":1,"ids":["3b4ea5bdcfee7fb0b4779cbd7995183b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934402129,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":88},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle.","v":"26","_id":"d87e066d406feab6708c9ac5e2ff28e0","_rev":"1-9f84f0019f7f833c5291a4cf412ecaa9","_revisions":{"start":1,"ids":["9f84f0019f7f833c5291a4cf412ecaa9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937044013,"range":{"startContainerPath":"//h3[@id=\"container-state-waiting\"]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[24]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Waiting If a container is not in either the Running or Terminated state, it is Waiting.","v":"26","_id":"d8c5e7788ad914bfb621bbe03aed6dbb","_rev":"1-dd66787816ce178918fb416c0028ab63","_revisions":{"start":1,"ids":["dd66787816ce178918fb416c0028ab63"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736932910268,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods are only scheduled once in their lifetime; assigning a Pod to a specific node is called binding, and the process of selecting which node to use is called scheduling.","v":"26","_id":"d99e70921943b3667984d0aa2cd34ff5","_rev":"1-a9f9a0329e8ffe8e1ebba9573b06116d","_revisions":{"start":1,"ids":["a9f9a0329e8ffe8e1ebba9573b06116d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937317981,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[4]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[4]/text()","endOffset":155},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Backoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.","v":"26","_id":"da679f4294fc14cbbe310c975113416f","_rev":"1-ee744ed0834871f9b46b05adb796efe8","_revisions":{"start":1,"ids":["ee744ed0834871f9b46b05adb796efe8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736869317359,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Static Pods are always bound to one Kubelet on a specific node. The main use for static Pods is to run a self-hosted control plane: in other words, using the kubelet to supervise the individual control plane components.","v":"26","_id":"dad3bca1fa0d3a22bae437129433767a","_rev":"1-11e933d78add7f8a484764b5f4321204","_revisions":{"start":1,"ids":["11e933d78add7f8a484764b5f4321204"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937257010,"range":{"startContainerPath":"//h2[@id=\"container-restarts\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()[3]","endOffset":139},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"How Pods handle problems with containers Kubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:","v":"26","_id":"dbe56d9b525acd927329969aab8c3380","_rev":"1-5a03f778d8f8684d24e175ac9987eb98","_revisions":{"start":1,"ids":["5a03f778d8f8684d24e175ac9987eb98"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937080947,"range":{"startContainerPath":"//h3[@id=\"container-state-terminated\"]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[26]/text()[2]","endOffset":83},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Terminated A container in the Terminated state began execution and then either ran to completion or failed for some reason.","v":"26","_id":"dc80d29b94adc8b5790d99f19088f671","_rev":"1-f9fb77ae6ff7bbdfce53fdda44837e98","_revisions":{"start":1,"ids":["f9fb77ae6ff7bbdfce53fdda44837e98"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/","date":1736869401947,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[8]/text()[2]","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The spec of a static Pod cannot refer to other API objects","v":"26","_id":"dd3cfab708e755750b3d2dea2bd77c79","_rev":"1-6d513e41ae8bed9c0d777c4ddf534ea7","_revisions":{"start":1,"ids":["6d513e41ae8bed9c0d777c4ddf534ea7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736933316270,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","endOffset":235},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods can however fail in a way that the cluster cannot recover from, and in that case Kubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the Pod and relies on other components to provide automatic healing.","v":"26","_id":"df19a0d2a2b959f0eb09b36b80807265","_rev":"1-f3224ea668c8be5438d10cb7ceac5962","_revisions":{"start":1,"ids":["f3224ea668c8be5438d10cb7ceac5962"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736932818082,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":97},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Whilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults.","v":"26","_id":"e001989375bd179fe6276c725848768b","_rev":"1-5827489def0cb75b165cc391c3e29a5f","_revisions":{"start":1,"ids":["5827489def0cb75b165cc391c3e29a5f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934196237,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[3]","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can even have same name (as in .metadata.name) that the old Pod had, but the replacement would have a different .metadata.uid from the old Pod.","v":"26","_id":"e4c8519af490d04084e01439e2ed34c7","_rev":"1-7a469ba052b6ac77acf2aaa3b19d7217","_revisions":{"start":1,"ids":["7a469ba052b6ac77acf2aaa3b19d7217"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936949830,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()[6]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Once the scheduler assigns a Pod to a Node, the kubelet starts creating containers for that Pod using a container runtime. There are three possible container states: Waiting, Running, and Terminated.","v":"26","_id":"e4cf44c695199461f70d46eb497c59ea","_rev":"1-4ea479f2d80fda412f9554771cb322a1","_revisions":{"start":1,"ids":["4ea479f2d80fda412f9554771cb322a1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736932624467,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":119,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[4]","endOffset":70},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pods are created, assigned a unique ID (UID), and scheduled to run on nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods running on (or scheduled to run on) that node are marked for deletion. The control plane marks the Pods for removal after a timeout period.","v":"26","first":true,"title":"Pod Lifecycle | Kubernetes","_id":"e54739a7f32fcda14a2e9a145741d8cf","_rev":"1-d5b9a21b41662edc9c875386c28ae94e","_revisions":{"start":1,"ids":["d5b9a21b41662edc9c875386c28ae94e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934209019,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":144},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes does not guarantee that a replacement for an existing Pod would be scheduled to the same node as the old Pod that was being replaced.","v":"26","_id":"e5ee4e1af00ae13fe938b70979190a44","_rev":"1-d52220016c83785c7216e96232d04227","_revisions":{"start":1,"ids":["d52220016c83785c7216e96232d04227"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937274797,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[2]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[2]/strong/text()","endOffset":16},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Repeated crashes","v":"26","_id":"e6a03cb6e977b4bb5e4afde9ad8348df","_rev":"1-5735a7a20ab501951ed59c12a9b6222a","_revisions":{"start":1,"ids":["5735a7a20ab501951ed59c12a9b6222a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937301850,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[4]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[4]/strong/text()","endOffset":13},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Backoff reset","v":"26","_id":"e6adfa4423b261b9309e9ca667a7ec35","_rev":"1-1473fd328e793860ffefad9ef06bf8b1","_revisions":{"start":1,"ids":["1473fd328e793860ffefad9ef06bf8b1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937069025,"range":{"startContainerPath":"//h3[@id=\"container-state-running\"]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()[2]","endOffset":63},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Running The Running status indicates that a container is executing without issues.","v":"26","_id":"e76cdd754a0c84444ea58e8da0bc6392","_rev":"1-3c435953c44a883af6a3227ab9f02cbf","_revisions":{"start":1,"ids":["3c435953c44a883af6a3227ab9f02cbf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937297601,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[3]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[3]/strong/text()","endOffset":22},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"CrashLoopBackOff state","v":"26","_id":"e910d1bfe19003fdff429cd00b5a7d6e","_rev":"1-b3d39e5d0daa46f738840df94320be21","_revisions":{"start":1,"ids":["b3d39e5d0daa46f738840df94320be21"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936843027,"correspondingDocumentId":"d0229b81e973782f99928dce78d8934b","_id":"ebebe838-0337-4bd1-baee-6af78fa35c8e","_rev":"1-9d681882561655a99b1127386963223d","_revisions":{"start":1,"ids":["9d681882561655a99b1127386963223d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937025417,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/text()","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[22]/code/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"o check the state of a Pod's containers, you can use kubectl describe pod <name-of-pod>","v":"26","_id":"f04eecc6433b30c1a724682ece910ddb","_rev":"1-9418c189e7a12ff5853880fdf2b20a40","_revisions":{"start":1,"ids":["9418c189e7a12ff5853880fdf2b20a40"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736934428482,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","startOffset":101,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()[2]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"nothing should be assumed about Pods that have a given phase value.","v":"26","_id":"f23525dc05275620cfe0bfeac7aaf8d8","_rev":"1-24cdc446b691fa4d039bbef5cadb9f2d","_revisions":{"start":1,"ids":["24cdc446b691fa4d039bbef5cadb9f2d"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736937314493,"correspondingDocumentId":"e6adfa4423b261b9309e9ca667a7ec35","_id":"f4eae347-9a91-479b-ab6c-732d03091a62","_rev":"1-a3df410da10262ccbeb781b83ca1935e","_revisions":{"start":1,"ids":["a3df410da10262ccbeb781b83ca1935e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736936969948,"range":{"startContainerPath":"//h2[@id=\"container-states\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container states","v":"26","_id":"fd5e3a9f9837e1ba77b23a89db2ce722","_rev":"1-5990fe30f9742bc7fbeb5b1373f573a4","_revisions":{"start":1,"ids":["5990fe30f9742bc7fbeb5b1373f573a4"]}}]}

{"seq":461}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024721886,"correspondingDocumentId":"b1f6a4b895e467dbfffd8d753d34c588","_id":"040b5205-4b9b-49a6-93f5-3c981c0a89a1","_rev":"1-fc94e77d625342c4268073ff4f3d7a2b","_revisions":{"start":1,"ids":["fc94e77d625342c4268073ff4f3d7a2b"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736945531247,"correspondingDocumentId":"ea301766d57f121363911011735071c0","_id":"278a1af7-027c-4991-ab11-b00f35f6dabc","_rev":"1-e36d2102d0f16b7fa184de29e997f009","_revisions":{"start":1,"ids":["e36d2102d0f16b7fa184de29e997f009"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024719550,"correspondingDocumentId":"ba7b7c236d5efbec4a5d98c58a2dae3d","_id":"2b83aaa9-4608-46ad-aa22-50090ae02428","_rev":"1-7b8c827f208ce6fe377a099cbd8d2a89","_revisions":{"start":1,"ids":["7b8c827f208ce6fe377a099cbd8d2a89"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736951931367,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/a/text()","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If your Pod includes one or more sidecar containers","v":"26","_id":"a004fb2d586c9c14a241f2f615654cd9","_rev":"1-67fa3d36f55b7b4bdafcc5ea2c33ab4b","_revisions":{"start":1,"ids":["67fa3d36f55b7b4bdafcc5ea2c33ab4b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736944111551,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[65]/text()[2]","startOffset":161,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[65]/text()[3]","endOffset":120},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Many container runtimes respect the STOPSIGNAL value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.","v":"26","_id":"a04549353e6be4d4e7a51c6575c85160","_rev":"1-8bacd9ce885c630bb3d9111c57982685","_revisions":{"start":1,"ids":["8bacd9ce885c630bb3d9111c57982685"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943363087,"range":{"startContainerPath":"//h3[@id=\"types-of-probe\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl[3]/dd[3]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Types of probe The kubelet can optionally perform and react to three kinds of probes on running containers:  livenessProbe Indicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a liveness probe, the default state is Success. readinessProbe Indicates whether the container is ready to respond to requests. If the readiness probe fails, the endpoints controller removes the Pod's IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a container does not provide a readiness probe, the default state is Success. startupProbe Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a startup probe, the default state is Success.","v":"26","_id":"a167fa100468106f6862fee82648406a","_rev":"1-3d27ce267d29ff54999fcb0ffa2fe23a","_revisions":{"start":1,"ids":["3d27ce267d29ff54999fcb0ffa2fe23a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025411909,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[5]/p/text()","endOffset":62},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Place values into a configuration file and run a template tool","v":"26","_id":"a2e74bf19ff2def0c3a5e789206cac57","_rev":"1-751eab285722fad56797ef5b07940bc2","_revisions":{"start":1,"ids":["751eab285722fad56797ef5b07940bc2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736944986601,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li/p/text()[5]","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If one of the Pod's containers has defined a preStop hook and the terminationGracePeriodSeconds in the Pod spec is not set to 0, the kubelet runs that hook inside of the container. The default terminationGracePeriodSeconds setting is 30 seconds.","v":"26","_id":"a2fa5769357fbc0830bfb8c5f1345f71","_rev":"1-38e8385b5f061d852d5d3f53cb93f6af","_revisions":{"start":1,"ids":["38e8385b5f061d852d5d3f53cb93f6af"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024640219,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[6]","endOffset":21},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Regular init containers (in other words: excluding sidecar containers) do not support the lifecycle, livenessProbe, readinessProbe, or startupProbe fields. Init containers must run to completion before the Pod can be ready; sidecar containers continue running during a Pod's lifetime, and do support some probes.","v":"26","_id":"a38ecf00123509ff904e6b2be24ff7e8","_rev":"1-a55fbfa9929b5eed83892d11ab8ac262","_revisions":{"start":1,"ids":["a55fbfa9929b5eed83892d11ab8ac262"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025717759,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]/text()[2]","startOffset":43,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]/text()[2]","endOffset":109},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The ports on an init container are not aggregated under a Service.","v":"26","_id":"a39745023ffa350827565961bd23a579","_rev":"1-8a8bbee86a9232f2de4214e2903a9fd7","_revisions":{"start":1,"ids":["8a8bbee86a9232f2de4214e2903a9fd7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736944998921,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li/p[2]/text()[2]","endOffset":129},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If the preStop hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds.","v":"26","_id":"a5228958d5dded5a7ada031364d05c08","_rev":"1-718f2c67b05b2fc20f3e608e4b3c9581","_revisions":{"start":1,"ids":["718f2c67b05b2fc20f3e608e4b3c9581"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736942782693,"range":{"startContainerPath":"//h2[@id=\"container-probes\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[50]/text()[3]","endOffset":124},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Container probes A probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.","v":"26","_id":"a86404456882a19c52ee875c904f3180","_rev":"1-d91c9fbc465e30af36718ace9eecca0f","_revisions":{"start":1,"ids":["d91c9fbc465e30af36718ace9eecca0f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943328676,"range":{"startContainerPath":"//h3[@id=\"probe-outcome\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl[2]/dd[3]/text()","endOffset":92},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Probe outcome Each probe has one of three results:  Success The container passed the diagnostic. Failure The container failed the diagnostic. Unknown The diagnostic failed (no action should be taken, and the kubelet will make further checks).","v":"26","_id":"a9f4cdd6e77e5c93ebbb68288da3b6a6","_rev":"1-4dad0489f5d0045e76f0694384a19443","_revisions":{"start":1,"ids":["4dad0489f5d0045e76f0694384a19443"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736945032444,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/ol/li[2]/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubelet triggers the container runtime to send a TERM signal to process 1 inside each container.","v":"26","_id":"ab663d50ffefecc55ef6984ee3e939fa","_rev":"1-9cde1f84591966e018b003590539f2d8","_revisions":{"start":1,"ids":["9cde1f84591966e018b003590539f2d8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736942760601,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[49]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[49]/text()[3]","endOffset":54},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For a Pod with init containers, the kubelet sets the Initialized condition to True after the init containers have successfully completed","v":"26","_id":"abeb567e106fff8d5ab5a02c7e4f456f","_rev":"1-4cbc5ce4d59c489bbb51e1d3f75b2039","_revisions":{"start":1,"ids":["4cbc5ce4d59c489bbb51e1d3f75b2039"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024403129,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":14,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"a sidecar container is a container that starts before the main application container and continues to run.","v":"26","_id":"b1f6a4b895e467dbfffd8d753d34c588","_rev":"1-67bc7d212c3902c5843f2144d0502ca4","_revisions":{"start":1,"ids":["67bc7d212c3902c5843f2144d0502ca4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736952541836,"range":{"startContainerPath":"//h3[@id=\"pod-garbage-collection\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[4]/li[3]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Garbage collection of Pods For failed Pods, the API objects remain in the cluster's API until a human or controller process explicitly removes them.  The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up terminated Pods (with a phase of Succeeded or Failed), when the number of Pods exceeds the configured threshold (determined by terminated-pod-gc-threshold in the kube-controller-manager). This avoids a resource leak as Pods are created and terminated over time.  Additionally, PodGC cleans up any Pods which satisfy any of the following conditions:  are orphan Pods - bound to a node which no longer exists, are unscheduled terminating Pods, are terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-service.","v":"26","_id":"b217029cd573c076ea7548142cb8d1f1","_rev":"1-88e99ed9ccb573d3bcccf85ef71f1397","_revisions":{"start":1,"ids":["88e99ed9ccb573d3bcccf85ef71f1397"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736951814268,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[12]/a[@id=\"pod-termination-beyond-grace-period\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[12]/a[@id=\"pod-termination-beyond-grace-period\"]/text()","endOffset":158},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.","v":"26","_id":"b359b3e45468be80e9b6a1e6dc7da302","_rev":"1-76665dc64cf96200aad630d714f1012a","_revisions":{"start":1,"ids":["76665dc64cf96200aad630d714f1012a"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024689005,"correspondingDocumentId":"a38ecf00123509ff904e6b2be24ff7e8","_id":"b3c35b4b-f70f-42d5-9556-70f87af5a3e5","_rev":"1-b98b9de372ddf3c9478df0a3966ac7e1","_revisions":{"start":1,"ids":["b98b9de372ddf3c9478df0a3966ac7e1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943656695,"range":{"startContainerPath":"//h4[@id=\"when-should-you-use-a-readiness-probe\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[57]/text()","endOffset":102},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When should you use a readiness probe? If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe.","v":"26","_id":"b487b671289ed66cfc7c50ef75b77107","_rev":"1-24ec2650bcfb856012f3423a0c203586","_revisions":{"start":1,"ids":["24ec2650bcfb856012f3423a0c203586"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938196705,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/pre/code/span[5]/span/span[4]/text()","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"# container restart delays will start at 10s, increasing # 2x each time they are restarted, to a maximum of 100s kind: KubeletConfiguration crashLoopBackOff:     maxContainerRestartPeriod: \"100s\"","v":"26","_id":"b5041094c667b3670334baf0dd168ebf","_rev":"1-c6f2398afb1ce5c2abdc62b4c647f34f","_revisions":{"start":1,"ids":["c6f2398afb1ce5c2abdc62b4c647f34f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938339048,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[40]/text()[2]","startOffset":21,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/pre/code/span[19]/span/span[2]/text()","endOffset":3},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If Kubernetes cannot find such a condition in the status.conditions field of a Pod, the status of the condition is defaulted to \"False\".  Here is an example:  kind: Pod ... spec:   readinessGates:     - conditionType: \"www.example.com/feature-1\" status:   conditions:     - type: Ready                              # a built in PodCondition       status: \"False\"       lastProbeTime: null       lastTransitionTime: 2018-01-01T00:00:00Z     - type: \"www.example.com/feature-1\"        # an extra PodCondition       status: \"False\"       lastProbeTime: null       lastTransitionTime: 2018-01-01T00:00:00Z   containerStatuses:     - containerID: docker://abcd...       ready: true ...","v":"26","_id":"b5d3d3984d78724558f03ccc424057ec","_rev":"1-57ca213cf95ef4e540e94c48436fc43e","_revisions":{"start":1,"ids":["57ca213cf95ef4e540e94c48436fc43e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024335557,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":51,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"specialized containers that run before app containers in a Pod.","v":"26","first":true,"title":"Init Containers | Kubernetes","_id":"b65af4546e49d0416665d114940266c6","_rev":"1-4226bd2cb65cfe3858e4910b71b71a49","_revisions":{"start":1,"ids":["4226bd2cb65cfe3858e4910b71b71a49"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943994516,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[62]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[62]/text()[4]","endOffset":145},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If your container usually starts in more than initialDelaySeconds + failureThreshold × periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.","v":"26","_id":"b662fcdf9c01563b7d0e6433007aa85f","_rev":"1-bdbb6af4320846d21bf384402c152046","_revisions":{"start":1,"ids":["bdbb6af4320846d21bf384402c152046"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024432217,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","startOffset":24,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":101},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"init containers: containers that run to completion during Pod initialization.","v":"26","_id":"ba7b7c236d5efbec4a5d98c58a2dae3d","_rev":"1-aa717e41d1778ce5115bb555df857ec6","_revisions":{"start":1,"ids":["aa717e41d1778ce5115bb555df857ec6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736944079910,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[65]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[65]/text()","endOffset":261},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container.","v":"26","_id":"bcd532eb5b53eb470acb21896c6e7bde","_rev":"1-bda299f81b26dcb4665ff26b4887a1a5","_revisions":{"start":1,"ids":["bda299f81b26dcb4665ff26b4887a1a5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938172685,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()[2]","endOffset":181},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"With the alpha feature gate KubeletCrashLoopBackOffMax enabled, you can reconfigure the maximum delay between container start retries from the default of 300s (5 minutes). This configuration is set per node using kubelet configuration.","v":"26","_id":"c1c803ad413a654c0dacda2fdbab4495","_rev":"1-a3cb1506d2c2e3007c86d33c46fc37a1","_revisions":{"start":1,"ids":["a3cb1506d2c2e3007c86d33c46fc37a1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024708052,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":197},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Init containers run and complete their tasks before the main application container starts. Unlike sidecar containers, init containers are not continuously running alongside the main containers.  Init containers run to completion sequentially, and the main container does not start until all the init containers have successfully completed.  init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe whereas sidecar containers support all these probes to control their lifecycle.  Init containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly with them. They can, however, use shared volumes for data exchange.","v":"26","_id":"c6dc3569e5074f087c52ff19a8005951","_rev":"1-adf2d7994222d8e11ba13aff50783108","_revisions":{"start":1,"ids":["adf2d7994222d8e11ba13aff50783108"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025509394,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[25]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[5]/pre/code/span[2]/span/span/text()","endOffset":35},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To see logs for the init containers in this Pod, run:  kubectl logs myapp-pod -c init-myservice # Inspect the first init container kubectl logs myapp-pod -c init-mydb      # Inspect the second init container","v":"26","_id":"c974a6f3d90aee44b96632a93f76f98f","_rev":"1-a40eaeb73f10606976406f357d4a68e5","_revisions":{"start":1,"ids":["a40eaeb73f10606976406f357d4a68e5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737029365471,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","startOffset":67,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","endOffset":108},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"init container code should be idempotent.","v":"26","_id":"cb5912c02f6dbdc7a6db38384ca05da5","_rev":"1-a2e390d46aba9f86e75d6ba47fa9341e","_revisions":{"start":1,"ids":["a2e390d46aba9f86e75d6ba47fa9341e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736944061365,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[63]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[63]/text()[2]","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Because Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed (rather than being abruptly stopped with a KILL signal and having no chance to clean up).","v":"26","_id":"cc582be2f888fb15c70ca2d13918d38f","_rev":"1-760aefdb70fefb37ee1d89b58c6b2bd8","_revisions":{"start":1,"ids":["760aefdb70fefb37ee1d89b58c6b2bd8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736951940420,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/text()","startOffset":50,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/text()","endOffset":392},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated. The sidecar containers will be terminated in the reverse order they are defined in the Pod spec. This ensures that sidecar containers continue serving the other containers in the Pod until they are no longer needed.","v":"26","_id":"dc61762fd48d2f44cfb7c5f43c6c3b4f","_rev":"1-3f2a87b8e7b144d21285fd547cb9d90c","_revisions":{"start":1,"ids":["3f2a87b8e7b144d21285fd547cb9d90c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938269348,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()[4]","endOffset":89},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Your application can inject extra feedback or signals into PodStatus: Pod readiness. To use this, set readinessGates in the Pod's spec to specify a list of additional conditions that the kubelet evaluates for Pod readiness.","v":"26","_id":"de6b5e76aa221b10dc4b3ef315bffa81","_rev":"1-a0f224c21d02d50e99a847e34bbdf96f","_revisions":{"start":1,"ids":["a0f224c21d02d50e99a847e34bbdf96f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024504839,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":113,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":111},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"if the Pod has a restartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.","v":"26","_id":"e0574b1ac968ac73fcf7a28402cbd8d0","_rev":"1-c0a7f197e999591fcd8b1e8ace897228","_revisions":{"start":1,"ids":["c0a7f197e999591fcd8b1e8ace897228"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025612368,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[2]","startOffset":11,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[4]","endOffset":11},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"if the Pod restartPolicy is set to Always, the init containers use restartPolicy OnFailure.","v":"26","_id":"e2eca2a3e2a300108cb6e12c6611cb49","_rev":"1-da0defe194b4affd30a1a4a5ea891e97","_revisions":{"start":1,"ids":["da0defe194b4affd30a1a4a5ea891e97"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736945535881,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[3]/p[3]/text()[2]","startOffset":71,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[3]/p[3]/text()[4]","endOffset":198},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Terminating endpoints always have their ready status as false (for backward compatibility with versions before 1.26), so load balancers will not use it for regular traffic. See: https://kubernetes.io/docs/tutorials/services/pods-and-endpoint-termination-flow/","v":"26","_id":"e4da96e058cd8238fe150513b221153f","_rev":"1-3eb1867744403c864f687fcc1b8a9533","_revisions":{"start":1,"ids":["3eb1867744403c864f687fcc1b8a9533"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737029701482,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Sidecar containers are the secondary containers that run along with the main application container within the same Pod.","v":"26","first":true,"title":"Sidecar Containers | Kubernetes","_id":"e5e6ed669b9e70f8f0d1d5a1e4f7221b","_rev":"1-d810050fb788cc1ff11ebc97b8cd5748","_revisions":{"start":1,"ids":["d810050fb788cc1ff11ebc97b8cd5748"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025346274,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()","endOffset":60},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Register this Pod with a remote server from the downward API","v":"26","_id":"e634a2ee8ae3e60051996c238b534811","_rev":"1-f92494909457824719bf415ee4f4631a","_revisions":{"start":1,"ids":["f92494909457824719bf415ee4f4631a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736947601904,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[4]/ol/li/a[@id=\"pod-termination-beyond-grace-period\"]/text()[2]","startOffset":61,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[4]/ol/li/a[@id=\"pod-termination-beyond-grace-period\"]/text()[3]","endOffset":46},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubelet also cleans up a hidden pause container if that container runtime uses one.","v":"26","_id":"e6e4afbb8b2ff7ae6863d93a164ff7db","_rev":"1-874b5e1989d2185b927dd17d396e5249","_revisions":{"start":1,"ids":["874b5e1989d2185b927dd17d396e5249"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938557944,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[44]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For a Pod that uses custom conditions, that Pod is evaluated to be ready only when both the following statements apply:  All containers in the Pod are ready. All conditions specified in readinessGates are True.","v":"26","_id":"e718e93da2f46a65d6247ea158a69b49","_rev":"1-2187438d0d1ddd6319309ca20d78370c","_revisions":{"start":1,"ids":["2187438d0d1ddd6319309ca20d78370c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943154010,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[51]/text()","startOffset":66,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[4]/text()","endOffset":242},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Each probe must define exactly one of these four mechanisms:  exec Executes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of 0. grpc Performs a remote procedure call using gRPC. The target should implement gRPC health checks. The diagnostic is considered successful if the status of the response is SERVING. httpGet Performs an HTTP GET request against the Pod's IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400. tcpSocket Performs a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open. If the remote system (the container) closes the connection immediately after it opens, this counts as healthy.","v":"26","_id":"e8f1148d263afb9493b4c027701dfc56","_rev":"1-5240add69820743421c03cf3a2406093","_revisions":{"start":1,"ids":["5240add69820743421c03cf3a2406093"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736945301400,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[3]/p[3]/text()[2]","startOffset":71,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[3]/p[3]/text()[4]","endOffset":111},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Terminating endpoints always have their ready status as false (for backward compatibility with versions before 1.26), so load balancers will not use it for regular traffic.","v":"26","_id":"ea301766d57f121363911011735071c0","_rev":"1-cefc36c8a421c13daa3baf204ed9b6cf","_revisions":{"start":1,"ids":["cefc36c8a421c13daa3baf204ed9b6cf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943194671,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/text()[4]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/text()[4]","endOffset":101},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.","v":"26","_id":"ec8b9a3be77b0a1ab65914e6b96702a8","_rev":"1-38052a61d301799bad91d549f611a8fc","_revisions":{"start":1,"ids":["38052a61d301799bad91d549f611a8fc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938117875,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]/text()","startOffset":205,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[35]/text()","endOffset":359},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"After containers in a Pod exit, the kubelet restarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at 300 seconds (5 minutes).","v":"26","_id":"efce5b9f40ee50725d1d69dda3f6827d","_rev":"1-589a4564dea7749af6097c977576453f","_revisions":{"start":1,"ids":["589a4564dea7749af6097c977576453f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736943853592,"range":{"startContainerPath":"//h4[@id=\"when-should-you-use-a-startup-probe\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[61]/text()","endOffset":99},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When should you use a startup probe? Startup probes are useful for Pods that have containers that take a long time to come into service.","v":"26","_id":"f4b39edbb8824ee8af9fdb307cd72ac0","_rev":"1-7c2146008dc62edb3cd02249d740f288","_revisions":{"start":1,"ids":["7c2146008dc62edb3cd02249d740f288"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737029748540,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":53,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":214},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, if you have a web application that requires a local webserver, the local webserver is a sidecar and the web application itself is the app container.","v":"26","_id":"f881d5c156076ad6c3c0bbc398beada8","_rev":"1-c49aa35488803c9b2c75e4703f931521","_revisions":{"start":1,"ids":["c49aa35488803c9b2c75e4703f931521"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025486288,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/pre/code/span[18]/span/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This example defines a simple Pod that has two init containers. The first waits for myservice, and the second waits for mydb. Once both init containers complete, the Pod runs the app container from its spec section.  apiVersion: v1 kind: Pod metadata:   name: myapp-pod   labels:     app.kubernetes.io/name: MyApp spec:   containers:   - name: myapp-container     image: busybox:1.28     command: ['sh', '-c', 'echo The app is running! && sleep 3600']   initContainers:   - name: init-myservice     image: busybox:1.28     command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]   - name: init-mydb     image: busybox:1.28     command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]","v":"26","_id":"fa09803da3c83eb95542bd00df569cb3","_rev":"1-2839a7ac1e0c4717d29de63abd3b5179","_revisions":{"start":1,"ids":["2839a7ac1e0c4717d29de63abd3b5179"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/","date":1736938791332,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[46]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[46]/text()[2]","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and to have any required storage volumes mounted. Once these phases are complete, the kubelet works with a container runtime (using Container Runtime Interface (CRI)) to set up a runtime sandbox and configure networking for the Pod.","v":"26","_id":"fa4ea71b27a6d30214bd38dfb75085b2","_rev":"1-59c25a6172bbbc8aa2c87313c4f762dd","_revisions":{"start":1,"ids":["59c25a6172bbbc8aa2c87313c4f762dd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737025329635,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li/p/text()[2]","endOffset":14},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Here are some ideas for how to use init containers:  Wait for a Service to be created","v":"26","_id":"fb93f0c3d61b38a9e0d918a67fa88437","_rev":"1-af1b36f69fee0ce493403898fa93b382","_revisions":{"start":1,"ids":["af1b36f69fee0ce493403898fa93b382"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/","date":1737024452763,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/text()","endOffset":74},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Init containers are exactly like regular containers, except:  Init containers always run to completion. Each init container must complete successfully before the next one starts.","v":"26","_id":"fcbfe8895a7a31449c67ead0aa6be3df","_rev":"1-082f3bb93fb6d8c704d79cba9dd9c5f7","_revisions":{"start":1,"ids":["082f3bb93fb6d8c704d79cba9dd9c5f7"]}}]}

{"seq":511}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737033448671,"correspondingDocumentId":"d40b88ba8af808871194285746a1ce6e","_id":"06214ce0-4f46-4f2e-8564-cc9a7e293d56","_rev":"1-9434f67e272aeb3bf956b41823c7f4d2","_revisions":{"start":1,"ids":["9434f67e272aeb3bf956b41823c7f4d2"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737036147929,"correspondingDocumentId":"cb93fa709f2cd8f0e968c1717fa29f55","_id":"27ab2cbb-f2d9-41f5-96b3-c6ad2942814b","_rev":"1-5484be03ac848ab3a1c968dc82d1f13a","_revisions":{"start":1,"ids":["5484be03ac848ab3a1c968dc82d1f13a"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737033485637,"correspondingDocumentId":"ada7219af432904b0b8674bf90d30e9d","_id":"455bd3ba-56fb-4e3b-ae03-751c12e835cf","_rev":"1-e65ad25d233ccd1300ab6d3660f59105","_revisions":{"start":1,"ids":["e65ad25d233ccd1300ab6d3660f59105"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737032062705,"correspondingDocumentId":"d78ea32989f188c0bb862828def6b4fe","_id":"61b825aa-3698-4c1b-8ae0-d7969527d633","_rev":"1-eb601047a4b151422d18d4d5b02b2575","_revisions":{"start":1,"ids":["eb601047a4b151422d18d4d5b02b2575"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/scheduling-eviction/","date":1737034629099,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[5]","endOffset":123},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or more Pods on Nodes.","v":"26","first":true,"title":"Scheduling, Preemption and Eviction | Kubernetes","_id":"a36b7c8f2408b317a66a818554d342cf","_rev":"1-f224dd6a5a714375934227b436f1bf5f","_revisions":{"start":1,"ids":["f224dd6a5a714375934227b436f1bf5f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034742962,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Involuntary disruptions cannot be prevented by PDBs; however they do count against the budget.","v":"26","_id":"a7ebd81c7964f3905391866b927930b0","_rev":"1-f6a16572de9891cf9c39e5434edb8e9b","_revisions":{"start":1,"ids":["f6a16572de9891cf9c39e5434edb8e9b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030468897,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()","endOffset":77},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"From Kubernetes perspective, sidecars graceful termination is less important.","v":"26","_id":"aa7420477253b13418d6472e248b7f50","_rev":"1-02015daaef07ff1c662069bb8a51f05e","_revisions":{"start":1,"ids":["02015daaef07ff1c662069bb8a51f05e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041024871,"range":{"startContainerPath":"//h3[@id=\"guaranteed\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":74},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Guaranteed Pods that are Guaranteed have the strictest resource limits and are least likely to face eviction.","v":"26","_id":"ab57c26e1173fb14a82200da50155cfa","_rev":"1-4d523fe44f4f0183dfc5837e782c6b8d","_revisions":{"start":1,"ids":["4d523fe44f4f0183dfc5837e782c6b8d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034399046,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Cluster managers and hosting providers should use tools which respect PodDisruptionBudgets by calling the Eviction API instead of directly deleting pods or deployments.","v":"26","_id":"ad2d112a7e5d1b52580c2a5fa447d513","_rev":"1-ee0038275c6731d88a127d756c8090a9","_revisions":{"start":1,"ids":["ee0038275c6731d88a127d756c8090a9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041085285,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":73},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For a Pod to be given a QoS class of Guaranteed:  Every Container in the Pod must have a memory limit and a memory request. For every Container in the Pod, the memory limit must equal the memory request. Every Container in the Pod must have a CPU limit and a CPU request. For every Container in the Pod, the CPU limit must equal the CPU request.","v":"26","_id":"ad2f72999dcca3d1233f16740dad73e3","_rev":"1-7594fcf81e026e274e27e68b4f7ea82b","_revisions":{"start":1,"ids":["7594fcf81e026e274e27e68b4f7ea82b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737031961012,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":64,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Since Pods are intended to be disposable and replaceable, you cannot add a container to a Pod once it has been created. Instead, you usually delete and replace Pods in a controlled fashion using deployments.","v":"26","first":true,"title":"Ephemeral Containers | Kubernetes","_id":"ada7219af432904b0b8674bf90d30e9d","_rev":"1-4895d69d787f5af9aeb03535bfd41868","_revisions":{"start":1,"ids":["4895d69d787f5af9aeb03535bfd41868"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030415511,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":242},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Sidecar containers have their own independent lifecycles. They can be started, stopped, and restarted independently of app containers. This means you can update, scale, or maintain sidecar containers without affecting the primary application.","v":"26","_id":"aee3b68e9154322af3e6b1629808acaa","_rev":"1-dc63532c0bf61f31c5bee2666d8e7bd0","_revisions":{"start":1,"ids":["dc63532c0bf61f31c5bee2666d8e7bd0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035933462,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[36]/text()[5]","endOffset":100},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A dedicated Pod DisruptionTarget condition is added to indicate that the Pod is about to be deleted due to a disruption. The reason field of the condition additionally indicates one of the following reasons for the Pod termination:","v":"26","_id":"af61965ad32593c33acb4564d69f8e94","_rev":"1-1e12982383ea661b7d433f51f459e223","_revisions":{"start":1,"ids":["1e12982383ea661b7d433f51f459e223"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737039299595,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","startOffset":33,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":311},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes assigns a QoS class to each Pod as a consequence of the resource constraints that you specify for the containers in that Pod. Kubernetes relies on this classification to make decisions about which Pods to evict when there are not enough available resources on a Node.","v":"26","first":true,"title":"Pod Quality of Service Classes | Kubernetes","_id":"b12c9e65f1069bca70b8cb7687e8843f","_rev":"1-a8e73848d219c3febd627dc2a541a0c8","_revisions":{"start":1,"ids":["a8e73848d219c3febd627dc2a541a0c8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041181606,"range":{"startContainerPath":"//h3[@id=\"besteffort\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":97},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"BestEffort Pods in the BestEffort QoS class can use node resources that aren't specifically assigned to Pods in other QoS classes.","v":"26","_id":"b290008a5222e7535802ae9dd94e8381","_rev":"1-dfdd23a978d8252a823f3b1c3dbbeae8","_revisions":{"start":1,"ids":["dfdd23a978d8252a823f3b1c3dbbeae8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030498787,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()[2]","startOffset":67,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]/text()[6]","endOffset":138},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"sidecar containers will receive the SIGTERM following with SIGKILL faster than may be expected. So exit codes different from 0 (0 indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.","v":"26","_id":"b5e0b7fb3dd368c2d872a99ca0b2556c","_rev":"1-75285530fe96a5a291e58b234c3d36eb","_revisions":{"start":1,"ids":["75285530fe96a5a291e58b234c3d36eb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737039770236,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[6]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[9]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The possible QoS classes are Guaranteed, Burstable, and BestEffort.","v":"26","_id":"ba3c535851a27396fc31de4e862a8620","_rev":"1-90e3a0deaa7353b873eb4802ebc813ae","_revisions":{"start":1,"ids":["90e3a0deaa7353b873eb4802ebc813ae"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737032025621,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","endOffset":209},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Ephemeral containers differ from other containers in that they lack guarantees for resources or execution, and they will never be automatically restarted, so they are not appropriate for building applications.","v":"26","_id":"ba6897fae1200b65f28120c9a1a632c6","_rev":"1-2eb7aae95a0d06e94d9bf12c00d1b429","_revisions":{"start":1,"ids":["2eb7aae95a0d06e94d9bf12c00d1b429"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737033455433,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[2]","endOffset":47},"className":"default-orange-da01945e-1964-4d27-8a6c-3331e1fe7f14","text":"When using ephemeral containers, it's helpful to enable process namespace sharing so you can view processes in other containers.","v":"26","_id":"c09bceb9e83cbfe0050910d05780ada0","_rev":"1-4829a90d55bf7312114f1470a43ff197","_revisions":{"start":1,"ids":["4829a90d55bf7312114f1470a43ff197"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737029923926,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[6]","startOffset":119,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[6]","endOffset":238},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"These can be started, stopped, or restarted without effecting the main application container and other init containers.","v":"26","_id":"c177f9bb2037491823bcf3743d02ce62","_rev":"1-2dd8f2d6b9b225195079b123e850bff6","_revisions":{"start":1,"ids":["2dd8f2d6b9b225195079b123e850bff6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737039794566,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[7]","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[10]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When a Node runs out of resources, Kubernetes will first evict BestEffort Pods running on that Node, followed by Burstable and finally Guaranteed Pods.","v":"26","_id":"c316188f23e3a2a03caf46a14de47406","_rev":"1-f0c601d389d5151852fafa8c92ff3a6f","_revisions":{"start":1,"ids":["f0c601d389d5151852fafa8c92ff3a6f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035925344,"range":{"startContainerPath":"//h2[@id=\"pod-disruption-conditions\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pod disruption conditions","v":"26","_id":"c507a76f39e435e1ec0b19c4171e7a79","_rev":"1-5db9bef4947de51ff34ed1a4f679fe73","_revisions":{"start":1,"ids":["5db9bef4947de51ff34ed1a4f679fe73"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737033664361,"range":{"startContainerPath":"//h2[@id=\"voluntary-and-involuntary-disruptions\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Voluntary and involuntary disruptions","v":"26","first":true,"title":"Disruptions | Kubernetes","_id":"c98d4010e7b4e62552518bf26987e4d0","_rev":"1-b453bf04493dd843fdb804de75322626","_revisions":{"start":1,"ids":["b453bf04493dd843fdb804de75322626"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030581562,"range":{"startContainerPath":"//h2[@id=\"differences-from-init-containers\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()[2]","endOffset":9},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Differences from init containers Sidecar containers work alongside the main container, extending its functionality and providing additional services.  Sidecar containers run concurrently with the main application container. They are active throughout the lifecycle of the pod and can be started and stopped independently of the main container. Unlike init containers, sidecar containers support probes to control their lifecycle.  Sidecar containers can interact directly with the main application containers, because like init containers they always share the same network, and can optionally also share volumes (filesystems).  Init containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a Pod. Any data passing is one-way (for example, an init container can put information inside an emptyDir volume).","v":"26","_id":"ca10ead8aff8ee247e31d35dc7ec3bbf","_rev":"1-7339279a672e9394fc2b038a0cad370f","_revisions":{"start":1,"ids":["7339279a672e9394fc2b038a0cad370f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737038974103,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[42]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[42]/text()","endOffset":126},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If you do not have such a separation of responsibilities in your organization, you may not need to use Pod Disruption Budgets.","v":"26","_id":"ca5fae9a44e8f0e43e33cfb61a26167a","_rev":"1-7843a3a49d8c7e245667863a4d0f9554","_revisions":{"start":1,"ids":["7843a3a49d8c7e245667863a4d0f9554"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737036145068,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt[5]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[5]/text()","endOffset":38},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"TerminationByKubelet Pod has been terminated by the kubelet","v":"26","_id":"cb93fa709f2cd8f0e968c1717fa29f55","_rev":"1-d5c8926ff5506d717e7a76a0c7103dd5","_revisions":{"start":1,"ids":["d5c8926ff5506d717e7a76a0c7103dd5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737032718614,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","endOffset":106},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Ephemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed or a container image doesn't include debugging utilities.","v":"26","_id":"cbea076a808914ca67e44dd4fa15fc6a","_rev":"1-5d41b2547ce908a8f559abdb3b667375","_revisions":{"start":1,"ids":["5d41b2547ce908a8f559abdb3b667375"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030332888,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[2]","endOffset":367},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Upon Pod termination, the kubelet postpones terminating sidecar containers until the main application container has fully stopped. The sidecar containers are then shut down in the opposite order of their appearance in the Pod specification. This approach ensures that the sidecars remain operational, supporting other containers within the Pod, until their service is no longer required.","v":"26","_id":"cbf8e0a4222584f05043f6aa784ceb75","_rev":"1-7842e7157b33806939e373d5acec34d3","_revisions":{"start":1,"ids":["7842e7157b33806939e373d5acec34d3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035993264,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt[3]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[3]/text()[2]","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"EvictionByEvictionAPI Pod has been marked for eviction using the Kubernetes API .","v":"26","_id":"ccfa43481dd47e1e04e24e6e2ac026b8","_rev":"1-226af50e626ba00c17fff456f36fafd0","_revisions":{"start":1,"ids":["226af50e626ba00c17fff456f36fafd0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737036156091,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt[5]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[5]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"TerminationByKubelet Pod has been terminated by the kubelet, because of either node pressure eviction, the graceful node shutdown, or preemption for system critical pods.","v":"26","_id":"d042a99ed682ca59ac20e6899d5b8de5","_rev":"1-97a123971e0fbc0a7e4bf08fcc1e35fb","_revisions":{"start":1,"ids":["97a123971e0fbc0a7e4bf08fcc1e35fb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034093087,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[3]/text()[3]","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For even higher availability when running replicated applications, spread applications across racks (using anti-affinity) or across zones (if using a multi-zone cluster.)","v":"26","_id":"d0754269d7ea16ed8a3108532f20766f","_rev":"1-e69f764c73298f8efd9da8a76166c037","_revisions":{"start":1,"ids":["e69f764c73298f8efd9da8a76166c037"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737036006754,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt[4]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[4]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DeletionByPodGC Pod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage collection.","v":"26","_id":"d0d88c7d9161cce54b57531e4af2e5f7","_rev":"1-cff952490ec1d96fa6e00afa3c859798","_revisions":{"start":1,"ids":["cff952490ec1d96fa6e00afa3c859798"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737033444204,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[2]","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When using ephemeral containers, it's helpful to enable process namespace sharing so you can view processes in other containers.","v":"26","_id":"d40b88ba8af808871194285746a1ce6e","_rev":"1-9b2078b286f748674005ae5a0bf8a067","_revisions":{"start":1,"ids":["9b2078b286f748674005ae5a0bf8a067"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737032059069,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":19},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Ephemeral containers may not have ports, so fields such as ports, livenessProbe, readinessProbe are disallowed. Pod resource allocations are immutable, so setting resources is disallowed. For a complete list of allowed fields, see the EphemeralContainer reference documentation. Ephemeral containers are created using a special ephemeralcontainers handler in the API","v":"26","_id":"d78ea32989f188c0bb862828def6b4fe","_rev":"1-17df3660fa44c07c95134895b2b06c3d","_revisions":{"start":1,"ids":["17df3660fa44c07c95134895b2b06c3d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041152110,"range":{"startContainerPath":"//h3[@id=\"burstable\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":101},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Burstable Pods that are Burstable have some lower-bound resource guarantees based on the request, but do not require a specific limit.","v":"26","_id":"da941aa6b4fe71fe8cb1750d636627ef","_rev":"1-c42b6d67a116e69d83934a0bb0587151","_revisions":{"start":1,"ids":["c42b6d67a116e69d83934a0bb0587151"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035519220,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]/text()[4]","endOffset":30},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"It is recommended to set AlwaysAllow Unhealthy Pod Eviction Policy to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.","v":"26","_id":"dec3b22815e2881c341607d29b88b761","_rev":"1-dda0eb014dfd554c8ac0357f9856acca","_revisions":{"start":1,"ids":["dda0eb014dfd554c8ac0357f9856acca"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034082649,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[4]/li[2]/text()","endOffset":59},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Here are some ways to mitigate involuntary disruptions:  Ensure your pod requests the resources it needs. Replicate your application if you need higher availability.","v":"26","_id":"dff5b9647b8a5ccb8fe35a564b0ad8db","_rev":"1-ee037f01ee105041fd0a652058b5a90d","_revisions":{"start":1,"ids":["ee037f01ee105041fd0a652058b5a90d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034164636,"range":{"startContainerPath":"//h2[@id=\"pod-disruption-budgets\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pod disruption budgets","v":"26","_id":"e013b781a9e5fde71aab65f0926ccd0d","_rev":"1-31f3dd695b088ae23224ed62d23e4d27","_revisions":{"start":1,"ids":["31f3dd695b088ae23224ed62d23e4d27"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041046001,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":204,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":23},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"These Pods can also make use of exclusive CPUs using the static CPU management policy.","v":"26","_id":"e4ac2ae197cf77f6fbc068b54ab194c3","_rev":"1-3675920255fc94252e1161d845ef7835","_revisions":{"start":1,"ids":["3675920255fc94252e1161d845ef7835"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030034245,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":69},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If an init container is created with its restartPolicy set to Always, it will start and remain running during the entire life of the Pod.","v":"26","_id":"e6f19190357b025d68ccce4791deca8a","_rev":"1-2f39c5ab575f2c8fa1dd155f11b7ff90","_revisions":{"start":1,"ids":["2f39c5ab575f2c8fa1dd155f11b7ff90"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737034219578,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","endOffset":205},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions","v":"26","_id":"eaf1600587d0a13113fe6159de76600d","_rev":"1-90f9117e7cb77a029b1f20ffd74a1464","_revisions":{"start":1,"ids":["90f9117e7cb77a029b1f20ffd74a1464"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035954477,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd/text()[2]","endOffset":73},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"PreemptionByScheduler Pod is due to be preempted by a scheduler in order to accommodate a new Pod with a higher priority.","v":"26","_id":"ec48535259100ec817ed4c4c84a85b09","_rev":"1-5f0529ac576742e2cf68ce3383c8137c","_revisions":{"start":1,"ids":["5f0529ac576742e2cf68ce3383c8137c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737035986739,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dt[2]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/dl/dd[2]/text()[3]","endOffset":37},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DeletionByTaintManager Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within kube-controller-manager) due to a NoExecute taint that the Pod does not tolerate","v":"26","_id":"f16fbbac5566da198f327b585a9b76c4","_rev":"1-ae2c9184689d700eee405645635c696d","_revisions":{"start":1,"ids":["ae2c9184689d700eee405645635c696d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/","date":1737032065835,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Ephemeral containers may not have ports, so fields such as ports, livenessProbe, readinessProbe are disallowed. Pod resource allocations are immutable, so setting resources is disallowed. For a complete list of allowed fields, see the EphemeralContainer reference documentation. Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it's not possible to add an ephemeral container using kubectl edit.","v":"26","_id":"f649fed6b587ef5c520e718272916aef","_rev":"1-4950fe96a61bb434a266be50f7fe2727","_revisions":{"start":1,"ids":["4950fe96a61bb434a266be50f7fe2727"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030019074,"range":{"startContainerPath":"//div[@id=\"application-deployment-sidecar-yaml\"]/div/pre/code/span[24]/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"application-deployment-sidecar-yaml\"]/div/pre/code/span[31]/span/text()[2]","endOffset":4},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"initContainers:         - name: logshipper           image: alpine:latest           restartPolicy: Always           command: ['sh', '-c', 'tail -F /opt/logs.txt']           volumeMounts:             - name: data               mountPath: /opt","v":"26","_id":"f78148d1affd77e17f06271f8f4d6301","_rev":"1-f1b9590adff0de92c6f8f88c6cd5b8ef","_revisions":{"start":1,"ids":["f1b9590adff0de92c6f8f88c6cd5b8ef"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041285024,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":1,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, if you have a node with 16 CPU cores available to the kubelet, and you assign 4 CPU cores to a Guaranteed Pod, then a Pod in the BestEffort QoS class can try to use any amount of the remaining 12 CPU cores.","v":"26","_id":"fa63f71bacc78dab44eb07031f8d9ca0","_rev":"1-74813ac87ed42d0ba94f42fb46f1d986","_revisions":{"start":1,"ids":["74813ac87ed42d0ba94f42fb46f1d986"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/","date":1737038968377,"range":{"startContainerPath":"//h2[@id=\"separating-cluster-owner-and-application-owner-roles\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[40]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Separating Cluster Owner and Application Owner Roles","v":"26","_id":"facadc4d50be55c6d8a9e9a5f4901113","_rev":"1-ccf84d5d325cb7d6881dcced91eb5fc3","_revisions":{"start":1,"ids":["ccf84d5d325cb7d6881dcced91eb5fc3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737029866681,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":54},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Kubernetes implements sidecar containers as a special case of init containers; sidecar containers remain running after Pod startup.","v":"26","_id":"fd26eea4373a85c8585e6f046299e7c0","_rev":"1-5c416bc332fbe91f3e0bc89fb6c59cf2","_revisions":{"start":1,"ids":["5c416bc332fbe91f3e0bc89fb6c59cf2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/","date":1737030216772,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[3]","startOffset":12,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[4]","endOffset":89},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"After a sidecar-style init container is running (the kubelet has set the started status for that init container to true), the kubelet then starts the next init container","v":"26","_id":"fd5da1d894c789102dd045bde2136e85","_rev":"1-878e39ef0ac5ada3247aae0a0594c10f","_revisions":{"start":1,"ids":["878e39ef0ac5ada3247aae0a0594c10f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041168870,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[2]/text()","endOffset":71},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod is given a QoS class of Burstable if:  The Pod does not meet the criteria for QoS class Guaranteed. At least one Container in the Pod has a memory or CPU request or limit.","v":"26","_id":"ffa3bd7a5259fa97feafa88fba5ead9f","_rev":"1-dbd05dc866ad9e4c6ea0142f2fbfeefe","_revisions":{"start":1,"ids":["dbd05dc866ad9e4c6ea0142f2fbfeefe"]}}]}

{"seq":561}

{"docs":[{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737125858666,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p[5]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).","v":"26","_id":"a0c02f7d5fc5605791ff2fe06fe72a10","_rev":"1-78463fa96d6ba24657a244d8bb12ac7a","_revisions":{"start":1,"ids":["78463fa96d6ba24657a244d8bb12ac7a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124888109,"range":{"startContainerPath":"//h2[@id=\"creating-a-deployment\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Creating a Deployment","v":"26","_id":"a10ee156103833257dc600361bdb1990","_rev":"1-8b22e0be74a14c7021dc742f8eafa8f4","_revisions":{"start":1,"ids":["8b22e0be74a14c7021dc742f8eafa8f4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124840050,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[7]/a/text()","startOffset":0,"endContainerPath":"//h2[@id=\"creating-a-deployment\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Clean up older ReplicaSets that you don't need anymore.","v":"26","_id":"a1b0c9e1a1ca38d4d1cf1bcc80b3e9b6","_rev":"1-b07f374350571396130b38272576205e","_revisions":{"start":1,"ids":["b07f374350571396130b38272576205e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041343597,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","startOffset":0,"endContainerPath":"//h4[@id=\"criteria-2\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kubelet prefers to evict BestEffort Pods if the node comes under resource pressure.","v":"26","_id":"a211a498361be6b0cb76a0e46d3f8193","_rev":"1-aa63d3944e08b2840e8ae8ec81cde84a","_revisions":{"start":1,"ids":["aa63d3944e08b2840e8ae8ec81cde84a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124763673,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()[3]","endOffset":68},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.","v":"26","_id":"a23f5a02727614750ffc4def3921b84a","_rev":"1-eecfa66de9030590b2ffc182f550b346","_revisions":{"start":1,"ids":["eecfa66de9030590b2ffc182f550b346"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124209269,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In Kubernetes, there are two ways to expose Pod and container fields to a running container:  as environment variables as files in a downwardAPI volume Together, these two ways of exposing Pod and container fields are called the downward API.","v":"26","_id":"a27bf2333c0af3c9a50c3c467908b46f","_rev":"1-b15f8d9fb7d9f12ff91690000e7ba34f","_revisions":{"start":1,"ids":["b15f8d9fb7d9f12ff91690000e7ba34f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041744232,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[4]","startOffset":36,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[5]","endOffset":91},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When memory.min is set to memory requests, memory resources are reserved and never reclaimed by the kernel","v":"26","_id":"a41b67a1b09e2fa55cabaf662d8ca7df","_rev":"1-7e17b2a77438dc6cc93b05bcc0650246","_revisions":{"start":1,"ids":["7e17b2a77438dc6cc93b05bcc0650246"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737125496341,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[6]/text()[2]","endOffset":12},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed","v":"26","_id":"a4dc424a312643fbf12d6f77df6e7bfa","_rev":"1-a19239b582a9793e658ddaf4c2bcf36f","_revisions":{"start":1,"ids":["a19239b582a9793e658ddaf4c2bcf36f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041974626,"range":{"startContainerPath":"//h2[@id=\"class-independent-behavior\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some behavior is independent of QoS class","v":"26","_id":"a54861b7b9c094c05e110c7d79e73880","_rev":"1-04aaf18ee143b9d30ff691814292603d","_revisions":{"start":1,"ids":["04aaf18ee143b9d30ff691814292603d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124818456,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/a/text()","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Rollback to an earlier Deployment revision","v":"26","_id":"a6b80b475a6e5a3d1aef8c7c6b849fa0","_rev":"1-b2033e8b191c9b2d400a28856c05e582","_revisions":{"start":1,"ids":["b2033e8b191c9b2d400a28856c05e582"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737125035019,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[3]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[3]/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment.","v":"26","_id":"a749feed4c0145b5a4afe3f63a12d53c","_rev":"1-9790ac82cec6343bd2e42503d0153af6","_revisions":{"start":1,"ids":["9790ac82cec6343bd2e42503d0153af6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737125297421,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[6]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol/li[6]/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"To see the labels automatically generated for each Pod, run kubectl get pods --show-labels.","v":"26","_id":"a87421ab287f5f89f6a3bcf439ce049f","_rev":"1-6c97ee46788d45b33a9b63a538d946eb","_revisions":{"start":1,"ids":["6c97ee46788d45b33a9b63a538d946eb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041977827,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Any Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that Pod.","v":"26","_id":"aaf7508ec4e8bfcd22eb8fd9d1138ad7","_rev":"1-6b4605c345598408133e708e976dfad4","_revisions":{"start":1,"ids":["6b4605c345598408133e708e976dfad4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737123211287,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","endOffset":277},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Without using a user namespace a container running as root, in the case of a container breakout, has root privileges on the node. And if some capability were granted to the container, the capabilities are valid on the host too. None of this is true when we use user namespaces.","v":"26","_id":"acbb8e238859998fe188569b2d874029","_rev":"1-ac5a7154d3a778a99ca239686840f219","_revisions":{"start":1,"ids":["ac5a7154d3a778a99ca239686840f219"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122406306,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[3]/text()","endOffset":92},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some OCI runtimes do not include the support needed for using user namespaces in Linux pods.","v":"26","_id":"aede0364873c36048086e4e761a12ce6","_rev":"1-c01910f2a66e58e4c76778d675e6c397","_revisions":{"start":1,"ids":["c01910f2a66e58e4c76778d675e6c397"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124827672,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[5]/a/text()","endOffset":33},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pause the rollout of a Deployment","v":"26","_id":"aef67d2e514b4c08a6d8f4a4e43a8703","_rev":"1-0bbce5d75a87161576b9d03ad371fd4c","_revisions":{"start":1,"ids":["0bbce5d75a87161576b9d03ad371fd4c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737125890743,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/p[6]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).","v":"26","_id":"b2458ff3e684bb902f87bbc2dc696f89","_rev":"1-d6fa8622dfd251ea7df70453947be94c","_revisions":{"start":1,"ids":["d6fa8622dfd251ea7df70453947be94c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/","date":1737124553599,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You use the Kubernetes API to create a workload object that represents a higher abstraction level than a Pod, and then the Kubernetes control plane automatically manages Pod objects on your behalf, based on the specification for the workload object you defined.","v":"26","first":true,"title":"Workload Management | Kubernetes","_id":"b3541c7df25cda1cf1caba678d76c15c","_rev":"1-f50bbcf7bbce604aedf77edc97787aa0","_revisions":{"start":1,"ids":["f50bbcf7bbce604aedf77edc97787aa0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126267036,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[8]/h4","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.","v":"26","_id":"b57ea88cf59f1ca5b1f684795cc5da9f","_rev":"1-6824d61c36916a672c26defeda908fdb","_revisions":{"start":1,"ids":["6824d61c36916a672c26defeda908fdb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041434809,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[9]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Pod has a QoS class of BestEffort if it doesn't meet the criteria for either Guaranteed or Burstable.","v":"26","_id":"b6f114449e3e2723b42c37a3d2900e0a","_rev":"1-c7828b7371d0d018d9959776231cc987","_revisions":{"start":1,"ids":["c7828b7371d0d018d9959776231cc987"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041717430,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Memory QoS uses the memory controller of cgroup v2","v":"26","_id":"b7803cf58d40c263d036fca2db0e6dc5","_rev":"1-7ed5de83159a3ee72440c97d7e6f9e17","_revisions":{"start":1,"ids":["7ed5de83159a3ee72440c97d7e6f9e17"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122559740,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()[5]","endOffset":47},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The runAsUser, runAsGroup, fsGroup, etc. fields in the pod.spec always refer to the user inside the container.","v":"26","_id":"b86e368a97a84c58341d420193d42b40","_rev":"1-6d308a61d6aff3bbbfb4b2f83eaf5821","_revisions":{"start":1,"ids":["6d308a61d6aff3bbbfb4b2f83eaf5821"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124471632,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[2]","startOffset":9,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[8]/text()","endOffset":85},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The fields available via either mechanism are:  https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef","v":"26","_id":"ba5e164cfd7c831e8110f347f1688e16","_rev":"1-d412e95689eff9f3b263a09a6ad71ecb","_revisions":{"start":1,"ids":["d412e95689eff9f3b263a09a6ad71ecb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124925404,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[3]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[3]/p/text()[2]","endOffset":69},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The .spec.selector field defines how the created ReplicaSet finds which Pods to manage.","v":"26","_id":"bf5e8b8a16f6d24e450fff5dc6372f52","_rev":"1-2919f539036c7992b7343a7989272108","_revisions":{"start":1,"ids":["2919f539036c7992b7343a7989272108"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124833513,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[6]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[6]/a/text()","endOffset":32},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Use the status of the Deployment","v":"26","_id":"c029c31eb115d4317627711018d61a11","_rev":"1-fe33d5ad93f19fff08f792423ac23b2a","_revisions":{"start":1,"ids":["fe33d5ad93f19fff08f792423ac23b2a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737042139469,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The kube-scheduler does not consider QoS class when selecting which Pods to preempt.","v":"26","_id":"c0d8ce2ed22dac315cddce84ea456f7f","_rev":"1-36716c108ef75210ab52c7b32025ed23","_revisions":{"start":1,"ids":["36716c108ef75210ab52c7b32025ed23"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124814430,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[2]/a/text()","endOffset":33},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Declare the new state of the Pods","v":"26","_id":"c1fb8a0845a05af39672dbaafcadb536","_rev":"1-6466959f3895f53e7f5cbdf8a0e11628","_revisions":{"start":1,"ids":["6466959f3895f53e7f5cbdf8a0e11628"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122474783,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A pod can opt-in to use user namespaces by setting the pod.spec.hostUsers field to false.","v":"26","_id":"c38b40f77961ce76b30ef0c1cf622d77","_rev":"1-4b7d82635ac619fe312f46a1a8fe4183","_revisions":{"start":1,"ids":["4b7d82635ac619fe312f46a1a8fe4183"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124749812,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Deployment provides declarative updates for Pods and ReplicaSets.","v":"26","first":true,"title":"Deployments | Kubernetes","_id":"c61d59ee91014bb1452655eaa7a2546d","_rev":"1-e0b3b6b3d1cb5802e311c4f55d8187f5","_revisions":{"start":1,"ids":["e0b3b6b3d1cb5802e311c4f55d8187f5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122428021,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","startOffset":13,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()","endOffset":95},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"the container runtime and its underlying OCI runtime must support user namespaces.","v":"26","_id":"c7fa626bc0ebc4b58ff46ee81e8b9ff3","_rev":"1-fe8bf475079641db183c04ac0b7176d6","_revisions":{"start":1,"ids":["fe8bf475079641db183c04ac0b7176d6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124801511,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":52},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The following are typical use cases for Deployments:","v":"26","_id":"ca5b1768bb181aa6c20b3548af8d092a","_rev":"1-2dcb84b415211b92d5c3b4e24b7c6e5c","_revisions":{"start":1,"ids":["2dcb84b415211b92d5c3b4e24b7c6e5c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737123072090,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[21]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This abstraction limits what can happen, for example, if the container manages to escape to the host. Given that the container is running as a non-privileged user on the host, it is limited what it can do to the host.","v":"26","_id":"cbd6d35e90d587dd2f87bc0072fdf14e","_rev":"1-2fb21b968735072819f0df434c217008","_revisions":{"start":1,"ids":["2fb21b968735072819f0df434c217008"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122236066,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","endOffset":91},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A process running as root in a container can run as a different (non-root) user in the host","v":"26","_id":"cd0cd609c16a98b2f563c22b9d7a03da","_rev":"1-17d7e6c8ca217b11bc988fc2d700d270","_revisions":{"start":1,"ids":["17d7e6c8ca217b11bc988fc2d700d270"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737042034957,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/p/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a candidate for eviction.","v":"26","_id":"cd6f021bb82b0c7cc1f7deff277d0182","_rev":"1-58381018fd8935c919eb2a3a3de6fc89","_revisions":{"start":1,"ids":["58381018fd8935c919eb2a3a3de6fc89"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124161625,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()[2]","endOffset":124},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"It is sometimes useful for a container to have information about itself, without being overly coupled to Kubernetes. The downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.","v":"26","first":true,"title":"Downward API | Kubernetes","_id":"cfbdb73fb36a983ae1e74bf198793ac5","_rev":"1-d2f47b10b6ddf65bca122fa1b9132cca","_revisions":{"start":1,"ids":["d2f47b10b6ddf65bca122fa1b9132cca"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122359195,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":23,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":109},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"you need at least Linux 6.3, as tmpfs started supporting idmap mounts in that version.","v":"26","_id":"d2c6e9fa6bcbafdc46fc3d8d1a9c32b8","_rev":"1-fa55fb889b09fbca30357d5b4a70479e","_revisions":{"start":1,"ids":["fa55fb889b09fbca30357d5b4a70479e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/","date":1737124657510,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","startOffset":83,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[7]/text()[3]","endOffset":190},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You use a DaemonSet when the driver, or other node-level service, has to run on the node where it's useful.","v":"26","_id":"d4b72a26bf6d2eea4e584472f1b57809","_rev":"1-602ad70ca874f97bd57ba5e2df9d35e5","_revisions":{"start":1,"ids":["602ad70ca874f97bd57ba5e2df9d35e5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124383630,"range":{"startContainerPath":"//h2[@id=\"available-fields\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Available fields","v":"26","_id":"d6e6322762b0525a33af2358b54c73af","_rev":"1-982d9329f819977f761d20b907fadc97","_revisions":{"start":1,"ids":["982d9329f819977f761d20b907fadc97"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/","date":1737041836786,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[5]","startOffset":187,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[10]/text()[6]","endOffset":131},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Memory QoS uses memory.high to throttle workload approaching its memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.","v":"26","_id":"d79b7f60a5e9fb7ec4ef193dc9902bed","_rev":"1-123552ee900303ecfeb6d7d0de00ce91","_revisions":{"start":1,"ids":["123552ee900303ecfeb6d7d0de00ce91"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124392961,"correspondingDocumentId":"d6e6322762b0525a33af2358b54c73af","_id":"dc16475b-a0fd-4c9c-83f8-104b0f477d39","_rev":"1-853b446a69d22add6506dceef9311b54","_revisions":{"start":1,"ids":["853b446a69d22add6506dceef9311b54"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/","date":1737124606103,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","startOffset":214,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[2]","endOffset":327},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The most common use for a StatefulSet is to be able to make a link between its Pods and their persistent storage.","v":"26","_id":"e06817993f6b91d41baf3e0109646fb1","_rev":"1-c1af679ede1df50955c811ad25aa0783","_revisions":{"start":1,"ids":["c1af679ede1df50955c811ad25aa0783"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124822589,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Scale up the Deployment to facilitate more load.","v":"26","_id":"e301675ae22bc8982b57d2acbe199cb7","_rev":"1-ccc18926310121963603c5c7b4aa920f","_revisions":{"start":1,"ids":["ccc18926310121963603c5c7b4aa920f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/","date":1737124358916,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can pass information from available Pod-level fields using fieldRef. At the API level, the spec for a Pod always defines at least one Container. You can pass information from available Container-level fields using resourceFieldRef.","v":"26","_id":"ec21befc0af3bd6af0310c1b004beddd","_rev":"1-14c984cf1c422638198ec81306783b3e","_revisions":{"start":1,"ids":["14c984cf1c422638198ec81306783b3e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124892386,"range":{"startContainerPath":"//div[@id=\"controllers-nginx-deployment-yaml\"]/div/pre/code/span/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"controllers-nginx-deployment-yaml\"]/div/pre/code/span[21]/span/span[4]/text()","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment   labels:     app: nginx spec:   replicas: 3   selector:     matchLabels:       app: nginx   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2         ports:         - containerPort: 80","v":"26","_id":"eca21e6661483dfcef5336210d68e3cf","_rev":"1-89c84c4dc00610d6e1ce39fe50ffbf6c","_revisions":{"start":1,"ids":["89c84c4dc00610d6e1ce39fe50ffbf6c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737124807235,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/a/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Create a Deployment to rollout a ReplicaSet.","v":"26","_id":"eeb6a2d8890a3345f87db52d474d240b","_rev":"1-14bd7deefd47d73efaf352ace56c18e7","_revisions":{"start":1,"ids":["14bd7deefd47d73efaf352ace56c18e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122735485,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()","startOffset":196,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()","endOffset":297},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If a user namespace is used, this will isolate the users in the container from the users in the node.","v":"26","_id":"f031571f46ccb4d31a96ff2384872b29","_rev":"1-80262a9ca60fa858b415a3ad0be4e462","_revisions":{"start":1,"ids":["80262a9ca60fa858b415a3ad0be4e462"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122279674,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","endOffset":120},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can use this feature to reduce the damage a compromised container can do to the host or other pods in the same node.","v":"26","_id":"f0652e08ebfab47756d5ef2cea297a43","_rev":"1-52e072b81ebe68e1c607f3a432ac0475","_revisions":{"start":1,"ids":["52e072b81ebe68e1c607f3a432ac0475"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122214766,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":68,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","endOffset":157},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A user namespace isolates the user running inside the container from the one in the host.","v":"26","first":true,"title":"User Namespaces | Kubernetes","_id":"f0b88972fb467a12e644fcc6b97cc4d5","_rev":"1-d86af6df6ee544856c833fa2249a4de1","_revisions":{"start":1,"ids":["d86af6df6ee544856c833fa2249a4de1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/","date":1737124577823,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","startOffset":62,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()[2]","endOffset":132},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment is a good fit for managing a stateless application workload","v":"26","_id":"fcd252019b6163f65200ad8775dab797","_rev":"1-f1d0b431bb3dd5e89779574b3750d81a","_revisions":{"start":1,"ids":["f1d0b431bb3dd5e89779574b3750d81a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/","date":1737122746606,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","endOffset":83},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This means containers can run as root and be mapped to a non-root user on the host.","v":"26","_id":"ff941dcc3d48d67d8f75355e7b8b94e9","_rev":"1-8c6db5207cbcbbc578012eedb2cc7f2f","_revisions":{"start":1,"ids":["8c6db5207cbcbbc578012eedb2cc7f2f"]}}]}

{"seq":611}

{"docs":[{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315179006,"correspondingDocumentId":"f936d804e4dffeed0307328bd5f94028","_id":"597b18cc-e84d-49f7-9834-57379ebc508b","_rev":"1-6e0d72922a46aa0085772b3ad4c83196","_revisions":{"start":1,"ids":["6e0d72922a46aa0085772b3ad4c83196"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314864304,"correspondingDocumentId":"b99410022e64c7ad7520666726d538ab","_id":"663a52ff-deee-4706-bbb2-24dcd83825aa","_rev":"1-3cabc46200e0983bcc3513cc67ef5e7b","_revisions":{"start":1,"ids":["3cabc46200e0983bcc3513cc67ef5e7b"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314865768,"correspondingDocumentId":"d3a511e240b4fd4f0cb78346106ac72b","_id":"74c335cf-5e07-4d60-8555-56197d1a8eeb","_rev":"1-dbc28de3ab1c8280a5cc51990cf37e2c","_revisions":{"start":1,"ids":["dbc28de3ab1c8280a5cc51990cf37e2c"]}},{"verb":"delete","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314786721,"correspondingDocumentId":"e78f1a7786dce7310eef79a9eb40d1c2","_id":"9fdee97b-451b-4708-a73d-8c5223c25156","_rev":"1-ce58941619d3466a37050d8294e8ca5c","_revisions":{"start":1,"ids":["ce58941619d3466a37050d8294e8ca5c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314576414,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[74]/code/text()","startOffset":0,"endContainerPath":"//h4[@id=\"recreate-deployment\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":".spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is the default value.","v":"26","_id":"a1188a9f46582956fbf870b6b83e611d","_rev":"1-cde5329858cd5a7799ee1595c49e3978","_revisions":{"start":1,"ids":["cde5329858cd5a7799ee1595c49e3978"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314490182,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[24]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[24]/text()","endOffset":320},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.","v":"26","_id":"a16e48189717f42a26602bdf77c758a8","_rev":"1-d0651fba6a5aaaf3c7a5b69e82e2a9a4","_revisions":{"start":1,"ids":["d0651fba6a5aaaf3c7a5b69e82e2a9a4"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738071467378,"range":{"startContainerPath":"//h3[@id=\"proportional-scaling\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"proportional-scaling\"]/text()","endOffset":20},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Proportional scaling","v":"26","_id":"a20875e89f49f4cc20513ad471581e89","_rev":"1-fc089f4b0a5f3530491dd489d877e9b9","_revisions":{"start":1,"ids":["fc089f4b0a5f3530491dd489d877e9b9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075464841,"range":{"startContainerPath":"//h2[@id=\"deployment-status\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deployment status","v":"26","_id":"a3cf9d83fd8a6cc70e909c259f68de52","_rev":"1-28226c500327cacd2d93a55885bcc9d1","_revisions":{"start":1,"ids":["28226c500327cacd2d93a55885bcc9d1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126456408,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li/p/text()","startOffset":7,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li/div/pre/code/span/span/text()[2]","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"check the revisions of this Deployment:  kubectl rollout history deployment/nginx-deployment","v":"26","_id":"a445ed302c4109a74d410817ccf8346d","_rev":"1-c8d60707eee087b13f7e68fddfec0733","_revisions":{"start":1,"ids":["c8d60707eee087b13f7e68fddfec0733"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314725890,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[25]/text()","startOffset":0,"endContainerPath":"//h4[@id=\"rolling-update-deployment\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an \"at most\" guarantee for your Pods, you should consider using a StatefulSet.","v":"26","_id":"a495d30efa4b9628618f8c9d0c5b058b","_rev":"1-f3a79566c54ee8b12b2287573e3ada5c","_revisions":{"start":1,"ids":["f3a79566c54ee8b12b2287573e3ada5c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126841178,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[11]/pre/code/span/span/span[6]/text()","endOffset":2},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Assuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods.  kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80","v":"26","_id":"a50c4f6a965690a1841b17e828f47f1b","_rev":"1-5acbc866d5bde95be76e43317dc4d2cb","_revisions":{"start":1,"ids":["5acbc866d5bde95be76e43317dc4d2cb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314920506,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[80]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[81]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.","v":"26","_id":"abdbbf319041cc749b7b852b16993cac","_rev":"1-b125373141b101fefde9bb132eaa4966","_revisions":{"start":1,"ids":["b125373141b101fefde9bb132eaa4966"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738145541618,"range":{"startContainerPath":"//h2[@id=\"clean-up-policy\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[56]/text()[2]","endOffset":174},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Clean up Policy You can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.","v":"26","_id":"ac8e8da701e4f0840b3a1588ac5551b6","_rev":"1-cdae69881d0cd89590033cc36dd94353","_revisions":{"start":1,"ids":["cdae69881d0cd89590033cc36dd94353"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738071674684,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[4]/p/text()","startOffset":59,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[6]/li[4]/p/text()","endOffset":613},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.","v":"26","_id":"acb3f25d6b8f204d82f5f787904cf14d","_rev":"1-f57a75bd1ccc43e82733d14bb7d1d254","_revisions":{"start":1,"ids":["f57a75bd1ccc43e82733d14bb7d1d254"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075726596,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[9]/li/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[32]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"type: Progressing status: \"True\" reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdated You can monitor the progress for a Deployment by using kubectl rollout status.","v":"26","_id":"b22bd3fbfb7abc53ef7124d682b98e65","_rev":"1-e08f2ca26ac8e19c4439fc7e4ca6d139","_revisions":{"start":1,"ids":["e08f2ca26ac8e19c4439fc7e4ca6d139"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738074789704,"range":{"startContainerPath":"//h2[@id=\"pausing-and-resuming-a-deployment\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pausing and Resuming a rollout of a Deployment","v":"26","_id":"b262db0f86e8c314d4c1e906146d4f6b","_rev":"1-2d366f263d4814e141cb2886fcba5339","_revisions":{"start":1,"ids":["2d366f263d4814e141cb2886fcba5339"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314868520,"range":{"startContainerPath":"//h5[@id=\"max-unavailable\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[77]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Max Unavailable","v":"26","_id":"b57378848cb2408fb782a465d38bebc1","_rev":"1-fed3012c0861ed75f2eda5b6cef52f18","_revisions":{"start":1,"ids":["fed3012c0861ed75f2eda5b6cef52f18"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738145672313,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[64]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[64]/text()[3]","endOffset":51},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.","v":"26","_id":"b587e94587521d03466a226e5283f606","_rev":"1-a082eedbed4778db481537905a9d4a40","_revisions":{"start":1,"ids":["a082eedbed4778db481537905a9d4a40"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314891886,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[79]/text()[2]","startOffset":7,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[79]/text()[2]","endOffset":76},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The absolute number is calculated from the percentage by rounding up.","v":"26","_id":"b705e98866259694b07442ccbabbe0c2","_rev":"1-0d8b47c042b0655739c35b87a0c2150c","_revisions":{"start":1,"ids":["0d8b47c042b0655739c35b87a0c2150c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126763013,"range":{"startContainerPath":"//h3[@id=\"rolling-back-to-a-previous-revision\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[17]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Rolling Back to a Previous Revision","v":"26","_id":"b7eaf42ab7d49445214a0770cd706420","_rev":"1-c2d9c82cee5f1844720d20a8a91984ad","_revisions":{"start":1,"ids":["c2d9c82cee5f1844720d20a8a91984ad"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314096882,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[72]/code/text()","endOffset":14},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template","v":"26","_id":"b89dabad018ba5cd6eac15b4637b0a8d","_rev":"1-d61741d27c950a63a385077fe9d4afa2","_revisions":{"start":1,"ids":["d61741d27c950a63a385077fe9d4afa2"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738074836624,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[2]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[4]/p/text()","endOffset":34},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pause by running the following command:  kubectl rollout pause deployment/nginx-deployment The output is similar to this:  deployment.apps/nginx-deployment paused Then update the image of the Deployment:  kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 The output is similar to this:  deployment.apps/nginx-deployment image updated Notice that no new rollout started","v":"26","_id":"b9496c8efefd8998b5fc795597ec100d","_rev":"1-a2dc4a7187db21755c31be64974edc6d","_revisions":{"start":1,"ids":["a2dc4a7187db21755c31be64974edc6d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314818281,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[77]/text()","startOffset":220,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[77]/text()","endOffset":287},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The absolute number is calculated from percentage by rounding down.","v":"26","_id":"b99410022e64c7ad7520666726d538ab","_rev":"1-e80782e1c9982d6eb221a4ecbe885cf3","_revisions":{"start":1,"ids":["e80782e1c9982d6eb221a4ecbe885cf3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075713507,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()[4]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete, or it can fail to progress.","v":"26","_id":"bde38da037c1a1d93d5e5cd4ec21fdef","_rev":"1-1e0e06c832b8c70c884414ffba8c7422","_revisions":{"start":1,"ids":["1e0e06c832b8c70c884414ffba8c7422"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314873488,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[77]/text()","startOffset":220,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[77]/text()","endOffset":287},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The absolute number is calculated from percentage by rounding down.","v":"26","_id":"bf5bac0aa0d31236cfe88078cdc9556a","_rev":"1-6ec5d9d6786e430ea0682a6e4ca6485b","_revisions":{"start":1,"ids":["6ec5d9d6786e430ea0682a6e4ca6485b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314505558,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[73]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"strategy\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly.","v":"26","_id":"c07aecab8c700206b64fe04dca010a74","_rev":"1-f0f9e5c80c607444ae981c03b8069f47","_revisions":{"start":1,"ids":["f0f9e5c80c607444ae981c03b8069f47"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075808814,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[13]/li/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[13]/li[3]/code/text()","endOffset":32},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"type: Progressing status: \"False\" reason: ProgressDeadlineExceeded","v":"26","_id":"c25387a654d7fba4a8b0e4f104b2a4a2","_rev":"1-49e1cab5814773821cd95de6a2364221","_revisions":{"start":1,"ids":["49e1cab5814773821cd95de6a2364221"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314811049,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[76]/text()[2]","startOffset":2,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[76]/text()[4]","endOffset":39},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can specify maxUnavailable and maxSurge to control the rolling update process.","v":"26","_id":"c6971009d3e0dfae756cf00e1372b47f","_rev":"1-43b83fac640c703c2ded5e64c32267c8","_revisions":{"start":1,"ids":["43b83fac640c703c2ded5e64c32267c8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126803244,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[4]/li/div[2]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[4]/li/p[4]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl rollout undo deployment/nginx-deployment --to-revision=2","v":"26","_id":"c9b455e93dce27d84609f749b307aa28","_rev":"1-8be7dc1e945109c197ab87c4db19c12b","_revisions":{"start":1,"ids":["8be7dc1e945109c197ab87c4db19c12b"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314880786,"range":{"startContainerPath":"//h5[@id=\"max-surge\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[79]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Max Surge","v":"26","_id":"cb3740cbb9579a5093447c7280359c3f","_rev":"1-fcc2fbbd761c35ce5f7cfd83101ddadd","_revisions":{"start":1,"ids":["fcc2fbbd761c35ce5f7cfd83101ddadd"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126566941,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li/p[3]/text()[2]","startOffset":33,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[3]/li[2]/div/pre/code/span/span/span[3]/text()","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can specify theCHANGE-CAUSE message by:  Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\" Manually editing the manifest of the resource. To see the details of each revision, run:  kubectl rollout history deployment/nginx-deployment --revision=2","v":"26","_id":"d2913b085606f30ad20542867137736b","_rev":"1-45319e03333c84433819f4f9e9670044","_revisions":{"start":1,"ids":["45319e03333c84433819f4f9e9670044"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315259474,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[86]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[86]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":".spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs.","v":"26","_id":"d349ce5a886238c3fc376d92539093e7","_rev":"1-2671ca16339cbedd650daf22378647d8","_revisions":{"start":1,"ids":["2671ca16339cbedd650daf22378647d8"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314843287,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[78]/text()","startOffset":0,"endContainerPath":"//h5[@id=\"max-surge\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.","v":"26","_id":"d3a511e240b4fd4f0cb78346106ac72b","_rev":"1-e88cd34ac5dd9923b4986e79fb345778","_revisions":{"start":1,"ids":["e88cd34ac5dd9923b4986e79fb345778"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738074863300,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[7]/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[7]/div/pre/code/span/span/text()","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:  kubectl rollout resume deployment/nginx-deployment","v":"26","_id":"d6df5bca73748c464b62c664341b0666","_rev":"1-0d918c46262af1e45f38f57bdeff43e7","_revisions":{"start":1,"ids":["0d918c46262af1e45f38f57bdeff43e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315231528,"range":{"startContainerPath":"//h3[@id=\"revision-history-limit\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[85]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Revision History Limit","v":"26","_id":"d949e79cdde4cdb1db90338898cd87cd","_rev":"1-9bf105d5c4d88675baf7a9fd268ff1bc","_revisions":{"start":1,"ids":["9bf105d5c4d88675baf7a9fd268ff1bc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075733154,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[11]/li/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[11]/li[3]/code/text()","endOffset":30},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"type: Progressing status: \"True\" reason: NewReplicaSetAvailable","v":"26","_id":"dc3c1c25ee2af7ccfbf2f66cf45503f5","_rev":"1-45b807499095751d73b5fd4c0ffa95de","_revisions":{"start":1,"ids":["45b807499095751d73b5fd4c0ffa95de"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126824167,"range":{"startContainerPath":"//h2[@id=\"scaling-a-deployment\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Scaling a Deployment","v":"26","_id":"e701321b181b3b29cd837cbd5715840d","_rev":"1-3dcce29d76f6c589fba5b8c89b3c4064","_revisions":{"start":1,"ids":["3dcce29d76f6c589fba5b8c89b3c4064"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314595888,"range":{"startContainerPath":"//h4[@id=\"recreate-deployment\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[75]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Recreate Deployment All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate.","v":"26","_id":"e78f1a7786dce7310eef79a9eb40d1c2","_rev":"1-e58e2f76b1fa096e717ba8106bb3d432","_revisions":{"start":1,"ids":["e58e2f76b1fa096e717ba8106bb3d432"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738074843321,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[4]/div/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[4]/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl rollout history deployment/nginx-deployment","v":"26","_id":"ea29b2b3bdf537e647aaba9ac143fffd","_rev":"1-4900e47e792b2ad87222dc3faaccc5bf","_revisions":{"start":1,"ids":["4900e47e792b2ad87222dc3faaccc5bf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314004211,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[69]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[70]/text()[2]","endOffset":36},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":".spec.selector is a required field that specifies a label selector for the Pods targeted by this Deployment.  .spec.selector must match .spec.template.metadata.labels, or it will be rejected by the API.","v":"26","_id":"ecfbe59d87b3a3363e26971db4192a64","_rev":"1-27132b94d7decb1f5a3c819484be73f1","_revisions":{"start":1,"ids":["27132b94d7decb1f5a3c819484be73f1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126765801,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[4]/li/div/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ol[4]/li/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl rollout undo deployment/nginx-deployment","v":"26","_id":"ed169c1c7a3c81a0fa130b9ba01206fe","_rev":"1-34d838ee7d4179ae5770706c8f2f5379","_revisions":{"start":1,"ids":["34d838ee7d4179ae5770706c8f2f5379"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315082048,"range":{"startContainerPath":"//h3[@id=\"progress-deadline-seconds\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[82]/a/text()","endOffset":18},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Progress Deadline Seconds .spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has failed progressing","v":"26","_id":"ee1f4965b89a27c58335cb0bb9ac3b2e","_rev":"1-81746f2cfbed876ef68b187adce3ae01","_revisions":{"start":1,"ids":["81746f2cfbed876ef68b187adce3ae01"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126827023,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl scale deployment/nginx-deployment --replicas=10","v":"26","_id":"f1e2461a09f010b93e2554ec81ffe30c","_rev":"1-6098e14a4661b4beac825be8d5b646f6","_revisions":{"start":1,"ids":["6098e14a4661b4beac825be8d5b646f6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314876370,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[78]/text()","startOffset":0,"endContainerPath":"//h5[@id=\"max-surge\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.","v":"26","_id":"f240f9524c0381105b9acf3de3d22385","_rev":"1-407ccb57226fe68390ddd9a413727127","_revisions":{"start":1,"ids":["407ccb57226fe68390ddd9a413727127"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1737126363639,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[9]/text()[2]","endOffset":12},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed","v":"26","_id":"f3ffd5502862a5571c14a38f485741d0","_rev":"1-7b314cb29d9fd2bef37d5934c7b2f99f","_revisions":{"start":1,"ids":["7b314cb29d9fd2bef37d5934c7b2f99f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738075798073,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[41]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[17]/pre/code/span/span/span/text()","endOffset":42},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes:  kubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'","v":"26","_id":"f4c69cedd388cfd117a93c88cb955f11","_rev":"1-0cdcb1e046ecdcc794bfa83f0d830221","_revisions":{"start":1,"ids":["0cdcb1e046ecdcc794bfa83f0d830221"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315156875,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[83]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"min-ready-seconds\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"If specified, this field needs to be greater than .spec.minReadySeconds.","v":"26","_id":"f936d804e4dffeed0307328bd5f94028","_rev":"1-149cd13e68f238f23b481ae534e44871","_revisions":{"start":1,"ids":["149cd13e68f238f23b481ae534e44871"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738074812088,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()","endOffset":328},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.","v":"26","_id":"f96f3a48c771e31ae5534893e3ffadc4","_rev":"1-4a20cf223ddbe3c27871a1e3e1e9560a","_revisions":{"start":1,"ids":["4a20cf223ddbe3c27871a1e3e1e9560a"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315204565,"range":{"startContainerPath":"//h3[@id=\"min-ready-seconds\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[84]/text()","endOffset":112},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Min Ready Seconds .spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready","v":"26","_id":"fac31e32c2c1ddf37bb08db7670a40e2","_rev":"1-6c82eec9f069aa744ae9ce7094aa8fbe","_revisions":{"start":1,"ids":["6c82eec9f069aa744ae9ce7094aa8fbe"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738314781681,"range":{"startContainerPath":"//h4[@id=\"rolling-update-deployment\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[76]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Rolling Update Deployment","v":"26","_id":"fe8a8a4645a98f7c8ea03d2bf7a6547c","_rev":"1-e908974cfabe8c565bb3ffc8191c7b15","_revisions":{"start":1,"ids":["e908974cfabe8c565bb3ffc8191c7b15"]}}]}

{"seq":661}

{"docs":[{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330700521,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[33]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The RollingUpdate update strategy can be partitioned, by specifying a .spec.updateStrategy.rollingUpdate.partition.","v":"26","_id":"a281bf3164e82e630ea6335949033d12","_rev":"1-16fde133cd8b8e110ec55669a7707677","_revisions":{"start":1,"ids":["16fde133cd8b8e110ec55669a7707677"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738315353609,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","endOffset":184},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.","v":"26","first":true,"title":"ReplicaSet | Kubernetes","_id":"a3590d87e9d0f10822c7f5eab84fe16f","_rev":"1-92be9742b9617278c2f722829911bec0","_revisions":{"start":1,"ids":["92be9742b9617278c2f722829911bec0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659499800,"range":{"startContainerPath":"//h3[@id=\"taints-and-tolerations\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[7]/td[3]/text()[2]","endOffset":75},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Taints and tolerations The DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods:  Toleration key Effect Details node.kubernetes.io/not-ready NoExecute DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. node.kubernetes.io/unreachable NoExecute DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. node.kubernetes.io/disk-pressure NoSchedule DaemonSet Pods can be scheduled onto nodes with disk pressure issues. node.kubernetes.io/memory-pressure NoSchedule DaemonSet Pods can be scheduled onto nodes with memory pressure issues. node.kubernetes.io/pid-pressure NoSchedule DaemonSet Pods can be scheduled onto nodes with process pressure issues. node.kubernetes.io/unschedulable NoSchedule DaemonSet Pods can be scheduled onto nodes that are unschedulable. node.kubernetes.io/network-unavailable NoSchedule Only added for DaemonSet Pods that request host networking, i.e., Pods having spec.hostNetwork: true. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.","v":"26","_id":"a40c04e1109de77f28f0c685cb0f4e37","_rev":"1-02d04128cd12b19b49f6b8f30c3ad46f","_revisions":{"start":1,"ids":["02d04128cd12b19b49f6b8f30c3ad46f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659512475,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[23]/text()","endOffset":67},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can add your own tolerations to the Pods of a DaemonSet as well","v":"26","_id":"a41c5e7ac540866f5e567b518dad6c03","_rev":"1-28e957f40620ddece7f74b24b1e586ee","_revisions":{"start":1,"ids":["28e957f40620ddece7f74b24b1e586ee"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738315738368,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":6},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences field","v":"26","_id":"a856e67f87af6018def5e10ebe5e1d83","_rev":"1-815992639eee3e2c4094c6d4d3e92171","_revisions":{"start":1,"ids":["815992639eee3e2c4094c6d4d3e92171"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","date":1738315288091,"range":{"startContainerPath":"//h3[@id=\"paused\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[88]/text()","endOffset":269},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Paused .spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused.","v":"26","_id":"a99ce8f778ecf138957f8b952de50f90","_rev":"1-4a15850f3c0af5b0f37a6209cfd89f66","_revisions":{"start":1,"ids":["4a15850f3c0af5b0f37a6209cfd89f66"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659203229,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()","startOffset":79,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[19]/text()[3]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The DaemonSet controller creates a Pod for each eligible node and adds the spec.affinity.nodeAffinity field of the Pod to match the target host. After the Pod is created, the default scheduler typically takes over and then binds the Pod to the target host by setting the .spec.nodeName field.","v":"26","_id":"a9a4cc8730385d2f7603d8f7449c34b4","_rev":"1-936f1865de9226ad2d58b49d18bc5ce9","_revisions":{"start":1,"ids":["936f1865de9226ad2d58b49d18bc5ce9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325376510,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[16]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[16]/text()[4]","endOffset":74},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"For 2 ReplicaSets specifying the same .spec.selector but different .spec.template.metadata.labels and .spec.template.spec fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.","v":"26","_id":"aa87ea73a397fc60f9b2e5452134a9e3","_rev":"1-a881c0b0fd0e31105d742ad481a6a0a3","_revisions":{"start":1,"ids":["a881c0b0fd0e31105d742ad481a6a0a3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738337081896,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()","startOffset":0,"endContainerPath":"//h2[@id=\"writing-a-daemonset-spec\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.","v":"26","_id":"aaa1c9dfe076884970f11da6e6782a4e","_rev":"1-a86dce38138b42ceed0309947290cc59","_revisions":{"start":1,"ids":["a86dce38138b42ceed0309947290cc59"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738327874000,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/a/text()","endOffset":4},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Manages the deployment and scaling of a set of Pods","v":"26","_id":"b0b56712a948e43e067654500248fd45","_rev":"1-3193b0927463a4113b8d2f4aa0854b55","_revisions":{"start":1,"ids":["3193b0927463a4113b8d2f4aa0854b55"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325747568,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[45]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[46]/text()","endOffset":275},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Using the controller.kubernetes.io/pod-deletion-cost annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.  The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion cost are preferred to be deleted before pods with higher deletion cost.","v":"26","_id":"b3ba8f4713c944c4afd16b82523ed298","_rev":"1-8eb1a26c4dd3d55199413712dcd35bfb","_revisions":{"start":1,"ids":["8eb1a26c4dd3d55199413712dcd35bfb"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738327964979,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","startOffset":14,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[6]/text()","endOffset":78},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"stable is synonymous with persistence across Pod (re)scheduling.","v":"26","_id":"b949d2fc6133fcc56ce6c85da9633dbf","_rev":"1-1caffcce27292a887c6f47861beae6db","_revisions":{"start":1,"ids":["1caffcce27292a887c6f47861beae6db"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328928298,"range":{"startContainerPath":"//h3[@id=\"ordinal-index\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[14]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Ordinal Index For a StatefulSet with N replicas, each Pod in the StatefulSet will be assigned an integer ordinal, that is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller will also add a pod label with this index: apps.kubernetes.io/pod-index.","v":"26","_id":"ba313c0a04bf781dc0dbba777523a410","_rev":"1-a869d57fe202f1b85f8200fdc9e2e66f","_revisions":{"start":1,"ids":["a869d57fe202f1b85f8200fdc9e2e66f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328034435,"range":{"startContainerPath":"//h2[@id=\"limitations\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[2]/li[5]/text()[5]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Limitations The storage for a given Pod must either be provisioned by a PersistentVolume Provisioner (examples here) based on the requested storage class, or pre-provisioned by an admin. Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources. StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for creating this Service. StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion. When using Rolling Updates with the default Pod Management Policy (OrderedReady), it's possible to get into a broken state that requires manual intervention to repair.","v":"26","_id":"bd7d2d6d981840676890f1eb3554bbc3","_rev":"1-29e5f97155a4c14f24181c2c85ae0af3","_revisions":{"start":1,"ids":["29e5f97155a4c14f24181c2c85ae0af3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738329758554,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[3]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.","v":"26","_id":"bda4707aa1dc83324e14ae48b4dfb227","_rev":"1-ad78e83f0222074386484090442cc855","_revisions":{"start":1,"ids":["ad78e83f0222074386484090442cc855"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738326121005,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div[23]/pre/code/span/span/text()","startOffset":0,"endContainerPath":"//h2[@id=\"alternatives-to-replicaset\"]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50","v":"26","_id":"c0980c7d1b07cf9a885bbf7ed3db8a6c","_rev":"1-d83d1788c1079a1ca367d0e0998f4ee5","_revisions":{"start":1,"ids":["d83d1788c1079a1ca367d0e0998f4ee5"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325457742,"range":{"startContainerPath":"//h3[@id=\"deleting-a-replicaset-and-its-pods\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[37]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Deleting a ReplicaSet and its Pods","v":"26","_id":"c113d947bdf22dd39809cddddcdfa9af","_rev":"1-9804cfe9e3734fd1e997dd84e5eb1ad0","_revisions":{"start":1,"ids":["9804cfe9e3734fd1e997dd84e5eb1ad0"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738329130710,"range":{"startContainerPath":"//h3[@id=\"stable-network-id\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[16]/text()","endOffset":107},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Stable Network ID Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod.","v":"26","_id":"c1ade4cc39d0c0a8708c33cc2a729e80","_rev":"1-9be39590abb35e26210a2ef00b8fca68","_revisions":{"start":1,"ids":["9be39590abb35e26210a2ef00b8fca68"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738334348113,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[3]/text()","endOffset":46},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Some typical uses of a DaemonSet are:  running a cluster storage daemon on every node running a logs collection daemon on every node running a node monitoring daemon on every node","v":"26","_id":"c42d65ab7893e00fa50f1b6becd62257","_rev":"1-5749bfa70c70799fed209226830a8dbf","_revisions":{"start":1,"ids":["5749bfa70c70799fed209226830a8dbf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659313922,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()[2]","endOffset":24},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The user can specify a different scheduler for the Pods of the DaemonSet, by setting the .spec.template.spec.schedulerName field of the DaemonSet.","v":"26","_id":"c76beed37739efb02cbe4bb21a3bc709","_rev":"1-634474134f4290a4aa6bf962a2a5e0c1","_revisions":{"start":1,"ids":["634474134f4290a4aa6bf962a2a5e0c1"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738316922165,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":101,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","endOffset":409},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.","v":"26","_id":"c7f01c202c253a0f5dace37f85a6b257","_rev":"1-827f5478fc29510a7e8b8d55a165f8a6","_revisions":{"start":1,"ids":["827f5478fc29510a7e8b8d55a165f8a6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328854536,"range":{"startContainerPath":"//h2[@id=\"pod-identity\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[13]/text()","endOffset":195},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Pod Identity StatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it's (re)scheduled on.","v":"26","_id":"cb34e00ae4ed7bcc368cba6833c78d80","_rev":"1-e19710261c0e9d26ad08bdd850799229","_revisions":{"start":1,"ids":["e19710261c0e9d26ad08bdd850799229"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328839188,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[12]/text()","endOffset":114},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":".spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be running","v":"26","_id":"d00cf1d333ead057e3cb59085cc83be1","_rev":"1-573f502689f7024fa65dbfa8a0014db9","_revisions":{"start":1,"ids":["573f502689f7024fa65dbfa8a0014db9"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738327885152,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","startOffset":161,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[3]/text()[2]","endOffset":308},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.","v":"26","_id":"d053f5955dc48bcd0836c002a59156df","_rev":"1-2d2de6dea308c871b61136bee4f14ab3","_revisions":{"start":1,"ids":["2d2de6dea308c871b61136bee4f14ab3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738327930331,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[5]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul/li[4]/text()","endOffset":35},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"StatefulSets are valuable for applications that require one or more of the following.  Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates.","v":"26","_id":"d16482817fd8805edca6c596d320c903","_rev":"1-405bcd78b873e89c5bba1183f5e6a075","_revisions":{"start":1,"ids":["405bcd78b873e89c5bba1183f5e6a075"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325544198,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()[6]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can delete a ReplicaSet without affecting any of its Pods using kubectl delete with the --cascade=orphan option. When using the REST API or the client-go library, you must set propagationPolicy to Orphan.","v":"26","_id":"d1bf8dcfc0cb91e41b29ddd88da12c8a","_rev":"1-e2959998e213fa575397c946344bb79c","_revisions":{"start":1,"ids":["e2959998e213fa575397c946344bb79c"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325527149,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[38]/code[4]/text()","endOffset":10},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When using the REST API or the client-go library, you must set propagationPolicy to Background or Foreground","v":"26","_id":"d22706b87c113477784928cd0caeb8d8","_rev":"1-1b4c3f416098daa22d5f56d4b14a2079","_revisions":{"start":1,"ids":["1b4c3f416098daa22d5f56d4b14a2079"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738332474503,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[40]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[41]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"The optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this field. Once enabled, there are two policies you can configure for each StatefulSet:  whenDeleted configures the volume retention behavior that applies when the StatefulSet is deleted whenScaled configures the volume retention behavior that applies when the replica count of the StatefulSet is reduced; for example, when scaling down the set. For each policy that you can configure, you can set the value to either Delete or Retain.","v":"26","_id":"d36363f10b22aa96b56bc2fb202ccb4c","_rev":"1-3733d1daf7ff39c8afc370ea0cebdb63","_revisions":{"start":1,"ids":["3733d1daf7ff39c8afc370ea0cebdb63"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738334335137,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div/text()","endOffset":180},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.","v":"26","first":true,"title":"DaemonSet | Kubernetes","_id":"d4a0fcc39d84f2d0c87771cdedb18cf8","_rev":"1-a44c01fc7bf9538e46f44497b5388197","_revisions":{"start":1,"ids":["a44c01fc7bf9538e46f44497b5388197"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330716940,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[34]/text()[2]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can control the maximum number of Pods that can be unavailable during an update by specifying the .spec.updateStrategy.rollingUpdate.maxUnavailable field.","v":"26","_id":"da9ccfa7e52299bf9d5a8a4c37b9e887","_rev":"1-9d2f6ecd6e59e36866c9639342a9f2f7","_revisions":{"start":1,"ids":["9d2f6ecd6e59e36866c9639342a9f2f7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738333494217,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[47]/text()","startOffset":16,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[47]/text()","endOffset":54},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"if the controller crashes and restarts","v":"26","_id":"dde0ffd2a244982565797e4e82cc8398","_rev":"1-f8662b121e740d3b9a84772e2a40127d","_revisions":{"start":1,"ids":["f8662b121e740d3b9a84772e2a40127d"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330837766,"range":{"startContainerPath":"//h3[@id=\"forced-rollback\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[39]/text()","endOffset":211},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Forced rollback When using Rolling Updates with the default Pod Management Policy (OrderedReady), it's possible to get into a broken state that requires manual intervention to repair.  If you update the Pod template to a configuration that never becomes Running and Ready (for example, due to a bad binary or application-level configuration error), StatefulSet will stop the rollout and wait.  In this state, it's not enough to revert the Pod template to a good configuration. Due to a known issue, StatefulSet will continue to wait for the broken Pod to become Ready (which never happens) before it will attempt to revert it back to the working configuration.  After reverting the template, you must also delete any Pods that StatefulSet had already attempted to run with the bad configuration. StatefulSet will then begin to recreate the Pods using the reverted template.","v":"26","_id":"de62581edff0b54532408686e2d28ea9","_rev":"1-71725be59cb76b9d8c156875bb3d5de6","_revisions":{"start":1,"ids":["71725be59cb76b9d8c156875bb3d5de6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738326676491,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[55]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[55]/text()[2]","endOffset":100},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Use a Job instead of a ReplicaSet for Pods that are expected to terminate on their own (that is, batch jobs).","v":"26","_id":"e2939e247d812af9ee421c9199d77c43","_rev":"1-afb24e499eede3b75a0c5729bd5fe882","_revisions":{"start":1,"ids":["afb24e499eede3b75a0c5729bd5fe882"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738326668548,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[56]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[56]/text()[2]","endOffset":119},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Use a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level function, such as machine monitoring or machine logging.","v":"26","_id":"e628bbf55bbae6d92a38e25606ebab65","_rev":"1-7ac80d2f8a8bcb016ef04128dc3af967","_revisions":{"start":1,"ids":["7ac80d2f8a8bcb016ef04128dc3af967"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328985806,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[15]/text()","endOffset":21},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":".spec.ordinals is an optional field","v":"26","_id":"e6926298c629cab7729d74c24c8ed7c6","_rev":"1-bcea2b4c496fdd1511c150c82e92efda","_revisions":{"start":1,"ids":["bcea2b4c496fdd1511c150c82e92efda"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330393654,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[29]/text()","endOffset":218},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Parallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and to not wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.","v":"26","_id":"e6a694bf7220776a90f7781571f3054d","_rev":"1-aa0d957f14adc96c92e0db6018cf7d9f","_revisions":{"start":1,"ids":["aa0d957f14adc96c92e0db6018cf7d9f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738325639124,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[43]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[44]/text()","endOffset":52},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to prioritize scaling down pods based on the following general algorithm:  Pending (and unschedulable) pods are scaled down first If controller.kubernetes.io/pod-deletion-cost annotation is set, then the pod with the lower value will come first. Pods on nodes with more replicas come before pods on nodes with fewer replicas. If the pods' creation times differ, the pod that was created more recently comes before the older pod (the creation times are bucketed on an integer log scale). If all of the above match, then selection is random.","v":"26","_id":"ebf5cc6be3bccb78fc086de62899cdb5","_rev":"1-fc204670553d6dc99b564505753b0130","_revisions":{"start":1,"ids":["fc204670553d6dc99b564505753b0130"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738332407863,"range":{"startContainerPath":"//h2[@id=\"persistentvolumeclaim-retention\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/div[10]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"PersistentVolumeClaim retention","v":"26","_id":"ecb784028fc938004d254bc03d98b92f","_rev":"1-ad3abfbce3a81ba1ceaaaa93f54b17ff","_revisions":{"start":1,"ids":["ad3abfbce3a81ba1ceaaaa93f54b17ff"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738316852009,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[4]/text()","endOffset":66},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A ReplicaSet identifies new Pods to acquire by using its selector.","v":"26","_id":"ecf6a94f1d02077e8feec581db0d7828","_rev":"1-bea16c442200006568f9ab139c4b8e71","_revisions":{"start":1,"ids":["bea16c442200006568f9ab139c4b8e71"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/","date":1738326112858,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[50]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[50]/text()[2]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA).","v":"26","_id":"ed43e4bef2446786878f71935b89faa7","_rev":"1-ba69cffc057dc870eaabb10b961e77e7","_revisions":{"start":1,"ids":["ba69cffc057dc870eaabb10b961e77e7"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738329427229,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[20]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/table/tbody/tr[3]/td[6]/text()","endOffset":12},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Here are some examples of choices for Cluster Domain, Service name, StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.  Cluster Domain Service (ns/name) StatefulSet (ns/name) StatefulSet Domain Pod DNS Pod Hostname cluster.local default/nginx default/web nginx.default.svc.cluster.local web-{0..N-1}.nginx.default.svc.cluster.local web-{0..N-1} cluster.local foo/nginx foo/web nginx.foo.svc.cluster.local web-{0..N-1}.nginx.foo.svc.cluster.local web-{0..N-1} kube.local foo/nginx foo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.local web-{0..N-1}","v":"26","_id":"ef34210840567da4ecf1c9c1fea8556e","_rev":"1-13d9a979df8b7f020fa33ec18981488f","_revisions":{"start":1,"ids":["13d9a979df8b7f020fa33ec18981488f"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738578928965,"range":{"startContainerPath":"//h3[@id=\"running-pods-on-select-nodes\"]/text()","startOffset":0,"endContainerPath":"//h3[@id=\"running-pods-on-select-nodes\"]/text()","endOffset":28},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Running Pods on select Nodes","v":"26","_id":"ef4f565f126497b67b8c656ec244e4d7","_rev":"1-08fe89da8877112890bc139fe4321fd3","_revisions":{"start":1,"ids":["08fe89da8877112890bc139fe4321fd3"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330421525,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[30]/text()[2]","endOffset":164},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"A StatefulSet's .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.","v":"26","_id":"f0c2d89a12a728b6f0fd0a437799d29f","_rev":"1-05aeb6ba0d473627bc14c4f5d68b3353","_revisions":{"start":1,"ids":["05aeb6ba0d473627bc14c4f5d68b3353"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330376629,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/code/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[28]/text()","endOffset":48},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"OrderedReady pod management is the default for StatefulSets.","v":"26","_id":"f2b3f9b9cc39f2ffd67b82c92415ad9c","_rev":"1-0c1871e4eb999e3343bc3344aa9c3937","_revisions":{"start":1,"ids":["0c1871e4eb999e3343bc3344aa9c3937"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738333514810,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[47]/text()[2]","startOffset":417,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[47]/text()[2]","endOffset":686},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"we recommend waiting for the controller to come back up, which will verify owner references before terminating Pods. If that is not possible, the operator should verify the owner references on PVCs to ensure the expected objects are deleted when Pods are force-deleted.","v":"26","_id":"f6a496a275607cd9968899d804450bb7","_rev":"1-dc5765123be759f5f1b147726ca348cc","_revisions":{"start":1,"ids":["dc5765123be759f5f1b147726ca348cc"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738330366163,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[27]/text()[2]","endOffset":7},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"StatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and identity guarantees via its .spec.podManagementPolicy field.","v":"26","_id":"f911c5024564e1fad1d8789f7df23f06","_rev":"1-ed6e7c1e8d4eea17ad1cd8b0e084cfdf","_revisions":{"start":1,"ids":["ed6e7c1e8d4eea17ad1cd8b0e084cfdf"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738578940769,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/text()","startOffset":7,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[18]/code/text()","endOffset":32},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"specify a .spec.template.spec.nodeSelector","v":"26","_id":"fd14a7568dd5c9389ba277dd800b929f","_rev":"1-23029ee15ffb0454ad086ce374fe991e","_revisions":{"start":1,"ids":["23029ee15ffb0454ad086ce374fe991e"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738328801173,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[11]/text()[3]","endOffset":1},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"You can set the .spec.volumeClaimTemplates field to create a PersistentVolumeClaim.","v":"26","_id":"fdd38a6716ebd98fff8e166a0d32a805","_rev":"1-b77a72e4a33de4cf6d7b92520351d839","_revisions":{"start":1,"ids":["b77a72e4a33de4cf6d7b92520351d839"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738327028833,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/p/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/p[2]","endOffset":0},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"StatefulSet is the workload API object used to manage stateful applications.","v":"26","first":true,"title":"StatefulSets | Kubernetes","_id":"fe49c4248f1457ce3ba37462c6d8cb3e","_rev":"1-9640df8f2a6a526a2c7db32c432bd8e6","_revisions":{"start":1,"ids":["9640df8f2a6a526a2c7db32c432bd8e6"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","date":1738329831999,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[3]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[7]/li[4]/text()","endOffset":78},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready. Before a Pod is terminated, all of its successors must be completely shutdown.","v":"26","_id":"fe8c2e85ba832fb375f5fda8328a2c0c","_rev":"1-38a9a676940fa96ca97b633dbf0a22ec","_revisions":{"start":1,"ids":["38a9a676940fa96ca97b633dbf0a22ec"]}}]}

{"seq":711}

{"docs":[{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659630919,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[4]/text()","endOffset":102},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Service: Create a service with the same Pod selector, and use the service to reach a daemon on a random node.","v":"26","_id":"df9ade9d102db245751ea12af7b16ae0","_rev":"1-34931c4043e5d84e3664334169a2c887","_revisions":{"start":1,"ids":["34931c4043e5d84e3664334169a2c887"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659603541,"range":{"startContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/strong/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[3]/text()[3]","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"DNS: Create a headless service with the same pod selector, and then discover DaemonSets using the endpoints resource or retrieve multiple A records from DNS.","v":"26","_id":"e70058c4b4e16c51b172d629eeed89e6","_rev":"1-f396a53ab7e8bc250cf51f54175b3b55","_revisions":{"start":1,"ids":["f396a53ab7e8bc250cf51f54175b3b55"]}},{"verb":"create","match":"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","date":1738659592950,"range":{"startContainerPath":"//h2[@id=\"communicating-with-daemon-pods\"]/text()","startOffset":0,"endContainerPath":"//div[@id=\"maindoc\"]/main/div/ul[3]/li[2]/text()[2]","endOffset":50},"className":"default-yellow-aaddcf5c-0e41-4f83-8a64-58c91f7c6250","text":"Communicating with Daemon Pods Some possible patterns for communicating with Pods in a DaemonSet are:  Push: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have clients. NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that the pods are reachable via the node IPs.","v":"26","_id":"f4794b40ed1418f7cc562bb9440342e3","_rev":"1-3589a944c665382835149b5e83f63246","_revisions":{"start":1,"ids":["3589a944c665382835149b5e83f63246"]}}]}

{"seq":714}
