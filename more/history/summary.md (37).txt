# [Getting started | Kubernetes](https://kubernetes.io/docs/setup/)

## Yellow

- Several Kubernetes components such as kube-apiserver or kube-proxy can also be deployed as container images within the cluster.

# [Production environment | Kubernetes](https://kubernetes.io/docs/setup/production-environment/)

## Yellow

- Creating a highly available cluster means considering: Separating the control plane from the worker nodes. Replicating the control plane components on multiple nodes. Load balancing traffic to the cluster’s API server. Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.
- Before building a Kubernetes production environment on your own, consider handing off some or all of this job to Turnkey Cloud Solutions providers or other Kubernetes Partners. Options include: Serverless: Just run workloads on third-party equipment without managing a cluster at all. You will be charged for things like CPU usage, memory, and disk requests. Managed control plane: Let the provider manage the scale and availability of the cluster's control plane, as well as handle patches and upgrades. Managed worker nodes: Configure pools of nodes to meet your needs, then the provider makes sure those nodes are available and ready to implement upgrades when needed. Integration: There are providers that integrate Kubernetes with other services you may need, such as storage, container registries, authentication methods, and development tools.
- Production control plane
- Manage certificates
- Configure load balancer for apiserver
- Separate and backup etcd service
- Create multiple control plane systems
- running control plane services as pods in Kubernetes ensures that the replicated number of services that you request will always be available. The scheduler should be fault tolerant, but not highly available. Some deployment tools set up Raft consensus algorithm to do leader election of Kubernetes services. If the primary goes away, another service elects itself and take over.
- Span multiple zones: If keeping your cluster available at all times is critical, consider creating a cluster that runs across multiple data centers, referred to as zones in cloud environments. Groups of zones are referred to as regions. By spreading a cluster across multiple zones in the same region, it can improve the chances that your cluster will continue to function even if one zone becomes unavailable.
- Production worker nodes
- Configure nodes: Nodes can be physical or virtual machines. If you want to create and manage your own nodes, you can install a supported operating system, then add and run the appropriate Node services. Consider: The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available. Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation. Validate nodes: See Valid node setup for information on how to ensure that a node meets the requirements to join a Kubernetes cluster. Add nodes to the cluster: If you are managing your own cluster you can add nodes by setting up your own machines and either adding them manually or having them register themselves to the cluster’s apiserver. See the Nodes section for information on how to set up Kubernetes to add nodes in these ways. Scale nodes: Have a plan for expanding the capacity your cluster will eventually need. See Considerations for large clusters to help determine how many nodes you need, based on the number of pods and containers you need to run. If you are managing nodes yourself, this can mean purchasing and installing your own physical equipment. Autoscale nodes: Read Cluster Autoscaling to learn about the tools available to automatically manage your nodes and the capacity they provide. Set up node health checks: For important workloads, you want to make sure that the nodes and pods running on those nodes are healthy. Using the Node Problem Detector daemon, you can ensure your nodes are healthy.
- Taking on a production-quality cluster means deciding how you want to selectively allow access by other users. In particular, you need to select strategies for validating the identities of those who try to access your cluster (authentication) and deciding if they have permissions to do what they are asking (authorization):
- Authentication: The apiserver can authenticate users using client certificates, bearer tokens, an authenticating proxy, or HTTP basic auth.
- Using plugins, the apiserver can leverage your organization’s existing authentication methods, such as LDAP or Kerberos.
- Authorization: When you set out to authorize your regular users, you will probably choose between RBAC and ABAC authorization.
- Role-based access control (RBAC): Lets you assign access to your cluster by allowing specific sets of permissions to authenticated users. Permissions can be assigned for a specific namespace (Role) or across the entire cluster (ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached to particular users.
- Attribute-based access control (ABAC): Lets you create policies based on resource attributes in the cluster and will allow or deny access based on those attributes. Each line of a policy file identifies versioning properties (apiVersion and kind) and a map of spec properties to match the subject (user or group), resource property, non-resource property (/version or /apis), and readonly.
- Set the authorization mode: When the Kubernetes API server (kube-apiserver) starts, supported authorization modes must be set using an --authorization-config file or the --authorization-mode flag.
- For example, that flag in the kube-adminserver.yaml file (in /etc/kubernetes/manifests) could be set to Node,RBAC.
- Webhooks and other special authorization types need to be enabled by adding Admission Controllers to the API server.
- Set namespace limits: Set per-namespace quotas on things like memory and CPU.
- Prepare for DNS demand
- Create additional service accounts: User accounts determine what users can do on a cluster, while a service account defines pod access within a particular namespace. By default, a pod takes on the default service account from its namespace.

# [Container Runtimes | Kubernetes](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

## Yellow

- You need to install a container runtime into each node in the cluster so that Pods can run there.

## Orange

- Network configuration

## Yellow

- By default, the Linux kernel does not allow IPv4 packets to be routed between interfaces. Most Kubernetes cluster networking implementations will change this setting

## Orange

- cgroup drivers

## Yellow

- Both the kubelet and the underlying container runtime need to interface with control groups to enforce resource management for pods and containers and set resources such as cpu/memory requests and limits. To interface with control groups, the kubelet and the container runtime need to use a cgroup driver. It's critical that the kubelet and the container runtime use the same cgroup driver and are configured the same. There are two cgroup drivers available: cgroupfs systemd
- Two cgroup managers result in two views of the available and in-use resources in the system.

# [Customizing components with the kubeadm API | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)

## Orange

- Customizing components with the kubeadm API

## Yellow

- Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually patch the kube-system/coredns ConfigMap and recreate the CoreDNS Pods after that.

# [Options for Highly Available Topology | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)

## Yellow

- You can set up an HA cluster: With stacked control plane nodes, where etcd nodes are colocated with control plane nodes With external etcd nodes, where etcd runs on separate nodes from the control plane

## Orange

- Stacked etcd topology

## Yellow

- Each control plane node creates a local etcd member and this etcd member communicates only with the kube-apiserver of this node.
- You should therefore run a minimum of three stacked control plane nodes for an HA cluster.

## Orange

- External etcd topology

## Yellow

- the kube-apiserver is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the kube-apiserver of each control plane node.
- However, this topology requires twice the number of hosts as the stacked HA topology. A minimum of three hosts for control plane nodes and three hosts for etcd nodes

# [Configuring each kubelet in your cluster using kubeadm | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/)

## Yellow

- The lifecycle of the kubeadm CLI tool is decoupled from the kubelet, which is a daemon that runs on each node within the Kubernetes cluster.
- You can manage the configuration of your kubelets manually, but kubeadm now provides a KubeletConfiguration API

## Orange

- Propagating cluster-level configuration to each kubelet

## Yellow

- You can provide the kubelet with default values to be used by kubeadm init and kubeadm join commands.
- If you want your services to use the subnet 10.96.0.0/12 as the default for services, you can pass the --service-cidr parameter to kubeadm: kubeadm init --service-cidr 10.96.0.0/12
- You also need to set the DNS address used by the kubelet, using the --cluster-dns flag.
- This setting needs to be the same for every kubelet on every manager and Node in the cluster.
- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10
- When you call kubeadm init, the kubelet configuration is marshalled to disk
- After marshalling these two files to disk, kubeadm attempts to run the following two commands, if you are using systemd: systemctl daemon-reload && systemctl restart kubelet
- Kubernetes binaries and package contents The DEB and RPM packages shipped with the Kubernetes releases are: Package name Description kubeadm Installs the /usr/bin/kubeadm CLI tool and the kubelet drop-in file for the kubelet. kubelet Installs the /usr/bin/kubelet binary. kubectl Installs the /usr/bin/kubectl binary. cri-tools Installs the /usr/bin/crictl binary from the cri-tools git repository. kubernetes-cni Installs the /opt/cni/bin binaries from the plugins git repository.

# [Considerations for large clusters | Kubernetes](https://kubernetes.io/docs/setup/best-practices/cluster-large/)

## Yellow

- A cluster is a set of nodes (physical or virtual machines) running Kubernetes agents, managed by the control plane. Kubernetes v1.32 supports clusters with up to 5,000 nodes. More specifically, Kubernetes is designed to accommodate configurations that meet all of the following criteria: No more than 110 pods per node No more than 5,000 nodes No more than 150,000 total pods No more than 300,000 total containers
- For a large cluster, you need a control plane with sufficient compute and other resources. Typically you would run one or two control plane instances per failure zone, scaling those instances vertically first and then scaling horizontally after reaching the point of falling returns to (vertical) scale. You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes nodes do not automatically steer traffic towards control-plane endpoints that are in the same failure zone; however, your cloud provider might have its own mechanisms to do this. For example, using a managed load balancer, you configure the load balancer to send traffic that originates from the kubelet and Pods in failure zone A, and direct that traffic only to the control plane hosts that are also in zone A. If a single control-plane host or endpoint failure zone A goes offline, that means that all the control-plane traffic for nodes in zone A is now being sent between zones. Running multiple control plane hosts in each zone makes that outcome less likely.
- To improve performance of large clusters, you can store Event objects in a separate dedicated etcd instance. When creating a cluster, you can (using custom tooling): start and configure additional etcd instance configure the API server to use it for storing events
- To avoid running into cluster addon resource issues, when creating a cluster with many nodes, consider the following
- Some addons scale vertically
- Many addons scale horizontally
- Some addons run as one copy per node, controlled by a DaemonSet

# [Running in multiple zones | Kubernetes](https://kubernetes.io/docs/setup/best-practices/multiple-zones/)

## Yellow

- Kubernetes is designed so that a single Kubernetes cluster can run across multiple failure zones, typically where these zones fit within a logical grouping called a region. Major cloud providers define a region as a set of failure zones (also called availability zones) that provide a consistent set of features: within a region, each zone offers the same APIs and services. Typical cloud architectures aim to minimize the chance that a failure in one zone also impairs services in another zone.
- All control plane components support running as a pool of interchangeable resources, replicated per component. When you deploy a cluster control plane, place replicas of control plane components across multiple failure zones. If availability is an important concern, select at least three failure zones and replicate each individual control plane component (API server, scheduler, etcd, cluster controller manager) across at least three failure zones. If you are running a cloud controller manager then you should also replicate this across all the failure zones you selected.
- Kubernetes automatically spreads the Pods for workload resources (such as Deployment or StatefulSet) across different nodes in a cluster. This spreading helps reduce the impact of failures. When nodes start up, the kubelet on each node automatically adds labels to the Node object that represents that specific kubelet in the Kubernetes API. These labels can include zone information.
- If your cluster spans multiple zones or regions, you can use node labels in conjunction with Pod topology spread constraints to control how Pods are spread across your cluster among fault domains
- Distributing nodes across zones Kubernetes' core does not create nodes for you; you need to do that yourself, or use a tool such as the Cluster API to manage nodes on your behalf. Using tools such as the Cluster API you can define sets of machines to run as worker nodes for your cluster across multiple failure domains, and rules to automatically heal the cluster in case of whole-zone service disruption.
- When persistent volumes are created, Kubernetes automatically adds zone labels to any PersistentVolumes that are linked to a specific zone. The scheduler then ensures, through its NoVolumeZoneConflict predicate, that pods which claim a given PersistentVolume are only placed into the same zone as that volume.
- By itself, Kubernetes does not include zone-aware networking. You can use a network plugin to configure cluster networking, and that network solution might have zone-specific elements. For example, if your cloud provider supports Services with type=LoadBalancer, the load balancer might only send traffic to Pods running in the same zone as the load balancer element processing a given connection

# [Validate node setup | Kubernetes](https://kubernetes.io/docs/setup/best-practices/node-conformance/)

## Yellow

- Node conformance test is a containerized test framework that provides a system verification and functionality test for a node. The test validates whether the node meets the minimum requirements for Kubernetes; a node that passes the test is qualified to join a Kubernetes cluster.
- At a minimum, the node should have the following daemons installed: CRI-compatible container runtimes such as Docker, containerd and CRI-O kubelet
- To run the node conformance test, perform the following steps: Work out the value of the --kubeconfig option for the kubelet; for example: --kubeconfig=/var/lib/kubelet/config.yaml. Because the test framework starts a local control plane to test the kubelet, use http://localhost:8080 as the URL of the API server. There are some other kubelet command line parameters you may want to use: --cloud-provider: If you are using --cloud-provider=gce, you should remove the flag to run the test. Run the node conformance test with command: # $CONFIG_DIR is the pod manifest path of your kubelet. # $LOG_DIR is the test output path. sudo docker run -it --rm --privileged --net=host \ -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \ registry.k8s.io/node-test:0.2
- Kubernetes also provides node conformance test docker images for other architectures: Arch Image amd64 node-test-amd64 arm node-test-arm arm64 node-test-arm64

# [Enforcing Pod Security Standards | Kubernetes](https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/)

## Yellow

- The Pod Security Admission Controller intends to replace the deprecated PodSecurityPolicies.
- Namespaces that lack any configuration at all should be considered significant gaps in your cluster security model. We recommend taking the time to analyze the types of workloads occurring in each namespace, and by referencing the Pod Security Standards, decide on an appropriate level for each of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.
- For workloads running in those permissive namespaces, maintain documentation about their unique security requirements. If at all possible, consider how those requirements could be further constrained.
- The audit and warn modes of the Pod Security Standards admission controller make it easy to collect important security insights about your pods without breaking existing workloads.
- If you expect workload authors to make changes to fit within the desired level, enable the warn mode. If you expect to use audit logs to monitor/drive changes to fit within the desired level, enable the audit mode.
- setting them to the desired level and version you would eventually like to enforce.
- Other alternatives for enforcing security profiles are being developed in the Kubernetes ecosystem: Kubewarden. Kyverno. OPA Gatekeeper.

# [PKI certificates and requirements | Kubernetes](https://kubernetes.io/docs/setup/best-practices/certificates/)

## Yellow

- Kubernetes requires PKI certificates for authentication over TLS.
- Kubernetes requires PKI for the following operations: Server certificates Server certificate for the API server endpoint Server certificate for the etcd server Server certificates for each kubelet (every node runs a kubelet) Optional server certificate for the front-proxy Client certificates Client certificates for each kubelet, used to authenticate to the API server as a client of the Kubernetes API Client certificate for each API server, used to authenticate to etcd Client certificate for the controller manager to securely communicate with the API server Client certificate for the scheduler to securely communicate with the API server Client certificates, one for each node, for kube-proxy to authenticate to the API server Optional client certificates for administrators of the cluster to authenticate to the API server Optional client certificate for the front-proxy Kubelet's server and client certificates To establish a secure connection and authenticate itself to the kubelet, the API Server requires a client certificate and key pair. In this scenario, there are two approaches for certificate usage: Shared Certificates: The kube-apiserver can utilize the same certificate and key pair it uses to authenticate its clients. This means that the existing certificates, such as apiserver.crt and apiserver.key, can be used for communicating with the kubelet servers. Separate Certificates: Alternatively, the kube-apiserver can generate a new client certificate and key pair to authenticate its communication with the kubelet servers. In this case, a distinct certificate named kubelet-client.crt and its corresponding private key, kubelet-client.key are created.
- most certificates are stored in /etc/kubernetes/pki
- exception of user account certificates which kubeadm places in /etc/kubernetes

# [Overview | Kubernetes](https://kubernetes.io/docs/concepts/overview/)

## Yellow

- Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.
- The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the "K" and the "s". Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production workloads at scale with best-of-breed ideas and practices from the community.
- However, Kubernetes is not monolithic, and these default solutions are optional and pluggable.
- Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.

# [Kubernetes Components | Kubernetes](https://kubernetes.io/docs/concepts/overview/components/)

## Yellow

- A Kubernetes cluster consists of a control plane and one or more worker nodes. Here's a brief overview of the main components:
- Control Plane Components Manage the overall state of the cluster: kube-apiserver The core component server that exposes the Kubernetes HTTP API etcd Consistent and highly-available key value store for all API server data kube-scheduler Looks for Pods not yet bound to a node, and assigns each Pod to a suitable node. kube-controller-manager Runs controllers to implement Kubernetes API behavior. cloud-controller-manager (optional) Integrates with underlying cloud provider(s).
- Node Components Run on every node, maintaining running pods and providing the Kubernetes runtime environment: kubelet Ensures that Pods are running, including their containers. kube-proxy (optional) Maintains network rules on nodes to implement Services. Container runtime Software responsible for running containers.
- Addons Addons extend the functionality of Kubernetes. A few important examples include: DNS For cluster-wide DNS resolution Web UI (Dashboard) For cluster management via a web interface Container Resource Monitoring For collecting and storing container metrics Cluster-level Logging For saving container logs to a central log store

# [Objects In Kubernetes | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/)

## Yellow

- Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance
- A Kubernetes object is a "record of intent"--once you create the object, the Kubernetes system will constantly work to ensure that the object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to look like; this is your cluster's desired state. To work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you.
- Object spec and status Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the resource to have: its desired state. The status describes the current state of the object, supplied and updated by the Kubernetes system and its components. The Kubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied.
- When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via kubectl), that API request must include that information as JSON in the request body.
- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80
- One way to create a Deployment using a manifest file like the one above is to use the kubectl apply command in the kubectl command-line interface, passing the .yaml file as an argument. Here's an example: kubectl apply -f https://k8s.io/examples/application/deployment.yaml The output is similar to this: deployment.apps/nginx-deployment created
- Required fields
- apiVersion - Which version of the Kubernetes API you're using to create this object kind - What kind of object you want to create metadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespace spec - What state you desire for the object
- The kubectl tool uses the --validate flag to set the level of field validation. It accepts the values ignore, warn, and strict while also accepting the values true (equivalent to strict) and false (equivalent to ignore). The default validation setting for kubectl is --validate=true.
- Kubernetes 1.27 and later versions always offer field validation; older Kubernetes releases might not.

# [Kubernetes Object Management | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/)

## Yellow

- Warning: A Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object results in undefined behavior.
- Imperative commands When using imperative commands, a user operates directly on live objects in a cluster.
- it provides no history of previous configurations.
- kubectl create deployment nginx --image nginx
- Imperative object configuration In imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one file name. The file specified must contain a full definition of the object in YAML or JSON format.
- kubectl create -f nginx.yaml
- Declarative object configuration When using declarative object configuration, a user operates on object configuration files stored locally, however the user does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by kubectl. This enables working on directories, where different operations might be needed for different objects.
- This is possible by using the patch API operation to write only observed differences, instead of using the replace API operation to replace the entire object configuration.
- kubectl diff -f configs/ kubectl apply -f configs/ Recursively process directories: kubectl diff -R -f configs/ kubectl apply -R -f configs/

# [Object Names and IDs | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/)

## Yellow

- Each object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique across your whole cluster.
- For example, you can only have one Pod named myapp-1234 within the same namespace, but you can have one Pod and one Deployment that are each named myapp-1234. For non-unique user-provided attributes, Kubernetes provides labels and annotations.
- Names A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.
- Names must be unique across all API versions of the same resource. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.
- In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.
- The server may generate a name when generateName is provided instead of name in a resource create request.
- it may conflict with existing names resulting in a HTTP 409 response. This became far less likely to happen in Kubernetes v1.31 and later, since the server will make up to 8 attempt to generate a unique name before returning a HTTP 409 response.
- DNS Subdomain Names Most resource types require a name that can be used as a DNS subdomain name as defined in RFC 1123. This means the name must: contain no more than 253 characters contain only lowercase alphanumeric characters, '-' or '.' start with an alphanumeric character end with an alphanumeric character
- Some resource types require their names to be able to be safely encoded as a path segment.
- UIDs A Kubernetes systems-generated string to uniquely identify objects.
- standardized as ISO/IEC 9834-8 and as ITU-T X.667.

# [Labels and Selectors | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

## Yellow

- Labels are key/value pairs that are attached to objects
- Each Key must be unique for a given object.
- Labels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-identifying information should be recorded using annotations.
- Example labels: "release" : "stable", "release" : "canary" "environment" : "dev", "environment" : "qa", "environment" : "production" "tier" : "frontend", "tier" : "backend", "tier" : "cache" "partition" : "customerA", "partition" : "customerB" "track" : "daily", "track" : "weekly"
- Valid label keys have two segments: an optional prefix and name, separated by a slash (/).
- The name segment is required and must be 63 characters or less
- The prefix is optional. If specified, the prefix must be a DNS subdomain
- If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add labels to end-user objects must specify a prefix. The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.
- we expect many objects to carry the same label(s). Via a label selector, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.
- The API currently supports two types of selectors: equality-based and set-based.
- Equality- or inequality-based requirements allow filtering by label keys and values.
- environment = production tier != frontend
- Set-based label requirements allow filtering keys according to a set of values.
- environment in (production, qa) tier notin (frontend, backend) partition !partition
- LIST and WATCH filtering For list and watch operations, you can specify label selectors to filter the sets of objects returned; you specify the filter using a query parameter.
- equality-based requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend set-based requirements: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29
- equality-based one may write: kubectl get pods -l environment=production,tier=frontend or using set-based requirements: kubectl get pods -l 'environment in (production),tier in (frontend)'
- set-based requirements are more expressive. For instance, they can implement the OR operator on values: kubectl get pods -l 'environment in (production, qa)' or restricting negative matching via notin operator: kubectl get pods -l 'environment,environment notin (frontend)'
- Some Kubernetes objects, such as services and replicationcontrollers, also use label selectors to specify sets of other resources, such as pods.
- only equality-based requirement selectors are supported
- selector: component: redis
- Newer resources, such as Job, Deployment, ReplicaSet, and DaemonSet, support set-based requirements as well. selector: matchLabels: component: redis matchExpressions: - { key: tier, operator: In, values: [cache] } - { key: environment, operator: NotIn, values: [dev] }
- matchExpressions is a list of pod selector requirements.
- Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn.
- One use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule
- The labels allow for slicing and dicing the resources along any dimension specified by a label:
- kubectl get pods -Lapp -Ltier -Lrole
- Updating labels
- if you want to label all your NGINX Pods as frontend tier, run: kubectl label pods -l app=nginx tier=fe

# [Namespaces | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)

## Yellow

- namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.).
- For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.
- Namespaces provide a scope for names.
- Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.
- Namespaces are a way to divide cluster resources between multiple users (via resource quota).
- It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.
- Kubernetes starts with four initial namespaces: default Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace. kube-node-lease This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. kube-public This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-system The namespace for objects created by the Kubernetes system.
- kubectl run nginx --image=nginx --namespace=<insert-namespace-name-here> kubectl get pods --namespace=<insert-namespace-name-here>
- kubectl config set-context --current --namespace=<insert-namespace-name-here> # Validate it kubectl config view --minify | grep namespace:
- When you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespace-name>.svc.cluster.local, which means that if a container only uses <service-name>, it will resolve to the service which is local to a namespace.
- If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).
- By creating namespaces with the same name as public top-level domains, Services in these namespaces can have short DNS names that overlap with public DNS records.
- configure third-party security controls, such as admission webhooks, to block creating any namespace with the name of public TLDs.
- # In a namespace kubectl api-resources --namespaced=true # Not in a namespace kubectl api-resources --namespaced=false

# [Annotations | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)

## Yellow

- You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects.
- You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects.
- The keys and the values in the map must be strings.
- Valid annotation keys have two segments: an optional prefix and name

# [Field Selectors | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/)

## Yellow

- Field selectors let you select Kubernetes objects based on the value of one or more resource fields.
- kubectl get pods --field-selector status.phase=Running
- Supported field selectors vary by Kubernetes resource type. All resource types support the metadata.name and metadata.namespace fields. Using unsupported field selectors produces an error. For example: kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find "ingresses" that match label selector "", field selector "foo.bar=baz": "foo.bar" is not a known field selector: only "metadata.name", "metadata.namespace"
- List of supported fields Kind Fields Pod spec.nodeName spec.restartPolicy spec.schedulerName spec.serviceAccountName spec.hostNetwork status.phase status.podIP status.nominatedNodeName Event involvedObject.kind involvedObject.namespace involvedObject.name involvedObject.uid involvedObject.apiVersion involvedObject.resourceVersion involvedObject.fieldPath reason reportingComponent source type Secret type Namespace status.phase ReplicaSet status.replicas ReplicationController status.replicas Job status.successful Node spec.unschedulable CertificateSigningRequest spec.signerName

# [Finalizers | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/)

## Yellow

- Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned. When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating .metadata.deletionTimestamp, and returns a 202 status code (HTTP "Accepted"). The target object remains in a terminating state while the control plane, or other components, take the actions defined by the finalizers. After these actions are complete, the controller removes the relevant finalizers from the target object. When the metadata.finalizers field is empty, Kubernetes considers the deletion complete and deletes the object. You can use finalizers to control garbage collection of resources. For example, you can define a finalizer to clean up related resources or infrastructure before the controller deletes the target resource. You can use finalizers to control garbage collection of objects by alerting controllers to perform specific cleanup tasks before deleting the target resource. Finalizers don't usually specify the code to execute. Instead, they are typically lists of keys on a specific resource similar to annotations. Kubernetes specifies some finalizers automatically, but you can also specify your own.
- How finalizers work When you create a resource using a manifest file, you can specify finalizers in the metadata.finalizers field. When you attempt to delete the resource, the API server handling the delete request notices the values in the finalizers field and does the following: Modifies the object to add a metadata.deletionTimestamp field with the time you started the deletion. Prevents the object from being removed until all items are removed from its metadata.finalizers field Returns a 202 status code (HTTP "Accepted")
- When the finalizers field is emptied, an object with a deletionTimestamp field set is automatically deleted.
- A common example of a finalizer is kubernetes.io/pv-protection, which prevents accidental deletion of PersistentVolume objects. When a PersistentVolume object is in use by a Pod, Kubernetes adds the pv-protection finalizer. If you try to delete the PersistentVolume, it enters a Terminating status, but the controller can't delete it because the finalizer exists. When the Pod stops using the PersistentVolume, Kubernetes clears the pv-protection finalizer, and the controller deletes the volume.
- when a Job creates one or more Pods, the Job controller applies labels to those pods and tracks changes to any Pods in the cluster with the same label. The Job controller also adds owner references to those Pods, pointing at the Job that created the Pods. If you delete the Job while these Pods are running, Kubernetes uses the owner references (not labels) to determine which Pods in the cluster need cleanup.

# [Owners and Dependents | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/)

## Yellow

- Owner references help different parts of Kubernetes avoid interfering with objects they don’t control.
- Dependent objects have a metadata.ownerReferences field that references their owner object. A valid owner reference consists of the object name and a UID within the same namespace as the dependent object.
- Dependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object.
- Kubernetes also adds finalizers to an owner resource when you use either foreground or orphan cascading deletion. In foreground deletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true before it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds the orphan finalizer so that the controller ignores dependent resources after it deletes the owner object.

# [Recommended Labels | Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)

## Yellow

- In addition to supporting tooling, the recommended labels describe applications in a way that can be queried.
- The metadata is organized around the concept of an application.
- The definition of what an application contains is loose.
- Shared labels and annotations share a common prefix: app.kubernetes.io. Labels without a prefix are private to users.
- In order to take full advantage of using these labels, they should be applied on every resource object. Key Description Example Type app.kubernetes.io/name The name of the application mysql string app.kubernetes.io/instance A unique name identifying the instance of an application mysql-abcxyz string app.kubernetes.io/version The current version of the application (e.g., a SemVer 1.0, revision hash, etc.) 5.7.21 string app.kubernetes.io/component The component within the architecture database string app.kubernetes.io/part-of The name of a higher level application this one is part of wordpress string app.kubernetes.io/managed-by The tool being used to manage the operation of an application Helm string To illustrate these labels in action, consider the following StatefulSet object: # This is an excerpt apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxyz app.kubernetes.io/version: "5.7.21" app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress app.kubernetes.io/managed-by: Helm

# [The Kubernetes API | Kubernetes](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)

## Yellow

- The core of Kubernetes' control plane is the API server. The API server exposes an HTTP API that lets end users, different parts of your cluster, and external components communicate with one another.
- The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes
- Most operations can be performed through the kubectl command-line interface or other command-line tools, such as kubeadm, which in turn use the API. However, you can also access the API directly using REST calls.
- The two supported mechanisms are as follows: The Discovery API provides information about the Kubernetes APIs
- The Kubernetes OpenAPI Document provides (full) OpenAPI v2.0 and 3.0 schemas for all Kubernetes API endpoints.
- Kubernetes implements an alternative Protobuf based serialization format that is primarily intended for intra-cluster communication.
- Kubernetes stores the serialized state of objects by writing them into etcd.
- API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.
- The Kubernetes API can be extended in one of two ways: Custom resources let you declaratively define how the API server should provide your chosen resource API. You can also extend the Kubernetes API by implementing an aggregation layer.

# [Cluster Architecture | Kubernetes](https://kubernetes.io/docs/concepts/architecture/)

## Yellow

- A Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes, that run containerized applications.
- The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.
- The control plane's components make global decisions about the cluster
- as well as detecting and responding to cluster events (for example, starting up a new pod when a Deployment's replicas field is unsatisfied).
- Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control plane components on the same machine, and do not run user containers on this machine.
- kube-apiserver The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.
- etcd Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data. You can find in-depth information about etcd in the official https://etcd.io/docs/documentation.
- kube-scheduler Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.
- kube-controller-manager Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. There are many different types of controllers. Some examples of them are: Node controller: Responsible for noticing and responding when nodes go down. Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion. EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods). ServiceAccount controller: Create default ServiceAccounts for new namespaces. The above is not an exhaustive list.
- cloud-controller-manager A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.
- The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.
- Node components Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.
- kubelet An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
- kube-proxy (optional) kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.
- kube-proxy maintains network rules on nodes.
- kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.
- Container runtime A fundamental component that empowers Kubernetes to run containers effectively.
- Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).
- Addons Addons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace. Selected addons are described below
- DNS
- While the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it. Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches.
- Web UI (Dashboard)
- Container resource monitoring
- Cluster-level Logging
- Network plugins
- Architecture variations
- Control plane deployment options The control plane components can be deployed in several ways: Traditional deployment Control plane components run directly on dedicated machines or VMs, often managed as systemd services. Static Pods Control plane components are deployed as static Pods, managed by the kubelet on specific nodes. This is a common approach used by tools like kubeadm. Self-hosted The control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments and StatefulSets or other Kubernetes primitives. Managed Kubernetes services Cloud providers often abstract away the control plane, managing its components as part of their service offering.
- Customization and extensibility Kubernetes architecture allows for significant customization: Custom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely. API servers can be extended with CustomResourceDefinitions and API Aggregation. Cloud providers can integrate deeply with Kubernetes using the cloud-controller-manager. The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as operational complexity, performance, and management overhead.

# [Nodes | Kubernetes](https://kubernetes.io/docs/concepts/architecture/nodes/)

## Yellow

- Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.
- The components on a node include the kubelet, a container runtime, and the kube-proxy.
- There are two main ways to have Nodes added to the API server: The kubelet on a node self-registers to the control plane You (or another human user) manually add a Node object
- Kubernetes creates a Node object internally (the representation). Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node.
- If the node is healthy
- then it is eligible to run a Pod.
- The name of a Node object must be a valid DNS subdomain name.
- Two Nodes cannot have the same name at the same time. Kubernetes also assumes that a resource with the same name is the same object.
- Self-registration of Nodes When the kubelet flag --register-node is true (the default), the kubelet will attempt to register itself with the API server. This is the preferred pattern, used by most distros. For self-registration, the kubelet is started with the following options: --kubeconfig - Path to credentials to authenticate itself to the API server. --cloud-provider - How to talk to a cloud provider to read metadata about itself. --register-node - Automatically register with the API server. --register-with-taints - Register the node with the given list of taints (comma separated <key>=<value>:<effect>). No-op if register-node is false. --node-ip - Optional comma-separated list of the IP addresses for the node.
- you set this value to be the IPv4 address that the kubelet should use for the node.
- --node-labels - Labels to add when registering the node in the cluster
- --node-status-update-frequency - Specifies how often kubelet posts its node status to the API server.
- when Node configuration needs to be updated, it is a good practice to re-register the node with the API server. For example, if the kubelet is being restarted with a new set of --node-labels, but the same Node name is used, the change will not take effect, as labels are only set (or modified) upon Node registration with the API server.
- Node re-registration ensures all Pods will be drained and properly re-scheduled.
- Manual Node administration You can create and modify Node objects using kubectl. When you want to create Node objects manually, set the kubelet flag --register-node=false. You can modify Node objects regardless of the setting of --register-node. For example, you can set labels on an existing Node or mark it unschedulable. You can use labels on Nodes in conjunction with node selectors on Pods to control scheduling. For example, you can constrain a Pod to only be eligible to run on a subset of the available nodes. Marking a node as unschedulable prevents the scheduler from placing new pods onto that Node but does not affect existing Pods on the Node. This is useful as a preparatory step before a node reboot or other maintenance. To mark a Node unschedulable, run: kubectl cordon $NODENAME See (https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)Safely Drain a Node for more details.
- Note: Pods that are part of a DaemonSet tolerate being run on an unschedulable Node.
- For nodes there are two forms of heartbeats: Updates to the .status of a Node. Lease objects within the kube-node-lease namespace. Each Node has an associated Lease object.
- Node controller
- The node controller has multiple roles in a node's life. The first is assigning a CIDR block to the node when it is registered (if CIDR assignment is turned on).
- The second is keeping the node controller's internal list of nodes up to date with the cloud provider's list of available machines.
- the node controller asks the cloud provider if the VM for that node is still available. If not, the node controller deletes the node from its list of nodes.
- The third is monitoring the nodes' health.
- By default, the node controller waits 5 minutes between marking the node as Unknown and submitting the first eviction request.
- By default, the node controller checks the state of each node every 5 seconds. This period can be configured using the --node-monitor-period flag on the kube-controller-manager component.
- In most cases, the node controller limits the eviction rate to --node-eviction-rate (default 0.1) per second, meaning it won't evict pods from more than 1 node per 10 seconds.
- The node eviction behavior changes when a node in a given availability zone becomes unhealthy.
- If the fraction of unhealthy nodes is at least --unhealthy-zone-threshold (default 0.55), then the eviction rate is reduced. If the cluster is small (i.e. has less than or equal to --large-cluster-size-threshold nodes - default 50), then evictions are stopped. Otherwise, the eviction rate is reduced to --secondary-node-eviction-rate (default 0.01) per second.
- The corner case is when all zones are completely unhealthy
- the node controller assumes that there is some problem with connectivity between the control plane and the nodes, and doesn't perform any evictions.
- The node controller also adds taints corresponding to node problems like node unreachable or not ready. This means that the scheduler won't place Pods onto unhealthy nodes.
- Resource capacity tracking Node objects track information about the Node's resource capacity: for example, the amount of memory available and the number of CPUs. Nodes that self register report their capacity during registration. If you manually add a Node, then you need to set the node's capacity information when you add it.
- Swap memory management
- To enable swap on a node, the NodeSwap feature gate must be enabled on the kubelet (default is true), and the --fail-swap-on command line flag or failSwapOn configuration setting must be set to false. To allow Pods to utilize swap, swapBehavior should not be set to NoSwap (which is the default behavior) in the kubelet config.
- When the memory swap feature is turned on, Kubernetes data such as the content of Secret objects that were written to tmpfs now could be swapped to disk.

# [Communication between Nodes and the Control Plane | Kubernetes](https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/)

## Yellow

- Kubernetes has a "hub-and-spoke" API pattern.
- None of the other control plane components are designed to expose remote services.
- Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the API server along with valid client credentials.
- Pods that wish to connect to the API server can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in default namespace) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the API server.
- Control plane to node There are two primary communication paths from the control plane (the API server) to the nodes. The first is from the API server to the kubelet process which runs on each node in the cluster. The second is from the API server to any node, pod, or service through the API server's proxy functionality.
- API server to kubelet The connections from the API server to the kubelet are used for: Fetching logs for pods. Attaching (usually through kubectl) to running pods. Providing the kubelet's port-forwarding functionality.
- These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks. To verify this connection, use the --kubelet-certificate-authority flag to provide the API server with a root certificate bundle to use to verify the kubelet's serving certificate. If that is not possible, use SSH tunneling between the API server and kubelet if required to avoid connecting over an untrusted or public network. Finally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.
- API server to nodes, pods, and services The connections from the API server to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So while the connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run over untrusted or public networks

# [Controllers | Kubernetes](https://kubernetes.io/docs/concepts/architecture/controller/)

## Yellow

- In robotics and automation, a control loop is a non-terminating loop that regulates the state of a system. Here is one example of a control loop: a thermostat in a room. When you set the temperature, that's telling the thermostat about your desired state. The actual room temperature is the current state. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off. In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
- A controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state.
- The Job controller is an example of a Kubernetes built-in controller.
- Job is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and then stop.
- The Job controller does not run any Pods or containers itself. Instead, the Job controller tells the API server to create or remove Pods.
- Controllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates that Job object to mark it Finished.
- (There actually is a controller (https://github.com/kubernetes/autoscaler/) that horizontally scales the nodes in your cluster.)
- As long as the controllers for your cluster are running and able to make useful changes, it doesn't matter if the overall state is stable or not.
- It's useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so Kubernetes is designed to allow for that.
- Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager.

# [Leases | Kubernetes](https://kubernetes.io/docs/concepts/architecture/leases/)

## Yellow

- Distributed systems often have a need for leases, which provide a mechanism to lock shared resources and coordinate activity between members of a set. In Kubernetes, the lease concept is represented by Lease objects in the coordination.k8s.io API Group, which are used for system-critical capabilities such as node heartbeats and component-level leader election.
- Node heartbeats Kubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server. For every Node , there is a Lease object with a matching name in the kube-node-lease namespace. Under the hood, every kubelet heartbeat is an update request to this Lease object, updating the spec.renewTime field for the Lease. The Kubernetes control plane uses the time stamp of this field to determine the availability of this Node.
- Leader election Kubernetes also uses Leases to ensure only one instance of a component is running at any given time. This is used by control plane components like kube-controller-manager and kube-scheduler in HA configurations, where only one instance of the component should be actively running while the other instances are on stand-by.
- API server identity
- Starting in Kubernetes v1.26, each kube-apiserver uses the Lease API to publish its identity to the rest of the system. While not particularly useful on its own, this provides a mechanism for clients to discover how many instances of kube-apiserver are operating the Kubernetes control plane. Existence of kube-apiserver leases enables future capabilities that may require coordination between each kube-apiserver. You can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-system namespace with the name kube-apiserver-<sha256-hash>. Alternatively you can use the label selector apiserver.kubernetes.io/identity=kube-apiserver: kubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver

# [Cloud Controller Manager | Kubernetes](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)

## Yellow

- Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds. Kubernetes believes in automated, API-driven infrastructure without tight coupling between components. The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster. By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project. The cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their platforms with Kubernetes.
- The controllers inside the cloud controller manager include: Node controller The node controller is responsible for updating Node objects when new servers are created in your cloud infrastructure.
- Route controller The route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in your Kubernetes cluster can communicate with each other.
- Service controller Services integrate with cloud infrastructure components such as managed load balancers, IP addresses, network packet filtering, and target health checking.

# [About cgroup v2 | Kubernetes](https://kubernetes.io/docs/concepts/architecture/cgroups/)

## Yellow

- About cgroup v2 On Linux, control groups constrain resources that are allocated to processes. The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads. There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new generation of the cgroup API.
- cgroup v2 offers several improvements over cgroup v1, such as the following: Single unified hierarchy design in API Safer sub-tree delegation to containers Newer features like Pressure Stall Information Enhanced resource allocation management and isolation

# [Container Runtime Interface (CRI) | Kubernetes](https://kubernetes.io/docs/concepts/architecture/cri/)

## Yellow

- The CRI is a plugin interface which enables the kubelet to use a wide variety of container runtimes, without having a need to recompile the cluster components. You need a working container runtime on each Node in your cluster, so that the kubelet can launch Pods and their containers. The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime. The Kubernetes Container Runtime Interface (CRI) defines the main gRPC protocol for the communication between the node components kubelet and container runtime.

# [Garbage Collection | Kubernetes](https://kubernetes.io/docs/concepts/architecture/garbage-collection/)

## Yellow

- Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources. This allows the clean up of resources like the following: Terminated pods Completed Jobs Objects without owner references Unused containers and container images Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete Stale or expired CertificateSigningRequests (CSRs) Nodes deleted in the following scenarios: On a cloud when the cluster uses a cloud controller manager On-premises when the cluster uses an addon similar to a cloud controller manager Node Lease objects
- When you delete an object, you can control whether Kubernetes deletes the object's dependents automatically, in a process called cascading deletion. There are two types of cascading deletion, as follows: Foreground cascading deletion Background cascading deletion You can also control how and when garbage collection deletes resources that have owner references using Kubernetes finalizers.
- Foreground cascading deletion In foreground cascading deletion, the owner object you're deleting first enters a deletion in progress state. In this state, the following happens to the owner object: The Kubernetes API server sets the object's metadata.deletionTimestamp field to the time the object was marked for deletion. The Kubernetes API server also sets the metadata.finalizers field to foregroundDeletion. The object remains visible through the Kubernetes API until the deletion process is complete.
- During foreground cascading deletion, the only dependents that block owner deletion are those that have the ownerReference.blockOwnerDeletion=true field and are in the garbage collection controller cache.
- Background cascading deletion In background cascading deletion, the Kubernetes API server deletes the owner object immediately and the garbage collector controller (custom or default) cleans up the dependent objects in the background. If a finalizer exists, it ensures that objects are not deleted until all necessary clean-up tasks are completed. By default, Kubernetes uses background cascading deletion unless you manually use foreground deletion or choose to orphan the dependent objects.
- Container image lifecycle
- Disk usage above the configured HighThresholdPercent value triggers garbage collection, which deletes images in order based on the last time they were used, starting with the oldest first. The kubelet deletes images until disk usage reaches the LowThresholdPercent value.
- Container garbage collection The kubelet garbage collects unused containers based on the following variables, which you can define: MinAge: the minimum age at which the kubelet can garbage collect a container. Disable by setting to 0. MaxPerPodContainer: the maximum number of dead containers each Pod can have. Disable by setting to less than 0. MaxContainers: the maximum number of dead containers the cluster can have. Disable by setting to less than 0.

# [Mixed Version Proxy | Kubernetes](https://kubernetes.io/docs/concepts/architecture/mixed-version-proxy/)

## Yellow

- Kubernetes 1.32 includes an alpha feature that lets an API Server proxy a resource requests to other peer API servers. This is useful when there are multiple API servers running different versions of Kubernetes in one cluster (for example, during a long-lived rollout to a new release of Kubernetes).

# [Containers | Kubernetes](https://kubernetes.io/docs/concepts/containers/)

## Yellow

- https://kubernetes.io/docs/concepts/containers/https://kubernetes.io/docs/concepts/containers/images/https://kubernetes.io/docs/concepts/containers/container-environment/https://kubernetes.io/docs/concepts/containers/runtime-class/https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/

# [Container Lifecycle Hooks | Kubernetes](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/)

## Yellow

- Container hooks There are two hooks that are exposed to Containers: PostStart This hook is executed immediately after a container is created.
- PreStop This hook is called immediately before a container is terminated

# [Workloads | Kubernetes](https://kubernetes.io/docs/concepts/workloads/)

## Yellow

- A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of pods. In Kubernetes, a Pod represents a set of running containers on your cluster.
- you can use workload resources that manage a set of pods on your behalf. These resources configure controllers that make sure the right number of the right kind of pod are running, to match the state you specified. Kubernetes provides several built-in workload resources:
- Deployment and ReplicaSet
- Deployment is a good fit for managing a stateless application workload on your cluster
- StatefulSet lets you run one or more related Pods that do track state somehow.
- DaemonSet defines Pods that provide facilities that are local to nodes.
- Each pod in a DaemonSet performs a job similar to a system daemon on a classic Unix / POSIX server.
- Job and CronJob provide different ways to define tasks that run to completion and then stop.
- Using a custom resource definition, you can add in a third-party workload resource if you want a specific behavior that's not part of Kubernetes' core.

# [Pods | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/)

## Yellow

- Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
- A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.
- A Pod's contents are always co-located and co-scheduled, and run in a shared context.
- A Pod models an application-specific "logical host": it contains one or more application containers which are relatively tightly coupled.
- You need to install a container runtime into each node in the cluster so that Pods can run there.
- The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container.
- A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.
- Pods in a Kubernetes cluster are used in two main ways: Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly. Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit.
- You don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see Workload management (https://kubernetes.io/docs/concepts/workloads/controllers/) .
- Using Pods The following is an example of a Pod which consists of a container running the image nginx:1.14.2.
- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80
- To create the Pod shown above, run the following command: kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
- Pods are generally not created directly and are created using workload resources.
- Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally
- you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.
- Pods natively provide two kinds of shared resources for their constituent containers: networking and storage.
- You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because Pods are designed as relatively ephemeral, disposable entities.
- When a Pod gets created (directly by you, or indirectly by a controller), the new Pod is scheduled to run on a Node in your cluster. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is evicted for lack of resources, or the node fails.
- Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.
- The name of a Pod must be a valid DNS subdomain value
- PodTemplates are specifications for creating Pods, and are included in workload resources such as Deployments, Jobs, and DaemonSets. Each controller for a workload resource uses the PodTemplate inside the workload object to make actual Pods.
- apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # This is the pod template spec: containers: - name: hello image: busybox:1.28 command: ['sh', '-c', 'echo "Hello, Kubernetes!" && sleep 3600'] restartPolicy: OnFailure # The pod template ends here
- Each workload resource implements its own rules for handling changes to the Pod template. If you want to read more about StatefulSet specifically, read Update strategy in the StatefulSet Basics tutorial.
- when the Pod template for a workload resource is changed, the controller creates new Pods based on the updated template instead of updating or patching the existing Pods.
- Kubernetes doesn't prevent you from managing Pods directly. It is possible to update some fields of a running Pod, in place. However, Pod update operations like patch, and replace have some limitations:
- Most of the metadata about a Pod is immutable. For example, you cannot change the namespace, name, uid, or creationTimestamp fields; the generation field is unique. It only accepts updates that increment the field's current value.
- If the metadata.deletionTimestamp is set, no new entry can be added to the metadata.finalizers list.
- Pod updates may not change fields other than spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.tolerations. For spec.tolerations, you can only add new entries.
- Pods enable data sharing and communication among their constituent containers.
- A Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data.
- Each Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports.
- Containers in different Pods have distinct IP addresses and can not communicate by OS-level IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.
- Containers within the Pod see the system hostname as being the same as the configured name for the Pod.
- To set security constraints on Pods and containers, you use the securityContext field in the Pod specification. This field gives you granular control over what a Pod or individual containers can do. For example: Drop specific Linux capabilities to avoid the impact of a CVE. Force all processes in the Pod to run as a non-root user or as a specific user or group ID. Set a specific seccomp profile. Set Windows security options, such as whether containers run as HostProcess.
- Static Pods are always bound to one Kubelet on a specific node. The main use for static Pods is to run a self-hosted control plane: in other words, using the kubelet to supervise the individual control plane components.
- Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Whereas most Pods are managed by the control plane (for example, a Deployment), for static Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).
- The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.
- The spec of a static Pod cannot refer to other API objects
- Pods in a Kubernetes cluster are used in two main ways: Pods that run a single container.
- Pods that run multiple containers that need to work together.

# [Pod Lifecycle | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)

## Yellow

- Pods are created, assigned a unique ID (UID), and scheduled to run on nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods running on (or scheduled to run on) that node are marked for deletion. The control plane marks the Pods for removal after a timeout period.
- Whilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults.
- Pods are only scheduled once in their lifetime; assigning a Pod to a specific node is called binding, and the process of selecting which node to use is called scheduling.
- If one of the containers in the Pod fails, then Kubernetes may try to restart that specific container.
- Pods can however fail in a way that the cluster cannot recover from, and in that case Kubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the Pod and relies on other components to provide automatic healing.
- A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead, that Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can even have same name (as in .metadata.name) that the old Pod had, but the replacement would have a different .metadata.uid from the old Pod.
- When something is said to have the same lifetime as a Pod, such as a volume, that means that the thing exists as long as that specific Pod (with that exact UID) exists.
- A Pod's status field is a PodStatus object, which has a phase field.
- The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle.
- The phase is not intended to be a comprehensive rollup of observations of container or Pod state, nor is it intended to be a comprehensive state machine.
- nothing should be assumed about Pods that have a given phase value.
- Here are the possible values for phase: Value Description Pending The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network. Running The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting. Succeeded All containers in the Pod have terminated in success, and will not be restarted. Failed All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting. Unknown For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.
- When a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands. Make sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.
- A Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force.
- Container states
- Once the scheduler assigns a Pod to a Node, the kubelet starts creating containers for that Pod using a container runtime. There are three possible container states: Waiting, Running, and Terminated.
- o check the state of a Pod's containers, you can use kubectl describe pod <name-of-pod>
- Waiting If a container is not in either the Running or Terminated state, it is Waiting.
- Running The Running status indicates that a container is executing without issues.
- Terminated A container in the Terminated state began execution and then either ran to completion or failed for some reason.
- How Pods handle problems with containers Kubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:
- Initial crash
- Repeated crashes
- CrashLoopBackOff state
- Backoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.
- In other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay
- This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.
- To investigate the root cause of a CrashLoopBackOff issue, a user can: Check logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes. Inspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues. Review configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available. Check resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue. Debug application: There might exist bugs or misconfigurations in the application code. Running this container image locally or in a development environment can help diagnose application specific issues.
- The spec of a Pod has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always.
- Sidecar containers ignore the Pod-level restartPolicy field
- Always: Automatically restarts the container after any termination. OnFailure: Only restarts the container if it exits with an error (non-zero exit status). Never: Does not automatically restart the terminated container.
- After containers in a Pod exit, the kubelet restarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at 300 seconds (5 minutes).
- With the alpha feature gate KubeletCrashLoopBackOffMax enabled, you can reconfigure the maximum delay between container start retries from the default of 300s (5 minutes). This configuration is set per node using kubelet configuration.
- # container restart delays will start at 10s, increasing # 2x each time they are restarted, to a maximum of 100s kind: KubeletConfiguration crashLoopBackOff: maxContainerRestartPeriod: "100s"
- Your application can inject extra feedback or signals into PodStatus: Pod readiness. To use this, set readinessGates in the Pod's spec to specify a list of additional conditions that the kubelet evaluates for Pod readiness.
- If Kubernetes cannot find such a condition in the status.conditions field of a Pod, the status of the condition is defaulted to "False". Here is an example: kind: Pod ... spec: readinessGates: - conditionType: "www.example.com/feature-1" status: conditions: - type: Ready # a built in PodCondition status: "False" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: "www.example.com/feature-1" # an extra PodCondition status: "False" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true ...
- For a Pod that uses custom conditions, that Pod is evaluated to be ready only when both the following statements apply: All containers in the Pod are ready. All conditions specified in readinessGates are True.
- After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and to have any required storage volumes mounted. Once these phases are complete, the kubelet works with a container runtime (using Container Runtime Interface (CRI)) to set up a runtime sandbox and configure networking for the Pod.
- For a Pod with init containers, the kubelet sets the Initialized condition to True after the init containers have successfully completed
- Container probes A probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.
- Each probe must define exactly one of these four mechanisms: exec Executes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of 0. grpc Performs a remote procedure call using gRPC. The target should implement gRPC health checks. The diagnostic is considered successful if the status of the response is SERVING. httpGet Performs an HTTP GET request against the Pod's IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400. tcpSocket Performs a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open. If the remote system (the container) closes the connection immediately after it opens, this counts as healthy.
- configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.
- Probe outcome Each probe has one of three results: Success The container passed the diagnostic. Failure The container failed the diagnostic. Unknown The diagnostic failed (no action should be taken, and the kubelet will make further checks).
- Types of probe The kubelet can optionally perform and react to three kinds of probes on running containers: livenessProbe Indicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a liveness probe, the default state is Success. readinessProbe Indicates whether the container is ready to respond to requests. If the readiness probe fails, the endpoints controller removes the Pod's IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a container does not provide a readiness probe, the default state is Success. startupProbe Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a startup probe, the default state is Success.
- When should you use a readiness probe? If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe.
- When should you use a startup probe? Startup probes are useful for Pods that have containers that take a long time to come into service.
- If your container usually starts in more than initialDelaySeconds + failureThreshold × periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.
- Because Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed (rather than being abruptly stopped with a KILL signal and having no chance to clean up).
- Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container.
- Many container runtimes respect the STOPSIGNAL value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.
- If one of the Pod's containers has defined a preStop hook and the terminationGracePeriodSeconds in the Pod spec is not set to 0, the kubelet runs that hook inside of the container. The default terminationGracePeriodSeconds setting is 30 seconds.
- If the preStop hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds.
- The kubelet triggers the container runtime to send a TERM signal to process 1 inside each container.
- Terminating endpoints always have their ready status as false (for backward compatibility with versions before 1.26), so load balancers will not use it for regular traffic. See: https://kubernetes.io/docs/tutorials/services/pods-and-endpoint-termination-flow/
- The kubelet also cleans up a hidden pause container if that container runtime uses one.
- Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
- If your Pod includes one or more sidecar containers
- the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated. The sidecar containers will be terminated in the reverse order they are defined in the Pod spec. This ensures that sidecar containers continue serving the other containers in the Pod until they are no longer needed.
- Garbage collection of Pods For failed Pods, the API objects remain in the cluster's API until a human or controller process explicitly removes them. The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up terminated Pods (with a phase of Succeeded or Failed), when the number of Pods exceeds the configured threshold (determined by terminated-pod-gc-threshold in the kube-controller-manager). This avoids a resource leak as Pods are created and terminated over time. Additionally, PodGC cleans up any Pods which satisfy any of the following conditions: are orphan Pods - bound to a node which no longer exists, are unscheduled terminating Pods, are terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-service.

# [Init Containers | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)

## Yellow

- specialized containers that run before app containers in a Pod.
- Init containers are exactly like regular containers, except: Init containers always run to completion. Each init container must complete successfully before the next one starts.
- if the Pod has a restartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.
- Init containers run and complete their tasks before the main application container starts. Unlike sidecar containers, init containers are not continuously running alongside the main containers. Init containers run to completion sequentially, and the main container does not start until all the init containers have successfully completed. init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe whereas sidecar containers support all these probes to control their lifecycle. Init containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly with them. They can, however, use shared volumes for data exchange.
- Here are some ideas for how to use init containers: Wait for a Service to be created
- Register this Pod with a remote server from the downward API
- Place values into a configuration file and run a template tool
- This example defines a simple Pod that has two init containers. The first waits for myservice, and the second waits for mydb. Once both init containers complete, the Pod runs the app container from its spec section. apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app.kubernetes.io/name: MyApp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! && sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
- To see logs for the init containers in this Pod, run: kubectl logs myapp-pod -c init-myservice # Inspect the first init container kubectl logs myapp-pod -c init-mydb # Inspect the second init container
- if the Pod restartPolicy is set to Always, the init containers use restartPolicy OnFailure.
- The ports on an init container are not aggregated under a Service.
- init container code should be idempotent.

# [Sidecar Containers | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/)

## Yellow

- Sidecar containers are the secondary containers that run along with the main application container within the same Pod.
- For example, if you have a web application that requires a local webserver, the local webserver is a sidecar and the web application itself is the app container.
- Kubernetes implements sidecar containers as a special case of init containers; sidecar containers remain running after Pod startup.
- These can be started, stopped, or restarted without effecting the main application container and other init containers.
- initContainers: - name: logshipper image: alpine:latest restartPolicy: Always command: ['sh', '-c', 'tail -F /opt/logs.txt'] volumeMounts: - name: data mountPath: /opt
- If an init container is created with its restartPolicy set to Always, it will start and remain running during the entire life of the Pod.
- After a sidecar-style init container is running (the kubelet has set the started status for that init container to true), the kubelet then starts the next init container
- Upon Pod termination, the kubelet postpones terminating sidecar containers until the main application container has fully stopped. The sidecar containers are then shut down in the opposite order of their appearance in the Pod specification. This approach ensures that the sidecars remain operational, supporting other containers within the Pod, until their service is no longer required.
- Sidecar containers have their own independent lifecycles. They can be started, stopped, and restarted independently of app containers. This means you can update, scale, or maintain sidecar containers without affecting the primary application.
- From Kubernetes perspective, sidecars graceful termination is less important.
- sidecar containers will receive the SIGTERM following with SIGKILL faster than may be expected. So exit codes different from 0 (0 indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.
- Differences from init containers Sidecar containers work alongside the main container, extending its functionality and providing additional services. Sidecar containers run concurrently with the main application container. They are active throughout the lifecycle of the pod and can be started and stopped independently of the main container. Unlike init containers, sidecar containers support probes to control their lifecycle. Sidecar containers can interact directly with the main application containers, because like init containers they always share the same network, and can optionally also share volumes (filesystems). Init containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a Pod. Any data passing is one-way (for example, an init container can put information inside an emptyDir volume).

# [Ephemeral Containers | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/)

## Yellow

- Ephemeral containers differ from other containers in that they lack guarantees for resources or execution, and they will never be automatically restarted, so they are not appropriate for building applications.
- Ephemeral containers may not have ports, so fields such as ports, livenessProbe, readinessProbe are disallowed. Pod resource allocations are immutable, so setting resources is disallowed. For a complete list of allowed fields, see the EphemeralContainer reference documentation. Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it's not possible to add an ephemeral container using kubectl edit.
- Ephemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed or a container image doesn't include debugging utilities.

## Orange

- When using ephemeral containers, it's helpful to enable process namespace sharing so you can view processes in other containers.

# [Scheduling, Preemption and Eviction | Kubernetes](https://kubernetes.io/docs/concepts/scheduling-eviction/)

## Yellow

- In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or more Pods on Nodes.

# [Disruptions | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)

## Yellow

- Voluntary and involuntary disruptions
- Here are some ways to mitigate involuntary disruptions: Ensure your pod requests the resources it needs. Replicate your application if you need higher availability.
- For even higher availability when running replicated applications, spread applications across racks (using anti-affinity) or across zones (if using a multi-zone cluster.)
- Pod disruption budgets
- As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions
- Cluster managers and hosting providers should use tools which respect PodDisruptionBudgets by calling the Eviction API instead of directly deleting pods or deployments.
- Involuntary disruptions cannot be prevented by PDBs; however they do count against the budget.
- It is recommended to set AlwaysAllow Unhealthy Pod Eviction Policy to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.
- Pod disruption conditions
- A dedicated Pod DisruptionTarget condition is added to indicate that the Pod is about to be deleted due to a disruption. The reason field of the condition additionally indicates one of the following reasons for the Pod termination:
- PreemptionByScheduler Pod is due to be preempted by a scheduler in order to accommodate a new Pod with a higher priority.
- DeletionByTaintManager Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within kube-controller-manager) due to a NoExecute taint that the Pod does not tolerate
- EvictionByEvictionAPI Pod has been marked for eviction using the Kubernetes API .
- DeletionByPodGC Pod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage collection.
- TerminationByKubelet Pod has been terminated by the kubelet, because of either node pressure eviction, the graceful node shutdown, or preemption for system critical pods.
- Separating Cluster Owner and Application Owner Roles
- If you do not have such a separation of responsibilities in your organization, you may not need to use Pod Disruption Budgets.

# [Pod Quality of Service Classes | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/)

## Yellow

- Kubernetes assigns a QoS class to each Pod as a consequence of the resource constraints that you specify for the containers in that Pod. Kubernetes relies on this classification to make decisions about which Pods to evict when there are not enough available resources on a Node.
- The possible QoS classes are Guaranteed, Burstable, and BestEffort.
- When a Node runs out of resources, Kubernetes will first evict BestEffort Pods running on that Node, followed by Burstable and finally Guaranteed Pods.
- Guaranteed Pods that are Guaranteed have the strictest resource limits and are least likely to face eviction.
- These Pods can also make use of exclusive CPUs using the static CPU management policy.
- For a Pod to be given a QoS class of Guaranteed: Every Container in the Pod must have a memory limit and a memory request. For every Container in the Pod, the memory limit must equal the memory request. Every Container in the Pod must have a CPU limit and a CPU request. For every Container in the Pod, the CPU limit must equal the CPU request.
- Burstable Pods that are Burstable have some lower-bound resource guarantees based on the request, but do not require a specific limit.
- A Pod is given a QoS class of Burstable if: The Pod does not meet the criteria for QoS class Guaranteed. At least one Container in the Pod has a memory or CPU request or limit.
- BestEffort Pods in the BestEffort QoS class can use node resources that aren't specifically assigned to Pods in other QoS classes.
- For example, if you have a node with 16 CPU cores available to the kubelet, and you assign 4 CPU cores to a Guaranteed Pod, then a Pod in the BestEffort QoS class can try to use any amount of the remaining 12 CPU cores.
- The kubelet prefers to evict BestEffort Pods if the node comes under resource pressure.
- A Pod has a QoS class of BestEffort if it doesn't meet the criteria for either Guaranteed or Burstable.
- Memory QoS uses the memory controller of cgroup v2
- When memory.min is set to memory requests, memory resources are reserved and never reclaimed by the kernel
- Memory QoS uses memory.high to throttle workload approaching its memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.
- Some behavior is independent of QoS class
- Any Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that Pod.
- If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a candidate for eviction.
- The kube-scheduler does not consider QoS class when selecting which Pods to preempt.

# [User Namespaces | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/)

## Yellow

- A user namespace isolates the user running inside the container from the one in the host.
- A process running as root in a container can run as a different (non-root) user in the host
- You can use this feature to reduce the damage a compromised container can do to the host or other pods in the same node.
- you need at least Linux 6.3, as tmpfs started supporting idmap mounts in that version.
- Some OCI runtimes do not include the support needed for using user namespaces in Linux pods.
- the container runtime and its underlying OCI runtime must support user namespaces.
- A pod can opt-in to use user namespaces by setting the pod.spec.hostUsers field to false.
- The runAsUser, runAsGroup, fsGroup, etc. fields in the pod.spec always refer to the user inside the container.
- If a user namespace is used, this will isolate the users in the container from the users in the node.
- This means containers can run as root and be mapped to a non-root user on the host.
- This abstraction limits what can happen, for example, if the container manages to escape to the host. Given that the container is running as a non-privileged user on the host, it is limited what it can do to the host.
- Without using a user namespace a container running as root, in the case of a container breakout, has root privileges on the node. And if some capability were granted to the container, the capabilities are valid on the host too. None of this is true when we use user namespaces.

# [Downward API | Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/)

## Yellow

- It is sometimes useful for a container to have information about itself, without being overly coupled to Kubernetes. The downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.
- In Kubernetes, there are two ways to expose Pod and container fields to a running container: as environment variables as files in a downwardAPI volume Together, these two ways of exposing Pod and container fields are called the downward API.
- You can pass information from available Pod-level fields using fieldRef. At the API level, the spec for a Pod always defines at least one Container. You can pass information from available Container-level fields using resourceFieldRef.
- The fields available via either mechanism are: https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef

# [Workload Management | Kubernetes](https://kubernetes.io/docs/concepts/workloads/controllers/)

## Yellow

- You use the Kubernetes API to create a workload object that represents a higher abstraction level than a Pod, and then the Kubernetes control plane automatically manages Pod objects on your behalf, based on the specification for the workload object you defined.
- Deployment is a good fit for managing a stateless application workload
- The most common use for a StatefulSet is to be able to make a link between its Pods and their persistent storage.
- You use a DaemonSet when the driver, or other node-level service, has to run on the node where it's useful.

# [Deployments | Kubernetes](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

## Yellow

- A Deployment provides declarative updates for Pods and ReplicaSets.
- You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
- The following are typical use cases for Deployments:
- Create a Deployment to rollout a ReplicaSet.
- Declare the new state of the Pods
- Rollback to an earlier Deployment revision
- Scale up the Deployment to facilitate more load.
- Pause the rollout of a Deployment
- Use the status of the Deployment
- Clean up older ReplicaSets that you don't need anymore.
- Creating a Deployment
- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80
- The .spec.selector field defines how the created ReplicaSet finds which Pods to manage.
- To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment.
- To see the labels automatically generated for each Pod, run kubectl get pods --show-labels.
- A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed
- Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).
- Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).
- It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.
- A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed
- check the revisions of this Deployment: kubectl rollout history deployment/nginx-deployment
- You can specify theCHANGE-CAUSE message by: Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1" Manually editing the manifest of the resource. To see the details of each revision, run: kubectl rollout history deployment/nginx-deployment --revision=2
- Rolling Back to a Previous Revision
- kubectl rollout undo deployment/nginx-deployment
- kubectl rollout undo deployment/nginx-deployment --to-revision=2
- Scaling a Deployment
- kubectl scale deployment/nginx-deployment --replicas=10
- Assuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods. kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80
- Proportional scaling
- The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.
- Pausing and Resuming a rollout of a Deployment
- When you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.
- Pause by running the following command: kubectl rollout pause deployment/nginx-deployment The output is similar to this: deployment.apps/nginx-deployment paused Then update the image of the Deployment: kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 The output is similar to this: deployment.apps/nginx-deployment image updated Notice that no new rollout started
- kubectl rollout history deployment/nginx-deployment
- Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates: kubectl rollout resume deployment/nginx-deployment
- Deployment status
- A Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete, or it can fail to progress.
- type: Progressing status: "True" reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdated You can monitor the progress for a Deployment by using kubectl rollout status.
- type: Progressing status: "True" reason: NewReplicaSetAvailable
- The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes: kubectl patch deployment/nginx-deployment -p '{"spec":{"progressDeadlineSeconds":600}}'
- type: Progressing status: "False" reason: ProgressDeadlineExceeded
- Clean up Policy You can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.
- Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.
- .spec.selector is a required field that specifies a label selector for the Pods targeted by this Deployment. .spec.selector must match .spec.template.metadata.labels, or it will be rejected by the API.
- A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template
- You should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.
- If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly.
- .spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be "Recreate" or "RollingUpdate". "RollingUpdate" is the default value.
- This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an "at most" guarantee for your Pods, you should consider using a StatefulSet.
- Rolling Update Deployment
- You can specify maxUnavailable and maxSurge to control the rolling update process.
- Max Unavailable
- The absolute number is calculated from percentage by rounding down.
- For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.
- Max Surge
- The absolute number is calculated from the percentage by rounding up.
- For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.
- Progress Deadline Seconds .spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has failed progressing
- Min Ready Seconds .spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready
- Revision History Limit
- .spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs.
- Paused .spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused.

---

Created with Super Simple Highlighter. ©2010-24 [Dexterous Logic software](https://www.dexterouslogic.com/)